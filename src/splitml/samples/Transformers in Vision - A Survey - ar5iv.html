<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2101.01169] Transformers in Vision: A Survey</title><meta property="og:description" content="Astounding results from Transformer models on natural language tasks have intrigued the vision community to study their application to computer vision problems. Among their salient benefits, Transformers enable modelin…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Transformers in Vision: A Survey">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Transformers in Vision: A Survey">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2101.01169">

<!--Generated on Wed Dec 14 20:48:19 2022 by LaTeXML (version 0.8.6) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Self-attention,  transformers,  bidirectional encoders,  deep neural networks,  convolutional networks,  self-supervision.
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv.0.7.7.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.1.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Transformers in Vision: A Survey</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Salman Khan, Muzammal Naseer, Munawar Hayat, Syed Waqas Zamir,
<br class="ltx_break"> Fahad Shahbaz Khan, and Mubarak Shah
</span><span class="ltx_author_notes">S. Khan, M. Naseer and F. S. Khan are with the MBZ University of Artificial Intelligence, Abu Dhabi, UAE.
<br class="ltx_break">E-mail: firstname.lastname@mbzuai.ac.ae
M. Hayat is with the Faculty of IT, Monash University, Clayton VIC 3800, Australia.S. W. Zamir is with the Inception Institute of Artificial Intelligence, Abu Dhabi, UAE.S. Khan and M. Naseer are also with the CECS, Australian National University, Canberra ACT 0200, Australia. F. S. Khan is also with the Computer Vision Laboratory, Linköping University, Sweden. M. Shah is with the
Center for Research in Computer Vision, University of Central Florida, Orlando, FL 32816, United States.
Manuscript received March, 2021.</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Astounding results from Transformer models on natural language tasks have intrigued the vision community to study their application to computer vision problems. Among their salient benefits, Transformers enable modeling long dependencies between input sequence elements and support parallel processing of sequence as compared to recurrent networks <em id="id1.id1.1" class="ltx_emph ltx_font_italic">e.g.</em>, Long short-term memory (LSTM). Different from convolutional networks, Transformers require minimal inductive biases for their design and are naturally suited as set-functions. Furthermore, the straightforward design of Transformers allows processing multiple modalities (<em id="id1.id1.2" class="ltx_emph ltx_font_italic">e.g.</em>, images, videos, text and speech) using similar processing blocks and demonstrates excellent scalability to very large capacity networks and huge datasets. These strengths have led to exciting progress on a number of vision tasks using Transformer networks. This survey aims to provide a comprehensive overview of the Transformer models in the computer vision discipline. We start with an introduction to fundamental concepts behind the success of Transformers i.e., self-attention, large-scale pre-training, and bidirectional feature encoding. We then cover extensive applications of transformers in vision including popular recognition tasks (<em id="id1.id1.3" class="ltx_emph ltx_font_italic">e.g.</em>, image classification, object detection, action recognition, and segmentation), generative modeling, multi-modal tasks (<em id="id1.id1.4" class="ltx_emph ltx_font_italic">e.g.</em>, visual-question answering, visual reasoning, and visual grounding), video processing (<em id="id1.id1.5" class="ltx_emph ltx_font_italic">e.g.</em>, activity recognition, video forecasting), low-level vision (<em id="id1.id1.6" class="ltx_emph ltx_font_italic">e.g.</em>, image super-resolution, image enhancement, and colorization) and 3D analysis (<em id="id1.id1.7" class="ltx_emph ltx_font_italic">e.g.</em>, point cloud classification and segmentation). We compare the respective advantages and limitations of popular techniques both in terms of architectural design and their experimental value. Finally, we provide an analysis on open research directions and possible future works. We hope this effort will ignite further interest in the community to solve current challenges towards the application of transformer models in computer vision.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Self-attention, transformers, bidirectional encoders, deep neural networks, convolutional networks, self-supervision.

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Transformer
models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> have recently demonstrated exemplary performance on a broad range of language tasks <em id="S1.p1.1.1" class="ltx_emph ltx_font_italic">e.g.</em>, text classification, machine translation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> and question answering. Among these models, the most popular ones include BERT (Bidirectional Encoder Representations from Transformers) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, GPT (Generative Pre-trained Transformer) v1-3 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, RoBERTa (Robustly Optimized BERT Pre-training) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> and T5 (Text-to-Text Transfer Transformer) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. The profound impact of Transformer models has become more clear with their scalability to very large capacity models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. For example, the BERT-large <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> model with 340 million parameters was significantly outperformed by the GPT-3 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> model with 175 billion parameters while the latest mixture-of-experts Switch transformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> scales up to a whopping 1.6 trillion parameters!</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2101.01169/assets/Figs/FieldGrowth.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="159" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.3.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.4.2" class="ltx_text" style="font-size:90%;">Statistics on the number of times keywords such as BERT, Self-Attention, and Transformers appear in the titles of Peer-reviewed and arXiv papers over the past few years (in Computer Vision and Machine Learning). The plots show consistent growth in recent literature. This survey covers recent progress on Transformers in the computer vision domain.</span></figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">The breakthroughs from Transformer networks in Natural Language Processing (NLP) domain has sparked great interest in the computer vision community to adapt these models for vision and multi-modal learning tasks (Fig. <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Transformers in Vision: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). However, visual data follows a typical structure (e.g., spatial and temporal coherence), thus demanding novel network designs and training schemes. As a result, Transformer models and their variants have been successfully used for image recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, object detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, image super-resolution <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, video understanding <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, image generation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, text-image synthesis <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> and visual question answering <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, among several other use cases <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>. This survey aims to cover such recent and exciting efforts in the computer vision domain, providing a comprehensive reference to interested readers. </p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Transformer architectures are based on a self-attention mechanism that learns the relationships between elements of a sequence. As opposed to recurrent networks that process sequence elements recursively and can only attend to short-term context, Transformers can attend to complete sequences thereby learning long-range relationships. <span id="S1.p3.1.1" class="ltx_text" style="color:#000000;">Although attention models have been extensively used in both feed-forward and recurrent networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, Transformers are based solely on the attention mechanism and have a unique implementation (i.e., multi-head attention) optimized for parallelization.</span> An important feature of these models is their scalability to high-complexity models and large-scale datasets <span id="S1.p3.1.2" class="ltx_text" style="color:#000000;">e.g., in comparison to some of the other alternatives such as hard attention <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> which is stochastic in nature and requires Monte Carlo sampling for sampling attention locations.</span> Since Transformers assume minimal prior knowledge about the structure of the problem as compared to their convolutional and recurrent counterparts <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, they are typically pre-trained using pretext tasks on large-scale (unlabelled) datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. Such a pre-training avoids costly manual annotations, thereby encoding highly expressive and generalizable representations that model rich relationships between the entities present in a given dataset. The learned representations are then fine-tuned on the downstream tasks in a supervised manner to obtain favorable results.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">This paper provides a holistic overview of the transformer models developed for computer vision applications. We develop a taxonomy of the network design space and highlight the major strengths and shortcomings of the existing methods. Other literature reviews mainly focus on the NLP domain <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> or cover generic attention-based approaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>. By focusing on the newly emerging area of visual transformers, we comprehensively organize the recent approaches according to the intrinsic features of self-attention and the investigated task. We first provide an introduction to the salient concepts underlying Transformer networks and then elaborate on the specifics of recent vision transformers.
Where ever possible, we draw parallels between the Transformers used in the NLP domain <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> and the ones developed for vision problems to flash major novelties and interesting domain-specific insights.
Recent approaches show that convolution operations can be fully replaced with attention-based transformer modules and have also been used jointly in a single design to encourage symbiosis between the two complementary set of operations.
This survey finally details open research questions with an outlook towards the possible future work.
</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Foundations</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">There exist two key ideas that have contributed towards the development of conventional transformer models.
(a) The first one is <em id="S2.p1.1.1" class="ltx_emph ltx_font_italic">self-attention</em>, which allows capturing ‘long-term’ dependencies between sequence elements as compared to conventional recurrent models that find it challenging to encode such relationships.
(b) The second key idea is that of <em id="S2.p1.1.2" class="ltx_emph ltx_font_italic">pre-training</em><span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><span id="footnote1.1" class="ltx_text" style="color:#000000;">Several recent Vision Transformers demonstrate that the model can be learned end-to-end on ImageNet-1K without any dedicated pre-training phase <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>.
However, the performance generally remains lower than the pre-trained counter-parts.</span></span></span></span> on a large (un)labelled corpus in a <span id="S2.p1.1.3" class="ltx_text" style="color:#000000;">(self)supervised manner</span>, and subsequently fine-tuning to the target task with a small labeled dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>.
Below, we provide a brief tutorial on these two ideas (Sec. <a href="#S2.SS2" title="2.2 (Self) Supervised Pre-training ‣ 2 Foundations ‣ Transformers in Vision: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a> and <a href="#S2.SS1" title="2.1 Self-Attention in Transformers ‣ 2 Foundations ‣ Transformers in Vision: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.1</span></a>), along with a summary of seminal Transformer networks (Sec. <a href="#S2.SS3" title="2.3 Transformer Model ‣ 2 Foundations ‣ Transformers in Vision: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.3</span></a> and <a href="#S2.SS4" title="2.4 Bidirectional Representations ‣ 2 Foundations ‣ Transformers in Vision: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.4</span></a>) where these ideas have been applied.
This background will help us better understand the forthcoming Transformer based models used in the computer vision domain (Sec. <a href="#S3" title="3 Self-Attention &amp; Transformers in Vision ‣ Transformers in Vision: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>).</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2101.01169/assets/Figs/Self-attention.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="236" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F2.4.2.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S2.F2.2.1" class="ltx_text" style="font-size:90%;">An example self-attention block used in the vision domain <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>.
Given the input sequence of image features, the triplet of (key, query, value) is calculated followed by attention calculation and applying it to reweight the values.
A single head is shown here and an output projection (<span id="S2.F2.2.1.1" class="ltx_text ltx_markedasmath ltx_font_bold">W</span>) is finally applied to obtain output features with the same dimension as the input. Figure adapted from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>.</span></figcaption>
</figure>
<figure id="S2.F3" class="ltx_figure"><img src="/html/2101.01169/assets/Figs/Transformer_Arch.png" id="S2.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="592" height="236" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F3.5.2.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><em id="S2.F3.6.3" class="ltx_emph ltx_font_italic" style="font-size:90%;">Architecture of the Transformer Model</em><span id="S2.F3.2.1" class="ltx_text" style="font-size:90%;"> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. The model was first developed for the language translation task where an input sequence in one language is required to be converted to the output sequence in another language. The Transformer encoder (<em id="S2.F3.2.1.1" class="ltx_emph ltx_font_italic">middle</em> row) operates on the input language sequence and converts it to an embedding before passing it on to the encoder blocks.
The Transformer decoder (<em id="S2.F3.2.1.2" class="ltx_emph ltx_font_italic">bottom</em> row) operates on the previously generated outputs in the translated language and the encoded input sequence from the middle branch to output the next word in the output sequence. The sequence of previous outputs (used as input to the decoder) is obtained by shifting the output sentence to the right by one position and appending start-of-sentence token at the beginning. This shifting avoids the model to learn to simply copy the decoder input to the output. The ground-truth to train the model is simply the output language sequence (without any right shift) appended with an end-of-sentence token. The blocks consisting of multi-head attention (<em id="S2.F3.2.1.3" class="ltx_emph ltx_font_italic">top</em> row) and feed-forward layers are repeated <math id="S2.F3.2.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S2.F3.2.1.m1.1b"><mi id="S2.F3.2.1.m1.1.1" xref="S2.F3.2.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S2.F3.2.1.m1.1c"><ci id="S2.F3.2.1.m1.1.1.cmml" xref="S2.F3.2.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.F3.2.1.m1.1d">N</annotation><annotation encoding="application/x-llamapun" id="S2.F3.2.1.m1.1e">italic_N</annotation></semantics></math> times in both the encoder and decoder.</span></figcaption>
</figure>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span><span id="S2.SS1.1.1" class="ltx_text ltx_font_italic" style="color:#000000;">Self-Attention in Transformers</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.14" class="ltx_p">Given a sequence of items, self-attention estimates the relevance of one item to other items (e.g., which words are likely to come together in a sentence). The self-attention mechanism is an integral component of Transformers, which explicitly models the interactions between all entities of a sequence for structured prediction tasks. Basically, a self-attention layer updates each component of a sequence by aggregating global information from the complete input sequence.
Lets denote a sequence of <math id="S2.SS1.p1.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S2.SS1.p1.1.m1.1a"><mi id="S2.SS1.p1.1.m1.1.1" xref="S2.SS1.p1.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.1.m1.1b"><ci id="S2.SS1.p1.1.m1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.1.m1.1c">n</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.1.m1.1d">italic_n</annotation></semantics></math> entities (<math id="S2.SS1.p1.2.m2.3" class="ltx_Math" alttext="\mathbf{x}_{1},\mathbf{x}_{2},\cdots\mathbf{x}_{n}" display="inline"><semantics id="S2.SS1.p1.2.m2.3a"><mrow id="S2.SS1.p1.2.m2.3.3.3" xref="S2.SS1.p1.2.m2.3.3.4.cmml"><msub id="S2.SS1.p1.2.m2.1.1.1.1" xref="S2.SS1.p1.2.m2.1.1.1.1.cmml"><mi id="S2.SS1.p1.2.m2.1.1.1.1.2" xref="S2.SS1.p1.2.m2.1.1.1.1.2.cmml">𝐱</mi><mn id="S2.SS1.p1.2.m2.1.1.1.1.3" xref="S2.SS1.p1.2.m2.1.1.1.1.3.cmml">1</mn></msub><mo id="S2.SS1.p1.2.m2.3.3.3.4" xref="S2.SS1.p1.2.m2.3.3.4.cmml">,</mo><msub id="S2.SS1.p1.2.m2.2.2.2.2" xref="S2.SS1.p1.2.m2.2.2.2.2.cmml"><mi id="S2.SS1.p1.2.m2.2.2.2.2.2" xref="S2.SS1.p1.2.m2.2.2.2.2.2.cmml">𝐱</mi><mn id="S2.SS1.p1.2.m2.2.2.2.2.3" xref="S2.SS1.p1.2.m2.2.2.2.2.3.cmml">2</mn></msub><mo id="S2.SS1.p1.2.m2.3.3.3.5" xref="S2.SS1.p1.2.m2.3.3.4.cmml">,</mo><mrow id="S2.SS1.p1.2.m2.3.3.3.3" xref="S2.SS1.p1.2.m2.3.3.3.3.cmml"><mi mathvariant="normal" id="S2.SS1.p1.2.m2.3.3.3.3.2" xref="S2.SS1.p1.2.m2.3.3.3.3.2.cmml">⋯</mi><mo id="S2.SS1.p1.2.m2.3.3.3.3.1" xref="S2.SS1.p1.2.m2.3.3.3.3.1.cmml" lspace='0px' rspace='0px'></mo><msub id="S2.SS1.p1.2.m2.3.3.3.3.3" xref="S2.SS1.p1.2.m2.3.3.3.3.3.cmml"><mi id="S2.SS1.p1.2.m2.3.3.3.3.3.2" xref="S2.SS1.p1.2.m2.3.3.3.3.3.2.cmml">𝐱</mi><mi id="S2.SS1.p1.2.m2.3.3.3.3.3.3" xref="S2.SS1.p1.2.m2.3.3.3.3.3.3.cmml">n</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.2.m2.3b"><list id="S2.SS1.p1.2.m2.3.3.4.cmml" xref="S2.SS1.p1.2.m2.3.3.3"><apply id="S2.SS1.p1.2.m2.1.1.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.2.m2.1.1.1.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1.1.1">subscript</csymbol><ci id="S2.SS1.p1.2.m2.1.1.1.1.2.cmml" xref="S2.SS1.p1.2.m2.1.1.1.1.2">𝐱</ci><cn type="integer" id="S2.SS1.p1.2.m2.1.1.1.1.3.cmml" xref="S2.SS1.p1.2.m2.1.1.1.1.3">1</cn></apply><apply id="S2.SS1.p1.2.m2.2.2.2.2.cmml" xref="S2.SS1.p1.2.m2.2.2.2.2"><csymbol cd="ambiguous" id="S2.SS1.p1.2.m2.2.2.2.2.1.cmml" xref="S2.SS1.p1.2.m2.2.2.2.2">subscript</csymbol><ci id="S2.SS1.p1.2.m2.2.2.2.2.2.cmml" xref="S2.SS1.p1.2.m2.2.2.2.2.2">𝐱</ci><cn type="integer" id="S2.SS1.p1.2.m2.2.2.2.2.3.cmml" xref="S2.SS1.p1.2.m2.2.2.2.2.3">2</cn></apply><apply id="S2.SS1.p1.2.m2.3.3.3.3.cmml" xref="S2.SS1.p1.2.m2.3.3.3.3"><times id="S2.SS1.p1.2.m2.3.3.3.3.1.cmml" xref="S2.SS1.p1.2.m2.3.3.3.3.1"></times><ci id="S2.SS1.p1.2.m2.3.3.3.3.2.cmml" xref="S2.SS1.p1.2.m2.3.3.3.3.2">⋯</ci><apply id="S2.SS1.p1.2.m2.3.3.3.3.3.cmml" xref="S2.SS1.p1.2.m2.3.3.3.3.3"><csymbol cd="ambiguous" id="S2.SS1.p1.2.m2.3.3.3.3.3.1.cmml" xref="S2.SS1.p1.2.m2.3.3.3.3.3">subscript</csymbol><ci id="S2.SS1.p1.2.m2.3.3.3.3.3.2.cmml" xref="S2.SS1.p1.2.m2.3.3.3.3.3.2">𝐱</ci><ci id="S2.SS1.p1.2.m2.3.3.3.3.3.3.cmml" xref="S2.SS1.p1.2.m2.3.3.3.3.3.3">𝑛</ci></apply></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.2.m2.3c">\mathbf{x}_{1},\mathbf{x}_{2},\cdots\mathbf{x}_{n}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.2.m2.3d">bold_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , bold_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ⋯ bold_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT</annotation></semantics></math>) by <math id="S2.SS1.p1.3.m3.1" class="ltx_Math" alttext="\mathbf{X}\in\mathbb{R}^{n\times d}" display="inline"><semantics id="S2.SS1.p1.3.m3.1a"><mrow id="S2.SS1.p1.3.m3.1.1" xref="S2.SS1.p1.3.m3.1.1.cmml"><mi id="S2.SS1.p1.3.m3.1.1.2" xref="S2.SS1.p1.3.m3.1.1.2.cmml">𝐗</mi><mo id="S2.SS1.p1.3.m3.1.1.1" xref="S2.SS1.p1.3.m3.1.1.1.cmml">∈</mo><msup id="S2.SS1.p1.3.m3.1.1.3" xref="S2.SS1.p1.3.m3.1.1.3.cmml"><mi id="S2.SS1.p1.3.m3.1.1.3.2" xref="S2.SS1.p1.3.m3.1.1.3.2.cmml">ℝ</mi><mrow id="S2.SS1.p1.3.m3.1.1.3.3" xref="S2.SS1.p1.3.m3.1.1.3.3.cmml"><mi id="S2.SS1.p1.3.m3.1.1.3.3.2" xref="S2.SS1.p1.3.m3.1.1.3.3.2.cmml">n</mi><mo lspace="0.222em" rspace="0.222em" id="S2.SS1.p1.3.m3.1.1.3.3.1" xref="S2.SS1.p1.3.m3.1.1.3.3.1.cmml">×</mo><mi id="S2.SS1.p1.3.m3.1.1.3.3.3" xref="S2.SS1.p1.3.m3.1.1.3.3.3.cmml">d</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.3.m3.1b"><apply id="S2.SS1.p1.3.m3.1.1.cmml" xref="S2.SS1.p1.3.m3.1.1"><in id="S2.SS1.p1.3.m3.1.1.1.cmml" xref="S2.SS1.p1.3.m3.1.1.1"></in><ci id="S2.SS1.p1.3.m3.1.1.2.cmml" xref="S2.SS1.p1.3.m3.1.1.2">𝐗</ci><apply id="S2.SS1.p1.3.m3.1.1.3.cmml" xref="S2.SS1.p1.3.m3.1.1.3"><csymbol cd="ambiguous" id="S2.SS1.p1.3.m3.1.1.3.1.cmml" xref="S2.SS1.p1.3.m3.1.1.3">superscript</csymbol><ci id="S2.SS1.p1.3.m3.1.1.3.2.cmml" xref="S2.SS1.p1.3.m3.1.1.3.2">ℝ</ci><apply id="S2.SS1.p1.3.m3.1.1.3.3.cmml" xref="S2.SS1.p1.3.m3.1.1.3.3"><times id="S2.SS1.p1.3.m3.1.1.3.3.1.cmml" xref="S2.SS1.p1.3.m3.1.1.3.3.1"></times><ci id="S2.SS1.p1.3.m3.1.1.3.3.2.cmml" xref="S2.SS1.p1.3.m3.1.1.3.3.2">𝑛</ci><ci id="S2.SS1.p1.3.m3.1.1.3.3.3.cmml" xref="S2.SS1.p1.3.m3.1.1.3.3.3">𝑑</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.3.m3.1c">\mathbf{X}\in\mathbb{R}^{n\times d}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.3.m3.1d">bold_X ∈ blackboard_R start_POSTSUPERSCRIPT italic_n × italic_d end_POSTSUPERSCRIPT</annotation></semantics></math>, where <math id="S2.SS1.p1.4.m4.1" class="ltx_Math" alttext="d" display="inline"><semantics id="S2.SS1.p1.4.m4.1a"><mi id="S2.SS1.p1.4.m4.1.1" xref="S2.SS1.p1.4.m4.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.4.m4.1b"><ci id="S2.SS1.p1.4.m4.1.1.cmml" xref="S2.SS1.p1.4.m4.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.4.m4.1c">d</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.4.m4.1d">italic_d</annotation></semantics></math> is the embedding dimension to represent each entity. The goal of self-attention is to capture the interaction amongst all <math id="S2.SS1.p1.5.m5.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S2.SS1.p1.5.m5.1a"><mi id="S2.SS1.p1.5.m5.1.1" xref="S2.SS1.p1.5.m5.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.5.m5.1b"><ci id="S2.SS1.p1.5.m5.1.1.cmml" xref="S2.SS1.p1.5.m5.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.5.m5.1c">n</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.5.m5.1d">italic_n</annotation></semantics></math> entities by encoding each entity in terms of the global contextual information. This is done by defining three learnable weight matrices to transform Queries (<math id="S2.SS1.p1.6.m6.1" class="ltx_Math" alttext="\mathbf{W}^{Q}\in\mathbb{R}^{d\times d_{q}}" display="inline"><semantics id="S2.SS1.p1.6.m6.1a"><mrow id="S2.SS1.p1.6.m6.1.1" xref="S2.SS1.p1.6.m6.1.1.cmml"><msup id="S2.SS1.p1.6.m6.1.1.2" xref="S2.SS1.p1.6.m6.1.1.2.cmml"><mi id="S2.SS1.p1.6.m6.1.1.2.2" xref="S2.SS1.p1.6.m6.1.1.2.2.cmml">𝐖</mi><mi id="S2.SS1.p1.6.m6.1.1.2.3" xref="S2.SS1.p1.6.m6.1.1.2.3.cmml">Q</mi></msup><mo id="S2.SS1.p1.6.m6.1.1.1" xref="S2.SS1.p1.6.m6.1.1.1.cmml">∈</mo><msup id="S2.SS1.p1.6.m6.1.1.3" xref="S2.SS1.p1.6.m6.1.1.3.cmml"><mi id="S2.SS1.p1.6.m6.1.1.3.2" xref="S2.SS1.p1.6.m6.1.1.3.2.cmml">ℝ</mi><mrow id="S2.SS1.p1.6.m6.1.1.3.3" xref="S2.SS1.p1.6.m6.1.1.3.3.cmml"><mi id="S2.SS1.p1.6.m6.1.1.3.3.2" xref="S2.SS1.p1.6.m6.1.1.3.3.2.cmml">d</mi><mo lspace="0.222em" rspace="0.222em" id="S2.SS1.p1.6.m6.1.1.3.3.1" xref="S2.SS1.p1.6.m6.1.1.3.3.1.cmml">×</mo><msub id="S2.SS1.p1.6.m6.1.1.3.3.3" xref="S2.SS1.p1.6.m6.1.1.3.3.3.cmml"><mi id="S2.SS1.p1.6.m6.1.1.3.3.3.2" xref="S2.SS1.p1.6.m6.1.1.3.3.3.2.cmml">d</mi><mi id="S2.SS1.p1.6.m6.1.1.3.3.3.3" xref="S2.SS1.p1.6.m6.1.1.3.3.3.3.cmml">q</mi></msub></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.6.m6.1b"><apply id="S2.SS1.p1.6.m6.1.1.cmml" xref="S2.SS1.p1.6.m6.1.1"><in id="S2.SS1.p1.6.m6.1.1.1.cmml" xref="S2.SS1.p1.6.m6.1.1.1"></in><apply id="S2.SS1.p1.6.m6.1.1.2.cmml" xref="S2.SS1.p1.6.m6.1.1.2"><csymbol cd="ambiguous" id="S2.SS1.p1.6.m6.1.1.2.1.cmml" xref="S2.SS1.p1.6.m6.1.1.2">superscript</csymbol><ci id="S2.SS1.p1.6.m6.1.1.2.2.cmml" xref="S2.SS1.p1.6.m6.1.1.2.2">𝐖</ci><ci id="S2.SS1.p1.6.m6.1.1.2.3.cmml" xref="S2.SS1.p1.6.m6.1.1.2.3">𝑄</ci></apply><apply id="S2.SS1.p1.6.m6.1.1.3.cmml" xref="S2.SS1.p1.6.m6.1.1.3"><csymbol cd="ambiguous" id="S2.SS1.p1.6.m6.1.1.3.1.cmml" xref="S2.SS1.p1.6.m6.1.1.3">superscript</csymbol><ci id="S2.SS1.p1.6.m6.1.1.3.2.cmml" xref="S2.SS1.p1.6.m6.1.1.3.2">ℝ</ci><apply id="S2.SS1.p1.6.m6.1.1.3.3.cmml" xref="S2.SS1.p1.6.m6.1.1.3.3"><times id="S2.SS1.p1.6.m6.1.1.3.3.1.cmml" xref="S2.SS1.p1.6.m6.1.1.3.3.1"></times><ci id="S2.SS1.p1.6.m6.1.1.3.3.2.cmml" xref="S2.SS1.p1.6.m6.1.1.3.3.2">𝑑</ci><apply id="S2.SS1.p1.6.m6.1.1.3.3.3.cmml" xref="S2.SS1.p1.6.m6.1.1.3.3.3"><csymbol cd="ambiguous" id="S2.SS1.p1.6.m6.1.1.3.3.3.1.cmml" xref="S2.SS1.p1.6.m6.1.1.3.3.3">subscript</csymbol><ci id="S2.SS1.p1.6.m6.1.1.3.3.3.2.cmml" xref="S2.SS1.p1.6.m6.1.1.3.3.3.2">𝑑</ci><ci id="S2.SS1.p1.6.m6.1.1.3.3.3.3.cmml" xref="S2.SS1.p1.6.m6.1.1.3.3.3.3">𝑞</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.6.m6.1c">\mathbf{W}^{Q}\in\mathbb{R}^{d\times d_{q}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.6.m6.1d">bold_W start_POSTSUPERSCRIPT italic_Q end_POSTSUPERSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d × italic_d start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT end_POSTSUPERSCRIPT</annotation></semantics></math>), Keys (<math id="S2.SS1.p1.7.m7.1" class="ltx_Math" alttext="\mathbf{W}^{K}\in\mathbb{R}^{d\times d_{k}}" display="inline"><semantics id="S2.SS1.p1.7.m7.1a"><mrow id="S2.SS1.p1.7.m7.1.1" xref="S2.SS1.p1.7.m7.1.1.cmml"><msup id="S2.SS1.p1.7.m7.1.1.2" xref="S2.SS1.p1.7.m7.1.1.2.cmml"><mi id="S2.SS1.p1.7.m7.1.1.2.2" xref="S2.SS1.p1.7.m7.1.1.2.2.cmml">𝐖</mi><mi id="S2.SS1.p1.7.m7.1.1.2.3" xref="S2.SS1.p1.7.m7.1.1.2.3.cmml">K</mi></msup><mo id="S2.SS1.p1.7.m7.1.1.1" xref="S2.SS1.p1.7.m7.1.1.1.cmml">∈</mo><msup id="S2.SS1.p1.7.m7.1.1.3" xref="S2.SS1.p1.7.m7.1.1.3.cmml"><mi id="S2.SS1.p1.7.m7.1.1.3.2" xref="S2.SS1.p1.7.m7.1.1.3.2.cmml">ℝ</mi><mrow id="S2.SS1.p1.7.m7.1.1.3.3" xref="S2.SS1.p1.7.m7.1.1.3.3.cmml"><mi id="S2.SS1.p1.7.m7.1.1.3.3.2" xref="S2.SS1.p1.7.m7.1.1.3.3.2.cmml">d</mi><mo lspace="0.222em" rspace="0.222em" id="S2.SS1.p1.7.m7.1.1.3.3.1" xref="S2.SS1.p1.7.m7.1.1.3.3.1.cmml">×</mo><msub id="S2.SS1.p1.7.m7.1.1.3.3.3" xref="S2.SS1.p1.7.m7.1.1.3.3.3.cmml"><mi id="S2.SS1.p1.7.m7.1.1.3.3.3.2" xref="S2.SS1.p1.7.m7.1.1.3.3.3.2.cmml">d</mi><mi id="S2.SS1.p1.7.m7.1.1.3.3.3.3" xref="S2.SS1.p1.7.m7.1.1.3.3.3.3.cmml">k</mi></msub></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.7.m7.1b"><apply id="S2.SS1.p1.7.m7.1.1.cmml" xref="S2.SS1.p1.7.m7.1.1"><in id="S2.SS1.p1.7.m7.1.1.1.cmml" xref="S2.SS1.p1.7.m7.1.1.1"></in><apply id="S2.SS1.p1.7.m7.1.1.2.cmml" xref="S2.SS1.p1.7.m7.1.1.2"><csymbol cd="ambiguous" id="S2.SS1.p1.7.m7.1.1.2.1.cmml" xref="S2.SS1.p1.7.m7.1.1.2">superscript</csymbol><ci id="S2.SS1.p1.7.m7.1.1.2.2.cmml" xref="S2.SS1.p1.7.m7.1.1.2.2">𝐖</ci><ci id="S2.SS1.p1.7.m7.1.1.2.3.cmml" xref="S2.SS1.p1.7.m7.1.1.2.3">𝐾</ci></apply><apply id="S2.SS1.p1.7.m7.1.1.3.cmml" xref="S2.SS1.p1.7.m7.1.1.3"><csymbol cd="ambiguous" id="S2.SS1.p1.7.m7.1.1.3.1.cmml" xref="S2.SS1.p1.7.m7.1.1.3">superscript</csymbol><ci id="S2.SS1.p1.7.m7.1.1.3.2.cmml" xref="S2.SS1.p1.7.m7.1.1.3.2">ℝ</ci><apply id="S2.SS1.p1.7.m7.1.1.3.3.cmml" xref="S2.SS1.p1.7.m7.1.1.3.3"><times id="S2.SS1.p1.7.m7.1.1.3.3.1.cmml" xref="S2.SS1.p1.7.m7.1.1.3.3.1"></times><ci id="S2.SS1.p1.7.m7.1.1.3.3.2.cmml" xref="S2.SS1.p1.7.m7.1.1.3.3.2">𝑑</ci><apply id="S2.SS1.p1.7.m7.1.1.3.3.3.cmml" xref="S2.SS1.p1.7.m7.1.1.3.3.3"><csymbol cd="ambiguous" id="S2.SS1.p1.7.m7.1.1.3.3.3.1.cmml" xref="S2.SS1.p1.7.m7.1.1.3.3.3">subscript</csymbol><ci id="S2.SS1.p1.7.m7.1.1.3.3.3.2.cmml" xref="S2.SS1.p1.7.m7.1.1.3.3.3.2">𝑑</ci><ci id="S2.SS1.p1.7.m7.1.1.3.3.3.3.cmml" xref="S2.SS1.p1.7.m7.1.1.3.3.3.3">𝑘</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.7.m7.1c">\mathbf{W}^{K}\in\mathbb{R}^{d\times d_{k}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.7.m7.1d">bold_W start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d × italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUPERSCRIPT</annotation></semantics></math>) and Values (<math id="S2.SS1.p1.8.m8.1" class="ltx_Math" alttext="\mathbf{W}^{V}\in\mathbb{R}^{d\times d_{v}}" display="inline"><semantics id="S2.SS1.p1.8.m8.1a"><mrow id="S2.SS1.p1.8.m8.1.1" xref="S2.SS1.p1.8.m8.1.1.cmml"><msup id="S2.SS1.p1.8.m8.1.1.2" xref="S2.SS1.p1.8.m8.1.1.2.cmml"><mi id="S2.SS1.p1.8.m8.1.1.2.2" xref="S2.SS1.p1.8.m8.1.1.2.2.cmml">𝐖</mi><mi id="S2.SS1.p1.8.m8.1.1.2.3" xref="S2.SS1.p1.8.m8.1.1.2.3.cmml">V</mi></msup><mo id="S2.SS1.p1.8.m8.1.1.1" xref="S2.SS1.p1.8.m8.1.1.1.cmml">∈</mo><msup id="S2.SS1.p1.8.m8.1.1.3" xref="S2.SS1.p1.8.m8.1.1.3.cmml"><mi id="S2.SS1.p1.8.m8.1.1.3.2" xref="S2.SS1.p1.8.m8.1.1.3.2.cmml">ℝ</mi><mrow id="S2.SS1.p1.8.m8.1.1.3.3" xref="S2.SS1.p1.8.m8.1.1.3.3.cmml"><mi id="S2.SS1.p1.8.m8.1.1.3.3.2" xref="S2.SS1.p1.8.m8.1.1.3.3.2.cmml">d</mi><mo lspace="0.222em" rspace="0.222em" id="S2.SS1.p1.8.m8.1.1.3.3.1" xref="S2.SS1.p1.8.m8.1.1.3.3.1.cmml">×</mo><msub id="S2.SS1.p1.8.m8.1.1.3.3.3" xref="S2.SS1.p1.8.m8.1.1.3.3.3.cmml"><mi id="S2.SS1.p1.8.m8.1.1.3.3.3.2" xref="S2.SS1.p1.8.m8.1.1.3.3.3.2.cmml">d</mi><mi id="S2.SS1.p1.8.m8.1.1.3.3.3.3" xref="S2.SS1.p1.8.m8.1.1.3.3.3.3.cmml">v</mi></msub></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.8.m8.1b"><apply id="S2.SS1.p1.8.m8.1.1.cmml" xref="S2.SS1.p1.8.m8.1.1"><in id="S2.SS1.p1.8.m8.1.1.1.cmml" xref="S2.SS1.p1.8.m8.1.1.1"></in><apply id="S2.SS1.p1.8.m8.1.1.2.cmml" xref="S2.SS1.p1.8.m8.1.1.2"><csymbol cd="ambiguous" id="S2.SS1.p1.8.m8.1.1.2.1.cmml" xref="S2.SS1.p1.8.m8.1.1.2">superscript</csymbol><ci id="S2.SS1.p1.8.m8.1.1.2.2.cmml" xref="S2.SS1.p1.8.m8.1.1.2.2">𝐖</ci><ci id="S2.SS1.p1.8.m8.1.1.2.3.cmml" xref="S2.SS1.p1.8.m8.1.1.2.3">𝑉</ci></apply><apply id="S2.SS1.p1.8.m8.1.1.3.cmml" xref="S2.SS1.p1.8.m8.1.1.3"><csymbol cd="ambiguous" id="S2.SS1.p1.8.m8.1.1.3.1.cmml" xref="S2.SS1.p1.8.m8.1.1.3">superscript</csymbol><ci id="S2.SS1.p1.8.m8.1.1.3.2.cmml" xref="S2.SS1.p1.8.m8.1.1.3.2">ℝ</ci><apply id="S2.SS1.p1.8.m8.1.1.3.3.cmml" xref="S2.SS1.p1.8.m8.1.1.3.3"><times id="S2.SS1.p1.8.m8.1.1.3.3.1.cmml" xref="S2.SS1.p1.8.m8.1.1.3.3.1"></times><ci id="S2.SS1.p1.8.m8.1.1.3.3.2.cmml" xref="S2.SS1.p1.8.m8.1.1.3.3.2">𝑑</ci><apply id="S2.SS1.p1.8.m8.1.1.3.3.3.cmml" xref="S2.SS1.p1.8.m8.1.1.3.3.3"><csymbol cd="ambiguous" id="S2.SS1.p1.8.m8.1.1.3.3.3.1.cmml" xref="S2.SS1.p1.8.m8.1.1.3.3.3">subscript</csymbol><ci id="S2.SS1.p1.8.m8.1.1.3.3.3.2.cmml" xref="S2.SS1.p1.8.m8.1.1.3.3.3.2">𝑑</ci><ci id="S2.SS1.p1.8.m8.1.1.3.3.3.3.cmml" xref="S2.SS1.p1.8.m8.1.1.3.3.3.3">𝑣</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.8.m8.1c">\mathbf{W}^{V}\in\mathbb{R}^{d\times d_{v}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.8.m8.1d">bold_W start_POSTSUPERSCRIPT italic_V end_POSTSUPERSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d × italic_d start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT end_POSTSUPERSCRIPT</annotation></semantics></math>), where <math id="S2.SS1.p1.9.m9.1" class="ltx_Math" alttext="d_{q}=d_{k}" display="inline"><semantics id="S2.SS1.p1.9.m9.1a"><mrow id="S2.SS1.p1.9.m9.1.1" xref="S2.SS1.p1.9.m9.1.1.cmml"><msub id="S2.SS1.p1.9.m9.1.1.2" xref="S2.SS1.p1.9.m9.1.1.2.cmml"><mi id="S2.SS1.p1.9.m9.1.1.2.2" xref="S2.SS1.p1.9.m9.1.1.2.2.cmml">d</mi><mi id="S2.SS1.p1.9.m9.1.1.2.3" xref="S2.SS1.p1.9.m9.1.1.2.3.cmml">q</mi></msub><mo id="S2.SS1.p1.9.m9.1.1.1" xref="S2.SS1.p1.9.m9.1.1.1.cmml">=</mo><msub id="S2.SS1.p1.9.m9.1.1.3" xref="S2.SS1.p1.9.m9.1.1.3.cmml"><mi id="S2.SS1.p1.9.m9.1.1.3.2" xref="S2.SS1.p1.9.m9.1.1.3.2.cmml">d</mi><mi id="S2.SS1.p1.9.m9.1.1.3.3" xref="S2.SS1.p1.9.m9.1.1.3.3.cmml">k</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.9.m9.1b"><apply id="S2.SS1.p1.9.m9.1.1.cmml" xref="S2.SS1.p1.9.m9.1.1"><eq id="S2.SS1.p1.9.m9.1.1.1.cmml" xref="S2.SS1.p1.9.m9.1.1.1"></eq><apply id="S2.SS1.p1.9.m9.1.1.2.cmml" xref="S2.SS1.p1.9.m9.1.1.2"><csymbol cd="ambiguous" id="S2.SS1.p1.9.m9.1.1.2.1.cmml" xref="S2.SS1.p1.9.m9.1.1.2">subscript</csymbol><ci id="S2.SS1.p1.9.m9.1.1.2.2.cmml" xref="S2.SS1.p1.9.m9.1.1.2.2">𝑑</ci><ci id="S2.SS1.p1.9.m9.1.1.2.3.cmml" xref="S2.SS1.p1.9.m9.1.1.2.3">𝑞</ci></apply><apply id="S2.SS1.p1.9.m9.1.1.3.cmml" xref="S2.SS1.p1.9.m9.1.1.3"><csymbol cd="ambiguous" id="S2.SS1.p1.9.m9.1.1.3.1.cmml" xref="S2.SS1.p1.9.m9.1.1.3">subscript</csymbol><ci id="S2.SS1.p1.9.m9.1.1.3.2.cmml" xref="S2.SS1.p1.9.m9.1.1.3.2">𝑑</ci><ci id="S2.SS1.p1.9.m9.1.1.3.3.cmml" xref="S2.SS1.p1.9.m9.1.1.3.3">𝑘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.9.m9.1c">d_{q}=d_{k}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.9.m9.1d">italic_d start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT = italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math>. The input sequence <math id="S2.SS1.p1.10.m10.1" class="ltx_Math" alttext="\mathbf{X}" display="inline"><semantics id="S2.SS1.p1.10.m10.1a"><mi id="S2.SS1.p1.10.m10.1.1" xref="S2.SS1.p1.10.m10.1.1.cmml">𝐗</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.10.m10.1b"><ci id="S2.SS1.p1.10.m10.1.1.cmml" xref="S2.SS1.p1.10.m10.1.1">𝐗</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.10.m10.1c">\mathbf{X}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.10.m10.1d">bold_X</annotation></semantics></math> is first projected onto these weight matrices to get <math id="S2.SS1.p1.11.m11.1" class="ltx_Math" alttext="\mathbf{Q}=\mathbf{X}\mathbf{W}^{Q}" display="inline"><semantics id="S2.SS1.p1.11.m11.1a"><mrow id="S2.SS1.p1.11.m11.1.1" xref="S2.SS1.p1.11.m11.1.1.cmml"><mi id="S2.SS1.p1.11.m11.1.1.2" xref="S2.SS1.p1.11.m11.1.1.2.cmml">𝐐</mi><mo id="S2.SS1.p1.11.m11.1.1.1" xref="S2.SS1.p1.11.m11.1.1.1.cmml">=</mo><msup id="S2.SS1.p1.11.m11.1.1.3" xref="S2.SS1.p1.11.m11.1.1.3.cmml"><mi id="S2.SS1.p1.11.m11.1.1.3.2" xref="S2.SS1.p1.11.m11.1.1.3.2.cmml">𝐗𝐖</mi><mi id="S2.SS1.p1.11.m11.1.1.3.3" xref="S2.SS1.p1.11.m11.1.1.3.3.cmml">Q</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.11.m11.1b"><apply id="S2.SS1.p1.11.m11.1.1.cmml" xref="S2.SS1.p1.11.m11.1.1"><eq id="S2.SS1.p1.11.m11.1.1.1.cmml" xref="S2.SS1.p1.11.m11.1.1.1"></eq><ci id="S2.SS1.p1.11.m11.1.1.2.cmml" xref="S2.SS1.p1.11.m11.1.1.2">𝐐</ci><apply id="S2.SS1.p1.11.m11.1.1.3.cmml" xref="S2.SS1.p1.11.m11.1.1.3"><csymbol cd="ambiguous" id="S2.SS1.p1.11.m11.1.1.3.1.cmml" xref="S2.SS1.p1.11.m11.1.1.3">superscript</csymbol><ci id="S2.SS1.p1.11.m11.1.1.3.2.cmml" xref="S2.SS1.p1.11.m11.1.1.3.2">𝐗𝐖</ci><ci id="S2.SS1.p1.11.m11.1.1.3.3.cmml" xref="S2.SS1.p1.11.m11.1.1.3.3">𝑄</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.11.m11.1c">\mathbf{Q}=\mathbf{X}\mathbf{W}^{Q}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.11.m11.1d">bold_Q = bold_XW start_POSTSUPERSCRIPT italic_Q end_POSTSUPERSCRIPT</annotation></semantics></math>, <math id="S2.SS1.p1.12.m12.1" class="ltx_Math" alttext="\mathbf{K}=\mathbf{X}\mathbf{W}^{K}" display="inline"><semantics id="S2.SS1.p1.12.m12.1a"><mrow id="S2.SS1.p1.12.m12.1.1" xref="S2.SS1.p1.12.m12.1.1.cmml"><mi id="S2.SS1.p1.12.m12.1.1.2" xref="S2.SS1.p1.12.m12.1.1.2.cmml">𝐊</mi><mo id="S2.SS1.p1.12.m12.1.1.1" xref="S2.SS1.p1.12.m12.1.1.1.cmml">=</mo><msup id="S2.SS1.p1.12.m12.1.1.3" xref="S2.SS1.p1.12.m12.1.1.3.cmml"><mi id="S2.SS1.p1.12.m12.1.1.3.2" xref="S2.SS1.p1.12.m12.1.1.3.2.cmml">𝐗𝐖</mi><mi id="S2.SS1.p1.12.m12.1.1.3.3" xref="S2.SS1.p1.12.m12.1.1.3.3.cmml">K</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.12.m12.1b"><apply id="S2.SS1.p1.12.m12.1.1.cmml" xref="S2.SS1.p1.12.m12.1.1"><eq id="S2.SS1.p1.12.m12.1.1.1.cmml" xref="S2.SS1.p1.12.m12.1.1.1"></eq><ci id="S2.SS1.p1.12.m12.1.1.2.cmml" xref="S2.SS1.p1.12.m12.1.1.2">𝐊</ci><apply id="S2.SS1.p1.12.m12.1.1.3.cmml" xref="S2.SS1.p1.12.m12.1.1.3"><csymbol cd="ambiguous" id="S2.SS1.p1.12.m12.1.1.3.1.cmml" xref="S2.SS1.p1.12.m12.1.1.3">superscript</csymbol><ci id="S2.SS1.p1.12.m12.1.1.3.2.cmml" xref="S2.SS1.p1.12.m12.1.1.3.2">𝐗𝐖</ci><ci id="S2.SS1.p1.12.m12.1.1.3.3.cmml" xref="S2.SS1.p1.12.m12.1.1.3.3">𝐾</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.12.m12.1c">\mathbf{K}=\mathbf{X}\mathbf{W}^{K}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.12.m12.1d">bold_K = bold_XW start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT</annotation></semantics></math> and <math id="S2.SS1.p1.13.m13.1" class="ltx_Math" alttext="\mathbf{V}=\mathbf{X}\mathbf{W}^{V}" display="inline"><semantics id="S2.SS1.p1.13.m13.1a"><mrow id="S2.SS1.p1.13.m13.1.1" xref="S2.SS1.p1.13.m13.1.1.cmml"><mi id="S2.SS1.p1.13.m13.1.1.2" xref="S2.SS1.p1.13.m13.1.1.2.cmml">𝐕</mi><mo id="S2.SS1.p1.13.m13.1.1.1" xref="S2.SS1.p1.13.m13.1.1.1.cmml">=</mo><msup id="S2.SS1.p1.13.m13.1.1.3" xref="S2.SS1.p1.13.m13.1.1.3.cmml"><mi id="S2.SS1.p1.13.m13.1.1.3.2" xref="S2.SS1.p1.13.m13.1.1.3.2.cmml">𝐗𝐖</mi><mi id="S2.SS1.p1.13.m13.1.1.3.3" xref="S2.SS1.p1.13.m13.1.1.3.3.cmml">V</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.13.m13.1b"><apply id="S2.SS1.p1.13.m13.1.1.cmml" xref="S2.SS1.p1.13.m13.1.1"><eq id="S2.SS1.p1.13.m13.1.1.1.cmml" xref="S2.SS1.p1.13.m13.1.1.1"></eq><ci id="S2.SS1.p1.13.m13.1.1.2.cmml" xref="S2.SS1.p1.13.m13.1.1.2">𝐕</ci><apply id="S2.SS1.p1.13.m13.1.1.3.cmml" xref="S2.SS1.p1.13.m13.1.1.3"><csymbol cd="ambiguous" id="S2.SS1.p1.13.m13.1.1.3.1.cmml" xref="S2.SS1.p1.13.m13.1.1.3">superscript</csymbol><ci id="S2.SS1.p1.13.m13.1.1.3.2.cmml" xref="S2.SS1.p1.13.m13.1.1.3.2">𝐗𝐖</ci><ci id="S2.SS1.p1.13.m13.1.1.3.3.cmml" xref="S2.SS1.p1.13.m13.1.1.3.3">𝑉</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.13.m13.1c">\mathbf{V}=\mathbf{X}\mathbf{W}^{V}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.13.m13.1d">bold_V = bold_XW start_POSTSUPERSCRIPT italic_V end_POSTSUPERSCRIPT</annotation></semantics></math>. The output <math id="S2.SS1.p1.14.m14.1" class="ltx_Math" alttext="\mathbf{Z}\in\mathbb{R}^{n\times d_{v}}" display="inline"><semantics id="S2.SS1.p1.14.m14.1a"><mrow id="S2.SS1.p1.14.m14.1.1" xref="S2.SS1.p1.14.m14.1.1.cmml"><mi id="S2.SS1.p1.14.m14.1.1.2" xref="S2.SS1.p1.14.m14.1.1.2.cmml">𝐙</mi><mo id="S2.SS1.p1.14.m14.1.1.1" xref="S2.SS1.p1.14.m14.1.1.1.cmml">∈</mo><msup id="S2.SS1.p1.14.m14.1.1.3" xref="S2.SS1.p1.14.m14.1.1.3.cmml"><mi id="S2.SS1.p1.14.m14.1.1.3.2" xref="S2.SS1.p1.14.m14.1.1.3.2.cmml">ℝ</mi><mrow id="S2.SS1.p1.14.m14.1.1.3.3" xref="S2.SS1.p1.14.m14.1.1.3.3.cmml"><mi id="S2.SS1.p1.14.m14.1.1.3.3.2" xref="S2.SS1.p1.14.m14.1.1.3.3.2.cmml">n</mi><mo lspace="0.222em" rspace="0.222em" id="S2.SS1.p1.14.m14.1.1.3.3.1" xref="S2.SS1.p1.14.m14.1.1.3.3.1.cmml">×</mo><msub id="S2.SS1.p1.14.m14.1.1.3.3.3" xref="S2.SS1.p1.14.m14.1.1.3.3.3.cmml"><mi id="S2.SS1.p1.14.m14.1.1.3.3.3.2" xref="S2.SS1.p1.14.m14.1.1.3.3.3.2.cmml">d</mi><mi id="S2.SS1.p1.14.m14.1.1.3.3.3.3" xref="S2.SS1.p1.14.m14.1.1.3.3.3.3.cmml">v</mi></msub></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.14.m14.1b"><apply id="S2.SS1.p1.14.m14.1.1.cmml" xref="S2.SS1.p1.14.m14.1.1"><in id="S2.SS1.p1.14.m14.1.1.1.cmml" xref="S2.SS1.p1.14.m14.1.1.1"></in><ci id="S2.SS1.p1.14.m14.1.1.2.cmml" xref="S2.SS1.p1.14.m14.1.1.2">𝐙</ci><apply id="S2.SS1.p1.14.m14.1.1.3.cmml" xref="S2.SS1.p1.14.m14.1.1.3"><csymbol cd="ambiguous" id="S2.SS1.p1.14.m14.1.1.3.1.cmml" xref="S2.SS1.p1.14.m14.1.1.3">superscript</csymbol><ci id="S2.SS1.p1.14.m14.1.1.3.2.cmml" xref="S2.SS1.p1.14.m14.1.1.3.2">ℝ</ci><apply id="S2.SS1.p1.14.m14.1.1.3.3.cmml" xref="S2.SS1.p1.14.m14.1.1.3.3"><times id="S2.SS1.p1.14.m14.1.1.3.3.1.cmml" xref="S2.SS1.p1.14.m14.1.1.3.3.1"></times><ci id="S2.SS1.p1.14.m14.1.1.3.3.2.cmml" xref="S2.SS1.p1.14.m14.1.1.3.3.2">𝑛</ci><apply id="S2.SS1.p1.14.m14.1.1.3.3.3.cmml" xref="S2.SS1.p1.14.m14.1.1.3.3.3"><csymbol cd="ambiguous" id="S2.SS1.p1.14.m14.1.1.3.3.3.1.cmml" xref="S2.SS1.p1.14.m14.1.1.3.3.3">subscript</csymbol><ci id="S2.SS1.p1.14.m14.1.1.3.3.3.2.cmml" xref="S2.SS1.p1.14.m14.1.1.3.3.3.2">𝑑</ci><ci id="S2.SS1.p1.14.m14.1.1.3.3.3.3.cmml" xref="S2.SS1.p1.14.m14.1.1.3.3.3.3">𝑣</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.14.m14.1c">\mathbf{Z}\in\mathbb{R}^{n\times d_{v}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.14.m14.1d">bold_Z ∈ blackboard_R start_POSTSUPERSCRIPT italic_n × italic_d start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT end_POSTSUPERSCRIPT</annotation></semantics></math> of the self attention layer is,</p>
<table id="S2.Ex1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.Ex1.m1.2" class="ltx_Math" alttext="\mathbf{Z}=\mathbf{softmax}\left(\frac{\mathbf{Q}\mathbf{K}^{T}}{\sqrt{d_{q}}}\right)\mathbf{V}." display="block"><semantics id="S2.Ex1.m1.2a"><mrow id="S2.Ex1.m1.2.2.1" xref="S2.Ex1.m1.2.2.1.1.cmml"><mrow id="S2.Ex1.m1.2.2.1.1" xref="S2.Ex1.m1.2.2.1.1.cmml"><mi id="S2.Ex1.m1.2.2.1.1.2" xref="S2.Ex1.m1.2.2.1.1.2.cmml">𝐙</mi><mo id="S2.Ex1.m1.2.2.1.1.1" xref="S2.Ex1.m1.2.2.1.1.1.cmml">=</mo><mrow id="S2.Ex1.m1.2.2.1.1.3" xref="S2.Ex1.m1.2.2.1.1.3.cmml"><mi id="S2.Ex1.m1.2.2.1.1.3.2" xref="S2.Ex1.m1.2.2.1.1.3.2.cmml">𝐬𝐨𝐟𝐭𝐦𝐚𝐱</mi><mo id="S2.Ex1.m1.2.2.1.1.3.1" xref="S2.Ex1.m1.2.2.1.1.3.1.cmml" lspace='0px' rspace='0px'></mo><mrow id="S2.Ex1.m1.2.2.1.1.3.3.2" xref="S2.Ex1.m1.1.1.cmml"><mo id="S2.Ex1.m1.2.2.1.1.3.3.2.1" xref="S2.Ex1.m1.1.1.cmml">(</mo><mfrac id="S2.Ex1.m1.1.1" xref="S2.Ex1.m1.1.1.cmml"><msup id="S2.Ex1.m1.1.1.2" xref="S2.Ex1.m1.1.1.2.cmml"><mi id="S2.Ex1.m1.1.1.2.2" xref="S2.Ex1.m1.1.1.2.2.cmml">𝐐𝐊</mi><mi id="S2.Ex1.m1.1.1.2.3" xref="S2.Ex1.m1.1.1.2.3.cmml">T</mi></msup><msqrt id="S2.Ex1.m1.1.1.3" xref="S2.Ex1.m1.1.1.3.cmml"><msub id="S2.Ex1.m1.1.1.3.2" xref="S2.Ex1.m1.1.1.3.2.cmml"><mi id="S2.Ex1.m1.1.1.3.2.2" xref="S2.Ex1.m1.1.1.3.2.2.cmml">d</mi><mi id="S2.Ex1.m1.1.1.3.2.3" xref="S2.Ex1.m1.1.1.3.2.3.cmml">q</mi></msub></msqrt></mfrac><mo id="S2.Ex1.m1.2.2.1.1.3.3.2.2" xref="S2.Ex1.m1.1.1.cmml">)</mo></mrow><mo id="S2.Ex1.m1.2.2.1.1.3.1a" xref="S2.Ex1.m1.2.2.1.1.3.1.cmml" lspace='0px' rspace='0px'></mo><mi id="S2.Ex1.m1.2.2.1.1.3.4" xref="S2.Ex1.m1.2.2.1.1.3.4.cmml">𝐕</mi></mrow></mrow><mo lspace="0em" id="S2.Ex1.m1.2.2.1.2" xref="S2.Ex1.m1.2.2.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.Ex1.m1.2b"><apply id="S2.Ex1.m1.2.2.1.1.cmml" xref="S2.Ex1.m1.2.2.1"><eq id="S2.Ex1.m1.2.2.1.1.1.cmml" xref="S2.Ex1.m1.2.2.1.1.1"></eq><ci id="S2.Ex1.m1.2.2.1.1.2.cmml" xref="S2.Ex1.m1.2.2.1.1.2">𝐙</ci><apply id="S2.Ex1.m1.2.2.1.1.3.cmml" xref="S2.Ex1.m1.2.2.1.1.3"><times id="S2.Ex1.m1.2.2.1.1.3.1.cmml" xref="S2.Ex1.m1.2.2.1.1.3.1"></times><ci id="S2.Ex1.m1.2.2.1.1.3.2.cmml" xref="S2.Ex1.m1.2.2.1.1.3.2">𝐬𝐨𝐟𝐭𝐦𝐚𝐱</ci><apply id="S2.Ex1.m1.1.1.cmml" xref="S2.Ex1.m1.2.2.1.1.3.3.2"><divide id="S2.Ex1.m1.1.1.1.cmml" xref="S2.Ex1.m1.2.2.1.1.3.3.2"></divide><apply id="S2.Ex1.m1.1.1.2.cmml" xref="S2.Ex1.m1.1.1.2"><csymbol cd="ambiguous" id="S2.Ex1.m1.1.1.2.1.cmml" xref="S2.Ex1.m1.1.1.2">superscript</csymbol><ci id="S2.Ex1.m1.1.1.2.2.cmml" xref="S2.Ex1.m1.1.1.2.2">𝐐𝐊</ci><ci id="S2.Ex1.m1.1.1.2.3.cmml" xref="S2.Ex1.m1.1.1.2.3">𝑇</ci></apply><apply id="S2.Ex1.m1.1.1.3.cmml" xref="S2.Ex1.m1.1.1.3"><root id="S2.Ex1.m1.1.1.3a.cmml" xref="S2.Ex1.m1.1.1.3"></root><apply id="S2.Ex1.m1.1.1.3.2.cmml" xref="S2.Ex1.m1.1.1.3.2"><csymbol cd="ambiguous" id="S2.Ex1.m1.1.1.3.2.1.cmml" xref="S2.Ex1.m1.1.1.3.2">subscript</csymbol><ci id="S2.Ex1.m1.1.1.3.2.2.cmml" xref="S2.Ex1.m1.1.1.3.2.2">𝑑</ci><ci id="S2.Ex1.m1.1.1.3.2.3.cmml" xref="S2.Ex1.m1.1.1.3.2.3">𝑞</ci></apply></apply></apply><ci id="S2.Ex1.m1.2.2.1.1.3.4.cmml" xref="S2.Ex1.m1.2.2.1.1.3.4">𝐕</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex1.m1.2c">\mathbf{Z}=\mathbf{softmax}\left(\frac{\mathbf{Q}\mathbf{K}^{T}}{\sqrt{d_{q}}}\right)\mathbf{V}.</annotation><annotation encoding="application/x-llamapun" id="S2.Ex1.m1.2d">bold_Z = bold_softmax ( divide start_ARG bold_QK start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT end_ARG start_ARG square-root start_ARG italic_d start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT end_ARG end_ARG ) bold_V .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S2.SS1.p1.15" class="ltx_p">For a given entity in the sequence, the self-attention basically computes the dot-product of the query with all keys, which is then normalized using softmax operator to get the attention scores. Each entity then becomes the weighted sum of all entities in the sequence, where weights are given by the attention scores (Fig. <a href="#S2.F2" title="Figure 2 ‣ 2 Foundations ‣ Transformers in Vision: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> and Fig. <a href="#S2.F3" title="Figure 3 ‣ 2 Foundations ‣ Transformers in Vision: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, top row-left block).</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.2" class="ltx_p"><span id="S2.SS1.p2.2.1" class="ltx_text ltx_font_bold">Masked Self-Attention:</span> The standard self-attention layer attends to all entities. For the Transformer model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> which is trained to predict the next entity of the sequence, the self-attention blocks used in the decoder are masked to prevent attending to the subsequent future entities. This is simply done by an element-wise multiplication operation with a mask <math id="S2.SS1.p2.1.m1.1" class="ltx_Math" alttext="\mathbf{M}\in\mathbb{R}^{n\times n}" display="inline"><semantics id="S2.SS1.p2.1.m1.1a"><mrow id="S2.SS1.p2.1.m1.1.1" xref="S2.SS1.p2.1.m1.1.1.cmml"><mi id="S2.SS1.p2.1.m1.1.1.2" xref="S2.SS1.p2.1.m1.1.1.2.cmml">𝐌</mi><mo id="S2.SS1.p2.1.m1.1.1.1" xref="S2.SS1.p2.1.m1.1.1.1.cmml">∈</mo><msup id="S2.SS1.p2.1.m1.1.1.3" xref="S2.SS1.p2.1.m1.1.1.3.cmml"><mi id="S2.SS1.p2.1.m1.1.1.3.2" xref="S2.SS1.p2.1.m1.1.1.3.2.cmml">ℝ</mi><mrow id="S2.SS1.p2.1.m1.1.1.3.3" xref="S2.SS1.p2.1.m1.1.1.3.3.cmml"><mi id="S2.SS1.p2.1.m1.1.1.3.3.2" xref="S2.SS1.p2.1.m1.1.1.3.3.2.cmml">n</mi><mo lspace="0.222em" rspace="0.222em" id="S2.SS1.p2.1.m1.1.1.3.3.1" xref="S2.SS1.p2.1.m1.1.1.3.3.1.cmml">×</mo><mi id="S2.SS1.p2.1.m1.1.1.3.3.3" xref="S2.SS1.p2.1.m1.1.1.3.3.3.cmml">n</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.1.m1.1b"><apply id="S2.SS1.p2.1.m1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1"><in id="S2.SS1.p2.1.m1.1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1.1"></in><ci id="S2.SS1.p2.1.m1.1.1.2.cmml" xref="S2.SS1.p2.1.m1.1.1.2">𝐌</ci><apply id="S2.SS1.p2.1.m1.1.1.3.cmml" xref="S2.SS1.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S2.SS1.p2.1.m1.1.1.3.1.cmml" xref="S2.SS1.p2.1.m1.1.1.3">superscript</csymbol><ci id="S2.SS1.p2.1.m1.1.1.3.2.cmml" xref="S2.SS1.p2.1.m1.1.1.3.2">ℝ</ci><apply id="S2.SS1.p2.1.m1.1.1.3.3.cmml" xref="S2.SS1.p2.1.m1.1.1.3.3"><times id="S2.SS1.p2.1.m1.1.1.3.3.1.cmml" xref="S2.SS1.p2.1.m1.1.1.3.3.1"></times><ci id="S2.SS1.p2.1.m1.1.1.3.3.2.cmml" xref="S2.SS1.p2.1.m1.1.1.3.3.2">𝑛</ci><ci id="S2.SS1.p2.1.m1.1.1.3.3.3.cmml" xref="S2.SS1.p2.1.m1.1.1.3.3.3">𝑛</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.1.m1.1c">\mathbf{M}\in\mathbb{R}^{n\times n}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.1.m1.1d">bold_M ∈ blackboard_R start_POSTSUPERSCRIPT italic_n × italic_n end_POSTSUPERSCRIPT</annotation></semantics></math>, where <math id="S2.SS1.p2.2.m2.1" class="ltx_Math" alttext="\mathbf{M}" display="inline"><semantics id="S2.SS1.p2.2.m2.1a"><mi id="S2.SS1.p2.2.m2.1.1" xref="S2.SS1.p2.2.m2.1.1.cmml">𝐌</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.2.m2.1b"><ci id="S2.SS1.p2.2.m2.1.1.cmml" xref="S2.SS1.p2.2.m2.1.1">𝐌</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.2.m2.1c">\mathbf{M}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.2.m2.1d">bold_M</annotation></semantics></math> is an upper-triangular matrix. The masked self-attention is defined by,</p>
<table id="S2.Ex2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.Ex2.m1.1" class="ltx_Math" alttext="\mathbf{softmax}\left(\frac{\mathbf{Q}\mathbf{K}^{T}}{\sqrt{d_{q}}}\circ\mathbf{M}\right)," display="block"><semantics id="S2.Ex2.m1.1a"><mrow id="S2.Ex2.m1.1.1.1" xref="S2.Ex2.m1.1.1.1.1.cmml"><mrow id="S2.Ex2.m1.1.1.1.1" xref="S2.Ex2.m1.1.1.1.1.cmml"><mi id="S2.Ex2.m1.1.1.1.1.3" xref="S2.Ex2.m1.1.1.1.1.3.cmml">𝐬𝐨𝐟𝐭𝐦𝐚𝐱</mi><mo id="S2.Ex2.m1.1.1.1.1.2" xref="S2.Ex2.m1.1.1.1.1.2.cmml" lspace='0px' rspace='0px'></mo><mrow id="S2.Ex2.m1.1.1.1.1.1.1" xref="S2.Ex2.m1.1.1.1.1.1.1.1.cmml"><mo id="S2.Ex2.m1.1.1.1.1.1.1.2" xref="S2.Ex2.m1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.Ex2.m1.1.1.1.1.1.1.1" xref="S2.Ex2.m1.1.1.1.1.1.1.1.cmml"><mfrac id="S2.Ex2.m1.1.1.1.1.1.1.1.2" xref="S2.Ex2.m1.1.1.1.1.1.1.1.2.cmml"><msup id="S2.Ex2.m1.1.1.1.1.1.1.1.2.2" xref="S2.Ex2.m1.1.1.1.1.1.1.1.2.2.cmml"><mi id="S2.Ex2.m1.1.1.1.1.1.1.1.2.2.2" xref="S2.Ex2.m1.1.1.1.1.1.1.1.2.2.2.cmml">𝐐𝐊</mi><mi id="S2.Ex2.m1.1.1.1.1.1.1.1.2.2.3" xref="S2.Ex2.m1.1.1.1.1.1.1.1.2.2.3.cmml">T</mi></msup><msqrt id="S2.Ex2.m1.1.1.1.1.1.1.1.2.3" xref="S2.Ex2.m1.1.1.1.1.1.1.1.2.3.cmml"><msub id="S2.Ex2.m1.1.1.1.1.1.1.1.2.3.2" xref="S2.Ex2.m1.1.1.1.1.1.1.1.2.3.2.cmml"><mi id="S2.Ex2.m1.1.1.1.1.1.1.1.2.3.2.2" xref="S2.Ex2.m1.1.1.1.1.1.1.1.2.3.2.2.cmml">d</mi><mi id="S2.Ex2.m1.1.1.1.1.1.1.1.2.3.2.3" xref="S2.Ex2.m1.1.1.1.1.1.1.1.2.3.2.3.cmml">q</mi></msub></msqrt></mfrac><mo lspace="0.222em" rspace="0.222em" id="S2.Ex2.m1.1.1.1.1.1.1.1.1" xref="S2.Ex2.m1.1.1.1.1.1.1.1.1.cmml">∘</mo><mi id="S2.Ex2.m1.1.1.1.1.1.1.1.3" xref="S2.Ex2.m1.1.1.1.1.1.1.1.3.cmml">𝐌</mi></mrow><mo id="S2.Ex2.m1.1.1.1.1.1.1.3" xref="S2.Ex2.m1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.Ex2.m1.1.1.1.2" xref="S2.Ex2.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.Ex2.m1.1b"><apply id="S2.Ex2.m1.1.1.1.1.cmml" xref="S2.Ex2.m1.1.1.1"><times id="S2.Ex2.m1.1.1.1.1.2.cmml" xref="S2.Ex2.m1.1.1.1.1.2"></times><ci id="S2.Ex2.m1.1.1.1.1.3.cmml" xref="S2.Ex2.m1.1.1.1.1.3">𝐬𝐨𝐟𝐭𝐦𝐚𝐱</ci><apply id="S2.Ex2.m1.1.1.1.1.1.1.1.cmml" xref="S2.Ex2.m1.1.1.1.1.1.1"><compose id="S2.Ex2.m1.1.1.1.1.1.1.1.1.cmml" xref="S2.Ex2.m1.1.1.1.1.1.1.1.1"></compose><apply id="S2.Ex2.m1.1.1.1.1.1.1.1.2.cmml" xref="S2.Ex2.m1.1.1.1.1.1.1.1.2"><divide id="S2.Ex2.m1.1.1.1.1.1.1.1.2.1.cmml" xref="S2.Ex2.m1.1.1.1.1.1.1.1.2"></divide><apply id="S2.Ex2.m1.1.1.1.1.1.1.1.2.2.cmml" xref="S2.Ex2.m1.1.1.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S2.Ex2.m1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S2.Ex2.m1.1.1.1.1.1.1.1.2.2">superscript</csymbol><ci id="S2.Ex2.m1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S2.Ex2.m1.1.1.1.1.1.1.1.2.2.2">𝐐𝐊</ci><ci id="S2.Ex2.m1.1.1.1.1.1.1.1.2.2.3.cmml" xref="S2.Ex2.m1.1.1.1.1.1.1.1.2.2.3">𝑇</ci></apply><apply id="S2.Ex2.m1.1.1.1.1.1.1.1.2.3.cmml" xref="S2.Ex2.m1.1.1.1.1.1.1.1.2.3"><root id="S2.Ex2.m1.1.1.1.1.1.1.1.2.3a.cmml" xref="S2.Ex2.m1.1.1.1.1.1.1.1.2.3"></root><apply id="S2.Ex2.m1.1.1.1.1.1.1.1.2.3.2.cmml" xref="S2.Ex2.m1.1.1.1.1.1.1.1.2.3.2"><csymbol cd="ambiguous" id="S2.Ex2.m1.1.1.1.1.1.1.1.2.3.2.1.cmml" xref="S2.Ex2.m1.1.1.1.1.1.1.1.2.3.2">subscript</csymbol><ci id="S2.Ex2.m1.1.1.1.1.1.1.1.2.3.2.2.cmml" xref="S2.Ex2.m1.1.1.1.1.1.1.1.2.3.2.2">𝑑</ci><ci id="S2.Ex2.m1.1.1.1.1.1.1.1.2.3.2.3.cmml" xref="S2.Ex2.m1.1.1.1.1.1.1.1.2.3.2.3">𝑞</ci></apply></apply></apply><ci id="S2.Ex2.m1.1.1.1.1.1.1.1.3.cmml" xref="S2.Ex2.m1.1.1.1.1.1.1.1.3">𝐌</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex2.m1.1c">\mathbf{softmax}\left(\frac{\mathbf{Q}\mathbf{K}^{T}}{\sqrt{d_{q}}}\circ\mathbf{M}\right),</annotation><annotation encoding="application/x-llamapun" id="S2.Ex2.m1.1d">bold_softmax ( divide start_ARG bold_QK start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT end_ARG start_ARG square-root start_ARG italic_d start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT end_ARG end_ARG ∘ bold_M ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S2.SS1.p2.3" class="ltx_p">where <math id="S2.SS1.p2.3.m1.1" class="ltx_Math" alttext="\circ" display="inline"><semantics id="S2.SS1.p2.3.m1.1a"><mo id="S2.SS1.p2.3.m1.1.1" xref="S2.SS1.p2.3.m1.1.1.cmml">∘</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.3.m1.1b"><compose id="S2.SS1.p2.3.m1.1.1.cmml" xref="S2.SS1.p2.3.m1.1.1"></compose></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.3.m1.1c">\circ</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.3.m1.1d">∘</annotation></semantics></math> denotes Hadamard product. Basically, while predicting an entity in the sequence, the attention scores of the future entities are set to zero in masked self-attention.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.7" class="ltx_p"><span id="S2.SS1.p3.7.1" class="ltx_text ltx_font_bold">Multi-Head Attention:</span>
In order to encapsulate multiple complex relationships amongst different elements in the sequence, the multi-head attention comprises multiple self-attention blocks (<math id="S2.SS1.p3.1.m1.1" class="ltx_Math" alttext="h=8" display="inline"><semantics id="S2.SS1.p3.1.m1.1a"><mrow id="S2.SS1.p3.1.m1.1.1" xref="S2.SS1.p3.1.m1.1.1.cmml"><mi id="S2.SS1.p3.1.m1.1.1.2" xref="S2.SS1.p3.1.m1.1.1.2.cmml">h</mi><mo id="S2.SS1.p3.1.m1.1.1.1" xref="S2.SS1.p3.1.m1.1.1.1.cmml">=</mo><mn id="S2.SS1.p3.1.m1.1.1.3" xref="S2.SS1.p3.1.m1.1.1.3.cmml">8</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.1.m1.1b"><apply id="S2.SS1.p3.1.m1.1.1.cmml" xref="S2.SS1.p3.1.m1.1.1"><eq id="S2.SS1.p3.1.m1.1.1.1.cmml" xref="S2.SS1.p3.1.m1.1.1.1"></eq><ci id="S2.SS1.p3.1.m1.1.1.2.cmml" xref="S2.SS1.p3.1.m1.1.1.2">ℎ</ci><cn type="integer" id="S2.SS1.p3.1.m1.1.1.3.cmml" xref="S2.SS1.p3.1.m1.1.1.3">8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.1.m1.1c">h=8</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p3.1.m1.1d">italic_h = 8</annotation></semantics></math> in the original Transformer model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>). Each block has its own set of learnable weight matrices <math id="S2.SS1.p3.2.m2.3" class="ltx_Math" alttext="\{\mathbf{W}^{Q_{i}},\mathbf{W}^{K_{i}},\mathbf{W}^{V_{i}}\}" display="inline"><semantics id="S2.SS1.p3.2.m2.3a"><mrow id="S2.SS1.p3.2.m2.3.3.3" xref="S2.SS1.p3.2.m2.3.3.4.cmml"><mo stretchy="false" id="S2.SS1.p3.2.m2.3.3.3.4" xref="S2.SS1.p3.2.m2.3.3.4.cmml">{</mo><msup id="S2.SS1.p3.2.m2.1.1.1.1" xref="S2.SS1.p3.2.m2.1.1.1.1.cmml"><mi id="S2.SS1.p3.2.m2.1.1.1.1.2" xref="S2.SS1.p3.2.m2.1.1.1.1.2.cmml">𝐖</mi><msub id="S2.SS1.p3.2.m2.1.1.1.1.3" xref="S2.SS1.p3.2.m2.1.1.1.1.3.cmml"><mi id="S2.SS1.p3.2.m2.1.1.1.1.3.2" xref="S2.SS1.p3.2.m2.1.1.1.1.3.2.cmml">Q</mi><mi id="S2.SS1.p3.2.m2.1.1.1.1.3.3" xref="S2.SS1.p3.2.m2.1.1.1.1.3.3.cmml">i</mi></msub></msup><mo id="S2.SS1.p3.2.m2.3.3.3.5" xref="S2.SS1.p3.2.m2.3.3.4.cmml">,</mo><msup id="S2.SS1.p3.2.m2.2.2.2.2" xref="S2.SS1.p3.2.m2.2.2.2.2.cmml"><mi id="S2.SS1.p3.2.m2.2.2.2.2.2" xref="S2.SS1.p3.2.m2.2.2.2.2.2.cmml">𝐖</mi><msub id="S2.SS1.p3.2.m2.2.2.2.2.3" xref="S2.SS1.p3.2.m2.2.2.2.2.3.cmml"><mi id="S2.SS1.p3.2.m2.2.2.2.2.3.2" xref="S2.SS1.p3.2.m2.2.2.2.2.3.2.cmml">K</mi><mi id="S2.SS1.p3.2.m2.2.2.2.2.3.3" xref="S2.SS1.p3.2.m2.2.2.2.2.3.3.cmml">i</mi></msub></msup><mo id="S2.SS1.p3.2.m2.3.3.3.6" xref="S2.SS1.p3.2.m2.3.3.4.cmml">,</mo><msup id="S2.SS1.p3.2.m2.3.3.3.3" xref="S2.SS1.p3.2.m2.3.3.3.3.cmml"><mi id="S2.SS1.p3.2.m2.3.3.3.3.2" xref="S2.SS1.p3.2.m2.3.3.3.3.2.cmml">𝐖</mi><msub id="S2.SS1.p3.2.m2.3.3.3.3.3" xref="S2.SS1.p3.2.m2.3.3.3.3.3.cmml"><mi id="S2.SS1.p3.2.m2.3.3.3.3.3.2" xref="S2.SS1.p3.2.m2.3.3.3.3.3.2.cmml">V</mi><mi id="S2.SS1.p3.2.m2.3.3.3.3.3.3" xref="S2.SS1.p3.2.m2.3.3.3.3.3.3.cmml">i</mi></msub></msup><mo stretchy="false" id="S2.SS1.p3.2.m2.3.3.3.7" xref="S2.SS1.p3.2.m2.3.3.4.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.2.m2.3b"><set id="S2.SS1.p3.2.m2.3.3.4.cmml" xref="S2.SS1.p3.2.m2.3.3.3"><apply id="S2.SS1.p3.2.m2.1.1.1.1.cmml" xref="S2.SS1.p3.2.m2.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p3.2.m2.1.1.1.1.1.cmml" xref="S2.SS1.p3.2.m2.1.1.1.1">superscript</csymbol><ci id="S2.SS1.p3.2.m2.1.1.1.1.2.cmml" xref="S2.SS1.p3.2.m2.1.1.1.1.2">𝐖</ci><apply id="S2.SS1.p3.2.m2.1.1.1.1.3.cmml" xref="S2.SS1.p3.2.m2.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.SS1.p3.2.m2.1.1.1.1.3.1.cmml" xref="S2.SS1.p3.2.m2.1.1.1.1.3">subscript</csymbol><ci id="S2.SS1.p3.2.m2.1.1.1.1.3.2.cmml" xref="S2.SS1.p3.2.m2.1.1.1.1.3.2">𝑄</ci><ci id="S2.SS1.p3.2.m2.1.1.1.1.3.3.cmml" xref="S2.SS1.p3.2.m2.1.1.1.1.3.3">𝑖</ci></apply></apply><apply id="S2.SS1.p3.2.m2.2.2.2.2.cmml" xref="S2.SS1.p3.2.m2.2.2.2.2"><csymbol cd="ambiguous" id="S2.SS1.p3.2.m2.2.2.2.2.1.cmml" xref="S2.SS1.p3.2.m2.2.2.2.2">superscript</csymbol><ci id="S2.SS1.p3.2.m2.2.2.2.2.2.cmml" xref="S2.SS1.p3.2.m2.2.2.2.2.2">𝐖</ci><apply id="S2.SS1.p3.2.m2.2.2.2.2.3.cmml" xref="S2.SS1.p3.2.m2.2.2.2.2.3"><csymbol cd="ambiguous" id="S2.SS1.p3.2.m2.2.2.2.2.3.1.cmml" xref="S2.SS1.p3.2.m2.2.2.2.2.3">subscript</csymbol><ci id="S2.SS1.p3.2.m2.2.2.2.2.3.2.cmml" xref="S2.SS1.p3.2.m2.2.2.2.2.3.2">𝐾</ci><ci id="S2.SS1.p3.2.m2.2.2.2.2.3.3.cmml" xref="S2.SS1.p3.2.m2.2.2.2.2.3.3">𝑖</ci></apply></apply><apply id="S2.SS1.p3.2.m2.3.3.3.3.cmml" xref="S2.SS1.p3.2.m2.3.3.3.3"><csymbol cd="ambiguous" id="S2.SS1.p3.2.m2.3.3.3.3.1.cmml" xref="S2.SS1.p3.2.m2.3.3.3.3">superscript</csymbol><ci id="S2.SS1.p3.2.m2.3.3.3.3.2.cmml" xref="S2.SS1.p3.2.m2.3.3.3.3.2">𝐖</ci><apply id="S2.SS1.p3.2.m2.3.3.3.3.3.cmml" xref="S2.SS1.p3.2.m2.3.3.3.3.3"><csymbol cd="ambiguous" id="S2.SS1.p3.2.m2.3.3.3.3.3.1.cmml" xref="S2.SS1.p3.2.m2.3.3.3.3.3">subscript</csymbol><ci id="S2.SS1.p3.2.m2.3.3.3.3.3.2.cmml" xref="S2.SS1.p3.2.m2.3.3.3.3.3.2">𝑉</ci><ci id="S2.SS1.p3.2.m2.3.3.3.3.3.3.cmml" xref="S2.SS1.p3.2.m2.3.3.3.3.3.3">𝑖</ci></apply></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.2.m2.3c">\{\mathbf{W}^{Q_{i}},\mathbf{W}^{K_{i}},\mathbf{W}^{V_{i}}\}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p3.2.m2.3d">{ bold_W start_POSTSUPERSCRIPT italic_Q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUPERSCRIPT , bold_W start_POSTSUPERSCRIPT italic_K start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUPERSCRIPT , bold_W start_POSTSUPERSCRIPT italic_V start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUPERSCRIPT }</annotation></semantics></math>, where <math id="S2.SS1.p3.3.m3.1" class="ltx_Math" alttext="i=0\cdots(h{-}1)" display="inline"><semantics id="S2.SS1.p3.3.m3.1a"><mrow id="S2.SS1.p3.3.m3.1.1" xref="S2.SS1.p3.3.m3.1.1.cmml"><mi id="S2.SS1.p3.3.m3.1.1.3" xref="S2.SS1.p3.3.m3.1.1.3.cmml">i</mi><mo id="S2.SS1.p3.3.m3.1.1.2" xref="S2.SS1.p3.3.m3.1.1.2.cmml">=</mo><mrow id="S2.SS1.p3.3.m3.1.1.1" xref="S2.SS1.p3.3.m3.1.1.1.cmml"><mn id="S2.SS1.p3.3.m3.1.1.1.3" xref="S2.SS1.p3.3.m3.1.1.1.3.cmml">0</mn><mo id="S2.SS1.p3.3.m3.1.1.1.2" xref="S2.SS1.p3.3.m3.1.1.1.2.cmml" lspace='0px' rspace='0px'></mo><mi mathvariant="normal" id="S2.SS1.p3.3.m3.1.1.1.4" xref="S2.SS1.p3.3.m3.1.1.1.4.cmml">⋯</mi><mo id="S2.SS1.p3.3.m3.1.1.1.2a" xref="S2.SS1.p3.3.m3.1.1.1.2.cmml" lspace='0px' rspace='0px'></mo><mrow id="S2.SS1.p3.3.m3.1.1.1.1.1" xref="S2.SS1.p3.3.m3.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.SS1.p3.3.m3.1.1.1.1.1.2" xref="S2.SS1.p3.3.m3.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.SS1.p3.3.m3.1.1.1.1.1.1" xref="S2.SS1.p3.3.m3.1.1.1.1.1.1.cmml"><mi id="S2.SS1.p3.3.m3.1.1.1.1.1.1.2" xref="S2.SS1.p3.3.m3.1.1.1.1.1.1.2.cmml">h</mi><mo id="S2.SS1.p3.3.m3.1.1.1.1.1.1.1" xref="S2.SS1.p3.3.m3.1.1.1.1.1.1.1.cmml">−</mo><mn id="S2.SS1.p3.3.m3.1.1.1.1.1.1.3" xref="S2.SS1.p3.3.m3.1.1.1.1.1.1.3.cmml">1</mn></mrow><mo stretchy="false" id="S2.SS1.p3.3.m3.1.1.1.1.1.3" xref="S2.SS1.p3.3.m3.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.3.m3.1b"><apply id="S2.SS1.p3.3.m3.1.1.cmml" xref="S2.SS1.p3.3.m3.1.1"><eq id="S2.SS1.p3.3.m3.1.1.2.cmml" xref="S2.SS1.p3.3.m3.1.1.2"></eq><ci id="S2.SS1.p3.3.m3.1.1.3.cmml" xref="S2.SS1.p3.3.m3.1.1.3">𝑖</ci><apply id="S2.SS1.p3.3.m3.1.1.1.cmml" xref="S2.SS1.p3.3.m3.1.1.1"><times id="S2.SS1.p3.3.m3.1.1.1.2.cmml" xref="S2.SS1.p3.3.m3.1.1.1.2"></times><cn type="integer" id="S2.SS1.p3.3.m3.1.1.1.3.cmml" xref="S2.SS1.p3.3.m3.1.1.1.3">0</cn><ci id="S2.SS1.p3.3.m3.1.1.1.4.cmml" xref="S2.SS1.p3.3.m3.1.1.1.4">⋯</ci><apply id="S2.SS1.p3.3.m3.1.1.1.1.1.1.cmml" xref="S2.SS1.p3.3.m3.1.1.1.1.1"><minus id="S2.SS1.p3.3.m3.1.1.1.1.1.1.1.cmml" xref="S2.SS1.p3.3.m3.1.1.1.1.1.1.1"></minus><ci id="S2.SS1.p3.3.m3.1.1.1.1.1.1.2.cmml" xref="S2.SS1.p3.3.m3.1.1.1.1.1.1.2">ℎ</ci><cn type="integer" id="S2.SS1.p3.3.m3.1.1.1.1.1.1.3.cmml" xref="S2.SS1.p3.3.m3.1.1.1.1.1.1.3">1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.3.m3.1c">i=0\cdots(h{-}1)</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p3.3.m3.1d">italic_i = 0 ⋯ ( italic_h - 1 )</annotation></semantics></math>. For an input <math id="S2.SS1.p3.4.m4.1" class="ltx_Math" alttext="\mathbf{X}" display="inline"><semantics id="S2.SS1.p3.4.m4.1a"><mi id="S2.SS1.p3.4.m4.1.1" xref="S2.SS1.p3.4.m4.1.1.cmml">𝐗</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.4.m4.1b"><ci id="S2.SS1.p3.4.m4.1.1.cmml" xref="S2.SS1.p3.4.m4.1.1">𝐗</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.4.m4.1c">\mathbf{X}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p3.4.m4.1d">bold_X</annotation></semantics></math>, the output of the <math id="S2.SS1.p3.5.m5.1" class="ltx_Math" alttext="h" display="inline"><semantics id="S2.SS1.p3.5.m5.1a"><mi id="S2.SS1.p3.5.m5.1.1" xref="S2.SS1.p3.5.m5.1.1.cmml">h</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.5.m5.1b"><ci id="S2.SS1.p3.5.m5.1.1.cmml" xref="S2.SS1.p3.5.m5.1.1">ℎ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.5.m5.1c">h</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p3.5.m5.1d">italic_h</annotation></semantics></math> self-attention blocks in multi-head attention is then concatenated into a single matrix <math id="S2.SS1.p3.6.m6.3" class="ltx_Math" alttext="[\mathbf{Z}_{0},\mathbf{Z}_{1},\cdots\mathbf{Z}_{h-1}]\in\mathbb{R}^{n\times h\cdot d_{v}}" display="inline"><semantics id="S2.SS1.p3.6.m6.3a"><mrow id="S2.SS1.p3.6.m6.3.3" xref="S2.SS1.p3.6.m6.3.3.cmml"><mrow id="S2.SS1.p3.6.m6.3.3.3.3" xref="S2.SS1.p3.6.m6.3.3.3.4.cmml"><mo stretchy="false" id="S2.SS1.p3.6.m6.3.3.3.3.4" xref="S2.SS1.p3.6.m6.3.3.3.4.cmml">[</mo><msub id="S2.SS1.p3.6.m6.1.1.1.1.1" xref="S2.SS1.p3.6.m6.1.1.1.1.1.cmml"><mi id="S2.SS1.p3.6.m6.1.1.1.1.1.2" xref="S2.SS1.p3.6.m6.1.1.1.1.1.2.cmml">𝐙</mi><mn id="S2.SS1.p3.6.m6.1.1.1.1.1.3" xref="S2.SS1.p3.6.m6.1.1.1.1.1.3.cmml">0</mn></msub><mo id="S2.SS1.p3.6.m6.3.3.3.3.5" xref="S2.SS1.p3.6.m6.3.3.3.4.cmml">,</mo><msub id="S2.SS1.p3.6.m6.2.2.2.2.2" xref="S2.SS1.p3.6.m6.2.2.2.2.2.cmml"><mi id="S2.SS1.p3.6.m6.2.2.2.2.2.2" xref="S2.SS1.p3.6.m6.2.2.2.2.2.2.cmml">𝐙</mi><mn id="S2.SS1.p3.6.m6.2.2.2.2.2.3" xref="S2.SS1.p3.6.m6.2.2.2.2.2.3.cmml">1</mn></msub><mo id="S2.SS1.p3.6.m6.3.3.3.3.6" xref="S2.SS1.p3.6.m6.3.3.3.4.cmml">,</mo><mrow id="S2.SS1.p3.6.m6.3.3.3.3.3" xref="S2.SS1.p3.6.m6.3.3.3.3.3.cmml"><mi mathvariant="normal" id="S2.SS1.p3.6.m6.3.3.3.3.3.2" xref="S2.SS1.p3.6.m6.3.3.3.3.3.2.cmml">⋯</mi><mo id="S2.SS1.p3.6.m6.3.3.3.3.3.1" xref="S2.SS1.p3.6.m6.3.3.3.3.3.1.cmml" lspace='0px' rspace='0px'></mo><msub id="S2.SS1.p3.6.m6.3.3.3.3.3.3" xref="S2.SS1.p3.6.m6.3.3.3.3.3.3.cmml"><mi id="S2.SS1.p3.6.m6.3.3.3.3.3.3.2" xref="S2.SS1.p3.6.m6.3.3.3.3.3.3.2.cmml">𝐙</mi><mrow id="S2.SS1.p3.6.m6.3.3.3.3.3.3.3" xref="S2.SS1.p3.6.m6.3.3.3.3.3.3.3.cmml"><mi id="S2.SS1.p3.6.m6.3.3.3.3.3.3.3.2" xref="S2.SS1.p3.6.m6.3.3.3.3.3.3.3.2.cmml">h</mi><mo id="S2.SS1.p3.6.m6.3.3.3.3.3.3.3.1" xref="S2.SS1.p3.6.m6.3.3.3.3.3.3.3.1.cmml">−</mo><mn id="S2.SS1.p3.6.m6.3.3.3.3.3.3.3.3" xref="S2.SS1.p3.6.m6.3.3.3.3.3.3.3.3.cmml">1</mn></mrow></msub></mrow><mo stretchy="false" id="S2.SS1.p3.6.m6.3.3.3.3.7" xref="S2.SS1.p3.6.m6.3.3.3.4.cmml">]</mo></mrow><mo id="S2.SS1.p3.6.m6.3.3.4" xref="S2.SS1.p3.6.m6.3.3.4.cmml">∈</mo><msup id="S2.SS1.p3.6.m6.3.3.5" xref="S2.SS1.p3.6.m6.3.3.5.cmml"><mi id="S2.SS1.p3.6.m6.3.3.5.2" xref="S2.SS1.p3.6.m6.3.3.5.2.cmml">ℝ</mi><mrow id="S2.SS1.p3.6.m6.3.3.5.3" xref="S2.SS1.p3.6.m6.3.3.5.3.cmml"><mrow id="S2.SS1.p3.6.m6.3.3.5.3.2" xref="S2.SS1.p3.6.m6.3.3.5.3.2.cmml"><mi id="S2.SS1.p3.6.m6.3.3.5.3.2.2" xref="S2.SS1.p3.6.m6.3.3.5.3.2.2.cmml">n</mi><mo lspace="0.222em" rspace="0.222em" id="S2.SS1.p3.6.m6.3.3.5.3.2.1" xref="S2.SS1.p3.6.m6.3.3.5.3.2.1.cmml">×</mo><mi id="S2.SS1.p3.6.m6.3.3.5.3.2.3" xref="S2.SS1.p3.6.m6.3.3.5.3.2.3.cmml">h</mi></mrow><mo lspace="0.222em" rspace="0.222em" id="S2.SS1.p3.6.m6.3.3.5.3.1" xref="S2.SS1.p3.6.m6.3.3.5.3.1.cmml">⋅</mo><msub id="S2.SS1.p3.6.m6.3.3.5.3.3" xref="S2.SS1.p3.6.m6.3.3.5.3.3.cmml"><mi id="S2.SS1.p3.6.m6.3.3.5.3.3.2" xref="S2.SS1.p3.6.m6.3.3.5.3.3.2.cmml">d</mi><mi id="S2.SS1.p3.6.m6.3.3.5.3.3.3" xref="S2.SS1.p3.6.m6.3.3.5.3.3.3.cmml">v</mi></msub></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.6.m6.3b"><apply id="S2.SS1.p3.6.m6.3.3.cmml" xref="S2.SS1.p3.6.m6.3.3"><in id="S2.SS1.p3.6.m6.3.3.4.cmml" xref="S2.SS1.p3.6.m6.3.3.4"></in><list id="S2.SS1.p3.6.m6.3.3.3.4.cmml" xref="S2.SS1.p3.6.m6.3.3.3.3"><apply id="S2.SS1.p3.6.m6.1.1.1.1.1.cmml" xref="S2.SS1.p3.6.m6.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p3.6.m6.1.1.1.1.1.1.cmml" xref="S2.SS1.p3.6.m6.1.1.1.1.1">subscript</csymbol><ci id="S2.SS1.p3.6.m6.1.1.1.1.1.2.cmml" xref="S2.SS1.p3.6.m6.1.1.1.1.1.2">𝐙</ci><cn type="integer" id="S2.SS1.p3.6.m6.1.1.1.1.1.3.cmml" xref="S2.SS1.p3.6.m6.1.1.1.1.1.3">0</cn></apply><apply id="S2.SS1.p3.6.m6.2.2.2.2.2.cmml" xref="S2.SS1.p3.6.m6.2.2.2.2.2"><csymbol cd="ambiguous" id="S2.SS1.p3.6.m6.2.2.2.2.2.1.cmml" xref="S2.SS1.p3.6.m6.2.2.2.2.2">subscript</csymbol><ci id="S2.SS1.p3.6.m6.2.2.2.2.2.2.cmml" xref="S2.SS1.p3.6.m6.2.2.2.2.2.2">𝐙</ci><cn type="integer" id="S2.SS1.p3.6.m6.2.2.2.2.2.3.cmml" xref="S2.SS1.p3.6.m6.2.2.2.2.2.3">1</cn></apply><apply id="S2.SS1.p3.6.m6.3.3.3.3.3.cmml" xref="S2.SS1.p3.6.m6.3.3.3.3.3"><times id="S2.SS1.p3.6.m6.3.3.3.3.3.1.cmml" xref="S2.SS1.p3.6.m6.3.3.3.3.3.1"></times><ci id="S2.SS1.p3.6.m6.3.3.3.3.3.2.cmml" xref="S2.SS1.p3.6.m6.3.3.3.3.3.2">⋯</ci><apply id="S2.SS1.p3.6.m6.3.3.3.3.3.3.cmml" xref="S2.SS1.p3.6.m6.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S2.SS1.p3.6.m6.3.3.3.3.3.3.1.cmml" xref="S2.SS1.p3.6.m6.3.3.3.3.3.3">subscript</csymbol><ci id="S2.SS1.p3.6.m6.3.3.3.3.3.3.2.cmml" xref="S2.SS1.p3.6.m6.3.3.3.3.3.3.2">𝐙</ci><apply id="S2.SS1.p3.6.m6.3.3.3.3.3.3.3.cmml" xref="S2.SS1.p3.6.m6.3.3.3.3.3.3.3"><minus id="S2.SS1.p3.6.m6.3.3.3.3.3.3.3.1.cmml" xref="S2.SS1.p3.6.m6.3.3.3.3.3.3.3.1"></minus><ci id="S2.SS1.p3.6.m6.3.3.3.3.3.3.3.2.cmml" xref="S2.SS1.p3.6.m6.3.3.3.3.3.3.3.2">ℎ</ci><cn type="integer" id="S2.SS1.p3.6.m6.3.3.3.3.3.3.3.3.cmml" xref="S2.SS1.p3.6.m6.3.3.3.3.3.3.3.3">1</cn></apply></apply></apply></list><apply id="S2.SS1.p3.6.m6.3.3.5.cmml" xref="S2.SS1.p3.6.m6.3.3.5"><csymbol cd="ambiguous" id="S2.SS1.p3.6.m6.3.3.5.1.cmml" xref="S2.SS1.p3.6.m6.3.3.5">superscript</csymbol><ci id="S2.SS1.p3.6.m6.3.3.5.2.cmml" xref="S2.SS1.p3.6.m6.3.3.5.2">ℝ</ci><apply id="S2.SS1.p3.6.m6.3.3.5.3.cmml" xref="S2.SS1.p3.6.m6.3.3.5.3"><ci id="S2.SS1.p3.6.m6.3.3.5.3.1.cmml" xref="S2.SS1.p3.6.m6.3.3.5.3.1">⋅</ci><apply id="S2.SS1.p3.6.m6.3.3.5.3.2.cmml" xref="S2.SS1.p3.6.m6.3.3.5.3.2"><times id="S2.SS1.p3.6.m6.3.3.5.3.2.1.cmml" xref="S2.SS1.p3.6.m6.3.3.5.3.2.1"></times><ci id="S2.SS1.p3.6.m6.3.3.5.3.2.2.cmml" xref="S2.SS1.p3.6.m6.3.3.5.3.2.2">𝑛</ci><ci id="S2.SS1.p3.6.m6.3.3.5.3.2.3.cmml" xref="S2.SS1.p3.6.m6.3.3.5.3.2.3">ℎ</ci></apply><apply id="S2.SS1.p3.6.m6.3.3.5.3.3.cmml" xref="S2.SS1.p3.6.m6.3.3.5.3.3"><csymbol cd="ambiguous" id="S2.SS1.p3.6.m6.3.3.5.3.3.1.cmml" xref="S2.SS1.p3.6.m6.3.3.5.3.3">subscript</csymbol><ci id="S2.SS1.p3.6.m6.3.3.5.3.3.2.cmml" xref="S2.SS1.p3.6.m6.3.3.5.3.3.2">𝑑</ci><ci id="S2.SS1.p3.6.m6.3.3.5.3.3.3.cmml" xref="S2.SS1.p3.6.m6.3.3.5.3.3.3">𝑣</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.6.m6.3c">[\mathbf{Z}_{0},\mathbf{Z}_{1},\cdots\mathbf{Z}_{h-1}]\in\mathbb{R}^{n\times h\cdot d_{v}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p3.6.m6.3d">[ bold_Z start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , bold_Z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ⋯ bold_Z start_POSTSUBSCRIPT italic_h - 1 end_POSTSUBSCRIPT ] ∈ blackboard_R start_POSTSUPERSCRIPT italic_n × italic_h ⋅ italic_d start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT end_POSTSUPERSCRIPT</annotation></semantics></math> and projected onto a weight matrix <math id="S2.SS1.p3.7.m7.1" class="ltx_Math" alttext="\mathbf{W}\in\mathbb{R}^{h\cdot d_{v}\times d}" display="inline"><semantics id="S2.SS1.p3.7.m7.1a"><mrow id="S2.SS1.p3.7.m7.1.1" xref="S2.SS1.p3.7.m7.1.1.cmml"><mi id="S2.SS1.p3.7.m7.1.1.2" xref="S2.SS1.p3.7.m7.1.1.2.cmml">𝐖</mi><mo id="S2.SS1.p3.7.m7.1.1.1" xref="S2.SS1.p3.7.m7.1.1.1.cmml">∈</mo><msup id="S2.SS1.p3.7.m7.1.1.3" xref="S2.SS1.p3.7.m7.1.1.3.cmml"><mi id="S2.SS1.p3.7.m7.1.1.3.2" xref="S2.SS1.p3.7.m7.1.1.3.2.cmml">ℝ</mi><mrow id="S2.SS1.p3.7.m7.1.1.3.3" xref="S2.SS1.p3.7.m7.1.1.3.3.cmml"><mrow id="S2.SS1.p3.7.m7.1.1.3.3.2" xref="S2.SS1.p3.7.m7.1.1.3.3.2.cmml"><mi id="S2.SS1.p3.7.m7.1.1.3.3.2.2" xref="S2.SS1.p3.7.m7.1.1.3.3.2.2.cmml">h</mi><mo lspace="0.222em" rspace="0.222em" id="S2.SS1.p3.7.m7.1.1.3.3.2.1" xref="S2.SS1.p3.7.m7.1.1.3.3.2.1.cmml">⋅</mo><msub id="S2.SS1.p3.7.m7.1.1.3.3.2.3" xref="S2.SS1.p3.7.m7.1.1.3.3.2.3.cmml"><mi id="S2.SS1.p3.7.m7.1.1.3.3.2.3.2" xref="S2.SS1.p3.7.m7.1.1.3.3.2.3.2.cmml">d</mi><mi id="S2.SS1.p3.7.m7.1.1.3.3.2.3.3" xref="S2.SS1.p3.7.m7.1.1.3.3.2.3.3.cmml">v</mi></msub></mrow><mo lspace="0.222em" rspace="0.222em" id="S2.SS1.p3.7.m7.1.1.3.3.1" xref="S2.SS1.p3.7.m7.1.1.3.3.1.cmml">×</mo><mi id="S2.SS1.p3.7.m7.1.1.3.3.3" xref="S2.SS1.p3.7.m7.1.1.3.3.3.cmml">d</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.7.m7.1b"><apply id="S2.SS1.p3.7.m7.1.1.cmml" xref="S2.SS1.p3.7.m7.1.1"><in id="S2.SS1.p3.7.m7.1.1.1.cmml" xref="S2.SS1.p3.7.m7.1.1.1"></in><ci id="S2.SS1.p3.7.m7.1.1.2.cmml" xref="S2.SS1.p3.7.m7.1.1.2">𝐖</ci><apply id="S2.SS1.p3.7.m7.1.1.3.cmml" xref="S2.SS1.p3.7.m7.1.1.3"><csymbol cd="ambiguous" id="S2.SS1.p3.7.m7.1.1.3.1.cmml" xref="S2.SS1.p3.7.m7.1.1.3">superscript</csymbol><ci id="S2.SS1.p3.7.m7.1.1.3.2.cmml" xref="S2.SS1.p3.7.m7.1.1.3.2">ℝ</ci><apply id="S2.SS1.p3.7.m7.1.1.3.3.cmml" xref="S2.SS1.p3.7.m7.1.1.3.3"><times id="S2.SS1.p3.7.m7.1.1.3.3.1.cmml" xref="S2.SS1.p3.7.m7.1.1.3.3.1"></times><apply id="S2.SS1.p3.7.m7.1.1.3.3.2.cmml" xref="S2.SS1.p3.7.m7.1.1.3.3.2"><ci id="S2.SS1.p3.7.m7.1.1.3.3.2.1.cmml" xref="S2.SS1.p3.7.m7.1.1.3.3.2.1">⋅</ci><ci id="S2.SS1.p3.7.m7.1.1.3.3.2.2.cmml" xref="S2.SS1.p3.7.m7.1.1.3.3.2.2">ℎ</ci><apply id="S2.SS1.p3.7.m7.1.1.3.3.2.3.cmml" xref="S2.SS1.p3.7.m7.1.1.3.3.2.3"><csymbol cd="ambiguous" id="S2.SS1.p3.7.m7.1.1.3.3.2.3.1.cmml" xref="S2.SS1.p3.7.m7.1.1.3.3.2.3">subscript</csymbol><ci id="S2.SS1.p3.7.m7.1.1.3.3.2.3.2.cmml" xref="S2.SS1.p3.7.m7.1.1.3.3.2.3.2">𝑑</ci><ci id="S2.SS1.p3.7.m7.1.1.3.3.2.3.3.cmml" xref="S2.SS1.p3.7.m7.1.1.3.3.2.3.3">𝑣</ci></apply></apply><ci id="S2.SS1.p3.7.m7.1.1.3.3.3.cmml" xref="S2.SS1.p3.7.m7.1.1.3.3.3">𝑑</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.7.m7.1c">\mathbf{W}\in\mathbb{R}^{h\cdot d_{v}\times d}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p3.7.m7.1d">bold_W ∈ blackboard_R start_POSTSUPERSCRIPT italic_h ⋅ italic_d start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT × italic_d end_POSTSUPERSCRIPT</annotation></semantics></math> (Fig. <a href="#S2.F3" title="Figure 3 ‣ 2 Foundations ‣ Transformers in Vision: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, top row).</p>
</div>
<div id="S2.SS1.p4" class="ltx_para">
<p id="S2.SS1.p4.1" class="ltx_p">The main difference of self-attention with convolution operation is that the filters are dynamically calculated instead of static filters (that stay the same for any input) as in the case of convolution. Further, self-attention is invariant to permutations and changes in the number of input points. <span id="S2.SS1.p4.1.1" class="ltx_text" style="color:#000000;">As a result, it can easily operate on irregular inputs as opposed to standard convolution that requires grid structure. Furthermore, it has been shown in the literature how self-attention (with positional encodings) is theoretically a more flexible operation which can model the behaviour of convolutional models towards encoding local features <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>. Cordonnier <span id="S2.SS1.p4.1.1.1" class="ltx_text ltx_font_italic">et al.<cite class="ltx_cite ltx_citemacro_cite"><span id="S2.SS1.p4.1.1.1.1.1" class="ltx_text ltx_font_upright">[</span><a href="#bib.bib41" title="" class="ltx_ref">41</a><span id="S2.SS1.p4.1.1.1.2.2" class="ltx_text ltx_font_upright">]</span></cite></span> further studied the relationships between self-attention and convolution operations. Their empirical results confirm that multi-head self-attention (with sufficient parameters) is a more generic operation which can model the expressiveness of convolution as a special case. In fact, self-attention provides the capability to learn the global as well as local features, and provide expressivity to adaptively learn kernel weights as well as the receptive field (similar to deformable convolutions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>).</span></p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span><span id="S2.SS2.1.1" class="ltx_text ltx_font_italic" style="color:#000000;">(Self) Supervised Pre-training</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Self-attention based Transformer models generally operate in a two-stage training mechanism. First, pre-training is performed on a large-scale dataset (and sometimes a combination of several available datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>) in either a supervised <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> or a <span id="S2.SS2.p1.1.1" class="ltx_text" style="color:#000000;">self-supervised manner</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>. Later, the pre-trained weights are adapted to the down-stream tasks using small-mid scale datasets. Examples of downstream tasks include image classification <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>, object detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, <span id="S2.SS2.p1.1.2" class="ltx_text" style="color:#000000;">zero-shot classification</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, question-answering <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> and action recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. The effectiveness of pre-training for large-scale Transformers has been advocated in both the language and vision domains. For example, Vision Transformer model (ViT-L) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> experiences an absolute <math id="S2.SS2.p1.1.m1.1" class="ltx_Math" alttext="13\%" display="inline"><semantics id="S2.SS2.p1.1.m1.1a"><mrow id="S2.SS2.p1.1.m1.1.1" xref="S2.SS2.p1.1.m1.1.1.cmml"><mn id="S2.SS2.p1.1.m1.1.1.2" xref="S2.SS2.p1.1.m1.1.1.2.cmml">13</mn><mo id="S2.SS2.p1.1.m1.1.1.1" xref="S2.SS2.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.1.m1.1b"><apply id="S2.SS2.p1.1.m1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1"><csymbol cd="latexml" id="S2.SS2.p1.1.m1.1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S2.SS2.p1.1.m1.1.1.2.cmml" xref="S2.SS2.p1.1.m1.1.1.2">13</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.1.m1.1c">13\%</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.1.m1.1d">13 %</annotation></semantics></math> drop in accuracy on ImageNet test set when trained only on ImageNet train set as compared to the case when pretrained on JFT dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite> with 300 million images.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">Since acquiring manual labels at a massive scale is cumbersome, self-supervised learning has been very effectively used in the pre-training stage. The self-supervision based pre-training stage training has played a crucial role in unleashing the scalability and generalization of Transformer networks, enabling training even above a <em id="S2.SS2.p2.1.1" class="ltx_emph ltx_font_italic">trillion</em> parameter networks (e.g., the latest Switch Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> from Google).
An extensive survey on SSL can be found in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>, <a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>. As nicely summarized by Y. LeCun <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>, the basic idea of SSL is to <em id="S2.SS2.p2.1.2" class="ltx_emph ltx_font_italic">fill in the blanks</em>, i.e., try to predict the occluded data in images, future or past frames in temporal video sequences or predict a pretext task <em id="S2.SS2.p2.1.3" class="ltx_emph ltx_font_italic">e.g.</em>, the amount of rotation applied to inputs, the permutation applied to image patches or the color of a gray-scale image. Another effective way to impose self-supervised constraints is via contrastive learning. In this case, nuisance transformations are used to create two types of modified versions of the same image i.e., without changing the underlying class semantics (<em id="S2.SS2.p2.1.4" class="ltx_emph ltx_font_italic">e.g.</em>, image stylizing, cropping) and with semantic changes (<em id="S2.SS2.p2.1.5" class="ltx_emph ltx_font_italic">e.g.</em>, replacing an object with another in the same scene, or changing the class with minor adversarial changes to the image). Subsequently, the model is trained to be invariant to the nuisance transformations and emphasize on modeling minor changes that can alter semantic labels.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">Self-supervised learning provides a promising learning paradigm since it enables learning from a vast amount of readily available non-annotated data. In the SSL based pre-training stage, a model is trained to learn a meaningful representation of the underlying data by solving a pretext task. The pseudo-labels for the pretext task are automatically generated (without requiring any expensive manual annotations) based on data attributes and task definition. Therefore, the pretext task definition is a critical choice in SSL. We can broadly categorize existing SSL methods based upon their pretext tasks into <span id="S2.SS2.p3.1.1" class="ltx_text ltx_font_bold">(a)</span> <span id="S2.SS2.p3.1.2" class="ltx_text ltx_font_italic">generative</span> approaches which synthesize images or videos (given conditional inputs), <span id="S2.SS2.p3.1.3" class="ltx_text ltx_font_bold">(b)</span> <span id="S2.SS2.p3.1.4" class="ltx_text ltx_font_italic">context-based</span> methods which exploit the relationships between image patches or video frames, and <span id="S2.SS2.p3.1.5" class="ltx_text ltx_font_bold">(c)</span> <span id="S2.SS2.p3.1.6" class="ltx_text ltx_font_italic">cross-modal</span> methods which leverage from multiple data modalities. Examples of <span id="S2.SS2.p3.1.7" class="ltx_text ltx_font_italic">generative</span> approaches include conditional generation tasks such as masked image modeling <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> and image colorization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>, image super-resolution <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite>, image in-painting <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>, and GANs based methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>, <a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>. The <span id="S2.SS2.p3.1.8" class="ltx_text ltx_font_italic">context-based</span> pretext methods solve problems such as a jigsaw puzzle on image patches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>, <a href="#bib.bib57" title="" class="ltx_ref">57</a>, <a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite>, masked object classification <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, predict geometric transformation such as rotation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>, <a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite>, or verify temporal sequence of video frames <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>, <a href="#bib.bib61" title="" class="ltx_ref">61</a>, <a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite>. Cross-modal pretext methods verify the correspondence of two input modalities <em id="S2.SS2.p3.1.9" class="ltx_emph ltx_font_italic">e.g.</em>, text &amp; image <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite>, audio &amp; video <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>, <a href="#bib.bib65" title="" class="ltx_ref">65</a>]</cite> or RGB &amp; flow <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span><span id="S2.SS3.1.1" class="ltx_text ltx_font_italic">Transformer Model</span>
</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">The architecture of the Transformer model proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> is shown in Fig. <a href="#S2.F3" title="Figure 3 ‣ 2 Foundations ‣ Transformers in Vision: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. It has an encoder-decoder structure. The encoder (<em id="S2.SS3.p1.1.1" class="ltx_emph ltx_font_italic">middle</em> row) consists of six identical blocks (i.e., <math id="S2.SS3.p1.1.m1.1" class="ltx_Math" alttext="N{=}6" display="inline"><semantics id="S2.SS3.p1.1.m1.1a"><mrow id="S2.SS3.p1.1.m1.1.1" xref="S2.SS3.p1.1.m1.1.1.cmml"><mi id="S2.SS3.p1.1.m1.1.1.2" xref="S2.SS3.p1.1.m1.1.1.2.cmml">N</mi><mo id="S2.SS3.p1.1.m1.1.1.1" xref="S2.SS3.p1.1.m1.1.1.1.cmml">=</mo><mn id="S2.SS3.p1.1.m1.1.1.3" xref="S2.SS3.p1.1.m1.1.1.3.cmml">6</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.1.m1.1b"><apply id="S2.SS3.p1.1.m1.1.1.cmml" xref="S2.SS3.p1.1.m1.1.1"><eq id="S2.SS3.p1.1.m1.1.1.1.cmml" xref="S2.SS3.p1.1.m1.1.1.1"></eq><ci id="S2.SS3.p1.1.m1.1.1.2.cmml" xref="S2.SS3.p1.1.m1.1.1.2">𝑁</ci><cn type="integer" id="S2.SS3.p1.1.m1.1.1.3.cmml" xref="S2.SS3.p1.1.m1.1.1.3">6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.1.m1.1c">N{=}6</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p1.1.m1.1d">italic_N = 6</annotation></semantics></math> in Fig. <a href="#S2.F3" title="Figure 3 ‣ 2 Foundations ‣ Transformers in Vision: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>), with each block having two sub-layers: a multi-head self-attention network, and a simple position-wise fully connected feed-forward network. Residual connections <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite> alongside layer normalization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite> are employed after each block as in Fig. <a href="#S2.F3" title="Figure 3 ‣ 2 Foundations ‣ Transformers in Vision: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. Note that, different from regular convolutional networks where feature aggregation and feature transformation are simultaneously performed (<em id="S2.SS3.p1.1.2" class="ltx_emph ltx_font_italic">e.g.</em>, with a convolution layer followed by a non-linearity), these two steps are decoupled in the Transformer model i.e., self-attention layer only performs aggregation while the feed-forward layer performs transformation.
Similar to the encoder, the decoder (<em id="S2.SS3.p1.1.3" class="ltx_emph ltx_font_italic">bottom</em> row) in the Transformer model comprises six identical blocks. Each decoder block has three sub-layers, first two (multi-head self-attention, and feed-forward) are similar to the encoder, while the third sub-layer performs multi-head attention on the outputs of the corresponding encoder block, as shown in Fig. <a href="#S2.F3" title="Figure 3 ‣ 2 Foundations ‣ Transformers in Vision: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.3" class="ltx_p">The original Transformer model in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> was trained for the Machine Translation task. The input to the encoder is a sequence of words (sentence) in one language. <span id="S2.SS3.p2.3.1" class="ltx_text ltx_font_bold">Positional encodings</span> are added to the input sequence to capture the relative position of each word in the sequence. Positional encodings have the same dimensions as the input <math id="S2.SS3.p2.1.m1.1" class="ltx_Math" alttext="d=512" display="inline"><semantics id="S2.SS3.p2.1.m1.1a"><mrow id="S2.SS3.p2.1.m1.1.1" xref="S2.SS3.p2.1.m1.1.1.cmml"><mi id="S2.SS3.p2.1.m1.1.1.2" xref="S2.SS3.p2.1.m1.1.1.2.cmml">d</mi><mo id="S2.SS3.p2.1.m1.1.1.1" xref="S2.SS3.p2.1.m1.1.1.1.cmml">=</mo><mn id="S2.SS3.p2.1.m1.1.1.3" xref="S2.SS3.p2.1.m1.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p2.1.m1.1b"><apply id="S2.SS3.p2.1.m1.1.1.cmml" xref="S2.SS3.p2.1.m1.1.1"><eq id="S2.SS3.p2.1.m1.1.1.1.cmml" xref="S2.SS3.p2.1.m1.1.1.1"></eq><ci id="S2.SS3.p2.1.m1.1.1.2.cmml" xref="S2.SS3.p2.1.m1.1.1.2">𝑑</ci><cn type="integer" id="S2.SS3.p2.1.m1.1.1.3.cmml" xref="S2.SS3.p2.1.m1.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p2.1.m1.1c">d=512</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p2.1.m1.1d">italic_d = 512</annotation></semantics></math>, and can be learned or pre-defined <em id="S2.SS3.p2.3.2" class="ltx_emph ltx_font_italic">e.g.</em>, by sine or cosine functions. Being an auto-regressive model, the decoder of the Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> uses previous predictions to output the next word in the sequence. The decoder, therefore, takes inputs from the encoder as well as the previous outputs to predict the next word of the sentence in the translated language. To facilitate residual connections the output dimensions of all layers are kept the same i.e., <math id="S2.SS3.p2.2.m2.1" class="ltx_Math" alttext="d=512" display="inline"><semantics id="S2.SS3.p2.2.m2.1a"><mrow id="S2.SS3.p2.2.m2.1.1" xref="S2.SS3.p2.2.m2.1.1.cmml"><mi id="S2.SS3.p2.2.m2.1.1.2" xref="S2.SS3.p2.2.m2.1.1.2.cmml">d</mi><mo id="S2.SS3.p2.2.m2.1.1.1" xref="S2.SS3.p2.2.m2.1.1.1.cmml">=</mo><mn id="S2.SS3.p2.2.m2.1.1.3" xref="S2.SS3.p2.2.m2.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p2.2.m2.1b"><apply id="S2.SS3.p2.2.m2.1.1.cmml" xref="S2.SS3.p2.2.m2.1.1"><eq id="S2.SS3.p2.2.m2.1.1.1.cmml" xref="S2.SS3.p2.2.m2.1.1.1"></eq><ci id="S2.SS3.p2.2.m2.1.1.2.cmml" xref="S2.SS3.p2.2.m2.1.1.2">𝑑</ci><cn type="integer" id="S2.SS3.p2.2.m2.1.1.3.cmml" xref="S2.SS3.p2.2.m2.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p2.2.m2.1c">d=512</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p2.2.m2.1d">italic_d = 512</annotation></semantics></math>. The dimensions of query, key and value weight matrices in multi-head attention are set to <math id="S2.SS3.p2.3.m3.2" class="ltx_Math" alttext="d_{q}=64,d_{k}=64,d_{v}=64" display="inline"><semantics id="S2.SS3.p2.3.m3.2a"><mrow id="S2.SS3.p2.3.m3.2.2.2" xref="S2.SS3.p2.3.m3.2.2.3.cmml"><mrow id="S2.SS3.p2.3.m3.1.1.1.1" xref="S2.SS3.p2.3.m3.1.1.1.1.cmml"><msub id="S2.SS3.p2.3.m3.1.1.1.1.2" xref="S2.SS3.p2.3.m3.1.1.1.1.2.cmml"><mi id="S2.SS3.p2.3.m3.1.1.1.1.2.2" xref="S2.SS3.p2.3.m3.1.1.1.1.2.2.cmml">d</mi><mi id="S2.SS3.p2.3.m3.1.1.1.1.2.3" xref="S2.SS3.p2.3.m3.1.1.1.1.2.3.cmml">q</mi></msub><mo id="S2.SS3.p2.3.m3.1.1.1.1.1" xref="S2.SS3.p2.3.m3.1.1.1.1.1.cmml">=</mo><mn id="S2.SS3.p2.3.m3.1.1.1.1.3" xref="S2.SS3.p2.3.m3.1.1.1.1.3.cmml">64</mn></mrow><mo id="S2.SS3.p2.3.m3.2.2.2.3" xref="S2.SS3.p2.3.m3.2.2.3a.cmml">,</mo><mrow id="S2.SS3.p2.3.m3.2.2.2.2.2" xref="S2.SS3.p2.3.m3.2.2.2.2.3.cmml"><mrow id="S2.SS3.p2.3.m3.2.2.2.2.1.1" xref="S2.SS3.p2.3.m3.2.2.2.2.1.1.cmml"><msub id="S2.SS3.p2.3.m3.2.2.2.2.1.1.2" xref="S2.SS3.p2.3.m3.2.2.2.2.1.1.2.cmml"><mi id="S2.SS3.p2.3.m3.2.2.2.2.1.1.2.2" xref="S2.SS3.p2.3.m3.2.2.2.2.1.1.2.2.cmml">d</mi><mi id="S2.SS3.p2.3.m3.2.2.2.2.1.1.2.3" xref="S2.SS3.p2.3.m3.2.2.2.2.1.1.2.3.cmml">k</mi></msub><mo id="S2.SS3.p2.3.m3.2.2.2.2.1.1.1" xref="S2.SS3.p2.3.m3.2.2.2.2.1.1.1.cmml">=</mo><mn id="S2.SS3.p2.3.m3.2.2.2.2.1.1.3" xref="S2.SS3.p2.3.m3.2.2.2.2.1.1.3.cmml">64</mn></mrow><mo id="S2.SS3.p2.3.m3.2.2.2.2.2.3" xref="S2.SS3.p2.3.m3.2.2.2.2.3a.cmml">,</mo><mrow id="S2.SS3.p2.3.m3.2.2.2.2.2.2" xref="S2.SS3.p2.3.m3.2.2.2.2.2.2.cmml"><msub id="S2.SS3.p2.3.m3.2.2.2.2.2.2.2" xref="S2.SS3.p2.3.m3.2.2.2.2.2.2.2.cmml"><mi id="S2.SS3.p2.3.m3.2.2.2.2.2.2.2.2" xref="S2.SS3.p2.3.m3.2.2.2.2.2.2.2.2.cmml">d</mi><mi id="S2.SS3.p2.3.m3.2.2.2.2.2.2.2.3" xref="S2.SS3.p2.3.m3.2.2.2.2.2.2.2.3.cmml">v</mi></msub><mo id="S2.SS3.p2.3.m3.2.2.2.2.2.2.1" xref="S2.SS3.p2.3.m3.2.2.2.2.2.2.1.cmml">=</mo><mn id="S2.SS3.p2.3.m3.2.2.2.2.2.2.3" xref="S2.SS3.p2.3.m3.2.2.2.2.2.2.3.cmml">64</mn></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p2.3.m3.2b"><apply id="S2.SS3.p2.3.m3.2.2.3.cmml" xref="S2.SS3.p2.3.m3.2.2.2"><csymbol cd="ambiguous" id="S2.SS3.p2.3.m3.2.2.3a.cmml" xref="S2.SS3.p2.3.m3.2.2.2.3">formulae-sequence</csymbol><apply id="S2.SS3.p2.3.m3.1.1.1.1.cmml" xref="S2.SS3.p2.3.m3.1.1.1.1"><eq id="S2.SS3.p2.3.m3.1.1.1.1.1.cmml" xref="S2.SS3.p2.3.m3.1.1.1.1.1"></eq><apply id="S2.SS3.p2.3.m3.1.1.1.1.2.cmml" xref="S2.SS3.p2.3.m3.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.SS3.p2.3.m3.1.1.1.1.2.1.cmml" xref="S2.SS3.p2.3.m3.1.1.1.1.2">subscript</csymbol><ci id="S2.SS3.p2.3.m3.1.1.1.1.2.2.cmml" xref="S2.SS3.p2.3.m3.1.1.1.1.2.2">𝑑</ci><ci id="S2.SS3.p2.3.m3.1.1.1.1.2.3.cmml" xref="S2.SS3.p2.3.m3.1.1.1.1.2.3">𝑞</ci></apply><cn type="integer" id="S2.SS3.p2.3.m3.1.1.1.1.3.cmml" xref="S2.SS3.p2.3.m3.1.1.1.1.3">64</cn></apply><apply id="S2.SS3.p2.3.m3.2.2.2.2.3.cmml" xref="S2.SS3.p2.3.m3.2.2.2.2.2"><csymbol cd="ambiguous" id="S2.SS3.p2.3.m3.2.2.2.2.3a.cmml" xref="S2.SS3.p2.3.m3.2.2.2.2.2.3">formulae-sequence</csymbol><apply id="S2.SS3.p2.3.m3.2.2.2.2.1.1.cmml" xref="S2.SS3.p2.3.m3.2.2.2.2.1.1"><eq id="S2.SS3.p2.3.m3.2.2.2.2.1.1.1.cmml" xref="S2.SS3.p2.3.m3.2.2.2.2.1.1.1"></eq><apply id="S2.SS3.p2.3.m3.2.2.2.2.1.1.2.cmml" xref="S2.SS3.p2.3.m3.2.2.2.2.1.1.2"><csymbol cd="ambiguous" id="S2.SS3.p2.3.m3.2.2.2.2.1.1.2.1.cmml" xref="S2.SS3.p2.3.m3.2.2.2.2.1.1.2">subscript</csymbol><ci id="S2.SS3.p2.3.m3.2.2.2.2.1.1.2.2.cmml" xref="S2.SS3.p2.3.m3.2.2.2.2.1.1.2.2">𝑑</ci><ci id="S2.SS3.p2.3.m3.2.2.2.2.1.1.2.3.cmml" xref="S2.SS3.p2.3.m3.2.2.2.2.1.1.2.3">𝑘</ci></apply><cn type="integer" id="S2.SS3.p2.3.m3.2.2.2.2.1.1.3.cmml" xref="S2.SS3.p2.3.m3.2.2.2.2.1.1.3">64</cn></apply><apply id="S2.SS3.p2.3.m3.2.2.2.2.2.2.cmml" xref="S2.SS3.p2.3.m3.2.2.2.2.2.2"><eq id="S2.SS3.p2.3.m3.2.2.2.2.2.2.1.cmml" xref="S2.SS3.p2.3.m3.2.2.2.2.2.2.1"></eq><apply id="S2.SS3.p2.3.m3.2.2.2.2.2.2.2.cmml" xref="S2.SS3.p2.3.m3.2.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S2.SS3.p2.3.m3.2.2.2.2.2.2.2.1.cmml" xref="S2.SS3.p2.3.m3.2.2.2.2.2.2.2">subscript</csymbol><ci id="S2.SS3.p2.3.m3.2.2.2.2.2.2.2.2.cmml" xref="S2.SS3.p2.3.m3.2.2.2.2.2.2.2.2">𝑑</ci><ci id="S2.SS3.p2.3.m3.2.2.2.2.2.2.2.3.cmml" xref="S2.SS3.p2.3.m3.2.2.2.2.2.2.2.3">𝑣</ci></apply><cn type="integer" id="S2.SS3.p2.3.m3.2.2.2.2.2.2.3.cmml" xref="S2.SS3.p2.3.m3.2.2.2.2.2.2.3">64</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p2.3.m3.2c">d_{q}=64,d_{k}=64,d_{v}=64</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p2.3.m3.2d">italic_d start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT = 64 , italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = 64 , italic_d start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT = 64</annotation></semantics></math>.</p>
</div>
<figure id="S2.F4" class="ltx_figure"><img src="/html/2101.01169/assets/x1.png" id="S2.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="508" height="233" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F4.4.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><em id="S2.F4.5.2" class="ltx_emph ltx_font_italic" style="font-size:90%;color:#000000;">A taxonomy of self-attention design space</em><span id="S2.F4.6.3" class="ltx_text" style="font-size:90%;color:#000000;">. Existing approaches based on self-attention explore single-head or multi-head (transformer) designs for vision tasks. We note that interesting efforts have been made to utilize knowledge from convolution based architectures to improve ViTs (e.g., multi-scale and hybrid designs). We categorize the upcoming sections of this survey according to the types of self-attention block (<em id="S2.F4.6.3.1" class="ltx_emph ltx_font_italic">left tree diagram</em>) as well as the prominent tasks in computer vision (<em id="S2.F4.6.3.2" class="ltx_emph ltx_font_italic">right</em>). </span></figcaption>
</figure>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span><span id="S2.SS4.1.1" class="ltx_text ltx_font_italic">Bidirectional Representations</span>
</h3>

<div id="S2.SS4.p1" class="ltx_para">
<p id="S2.SS4.p1.1" class="ltx_p">The training strategy of the original Transformer model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> could only attend to the context on the left of a given word in the sentence. This is limiting, since for most language tasks, contextual information from both left and right sides is important. Bidirectional Encoder Representations from
Transformers (BERT) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> proposed to jointly encode the right and left context of a word in a sentence, thus improving the learned feature representations for textual data in an self-supervised manner. To this end, BERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> introduced two pretext tasks to pre-train the Transformer model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> in a self-supervised manner: <span id="S2.SS4.p1.1.1" class="ltx_text ltx_font_italic">Masked Language Model</span> and <span id="S2.SS4.p1.1.2" class="ltx_text ltx_font_italic">Next Sentence Prediction</span>. For adapting the pre-trained model for downstream tasks, a task-specific additional output module is appended to the pre-trained model, and the full model is fine-tuned end-to-end.
Here, we briefly touch upon the pretext tasks. <span id="S2.SS4.p1.1.3" class="ltx_text ltx_font_bold">(1) Masked Language Model (MLM) -</span> A fixed percentage (15%) of words in a sentence are randomly masked and the model is trained to predict these masked words using cross-entropy loss. In predicting the masked words, the model learns to incorporate the bidirectional context.
<span id="S2.SS4.p1.1.4" class="ltx_text ltx_font_bold">(2) Next Sentence Prediction (NSP) -</span> Given a pair of sentences, the model predicts a binary label i.e., whether the pair is valid from the original document or not. The training data for this can easily be generated from any monolingual text corpus. A pair of sentences <span id="S2.SS4.p1.1.5" class="ltx_text ltx_font_italic">A</span> and <span id="S2.SS4.p1.1.6" class="ltx_text ltx_font_italic">B</span> is formed, such that <span id="S2.SS4.p1.1.7" class="ltx_text ltx_font_italic">B</span> is the actual sentence (next to <span id="S2.SS4.p1.1.8" class="ltx_text ltx_font_italic">A</span>) 50% of the time, and <span id="S2.SS4.p1.1.9" class="ltx_text ltx_font_italic">B</span> is a random sentence for other 50% of the time. NSP enables the model to capture sentence-to-sentence relationships which are crucial in many language modeling tasks such as Question Answering and Natural Language Inference.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Self-Attention &amp; Transformers in Vision</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p"><span id="S3.p1.1.1" class="ltx_text" style="color:#000000;">We broadly categorize vision models with self-attention into two categories: the models which use single-head self-attention (Sec. <a href="#S3.SS1" title="3.1 Single-head Self-Attention ‣ 3 Self-Attention &amp; Transformers in Vision ‣ Transformers in Vision: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>), and the models which employ multi-head self-attention based Transformer modules into their architectures (Sec. <a href="#S3.SS2" title="3.2 Multi-head Self-Attention (Transformers) ‣ 3 Self-Attention &amp; Transformers in Vision ‣ Transformers in Vision: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>). Below, we first discuss the first category of single-head self-attention based frameworks, which generally apply global or local self-attention within CNN architectures, or utilize matrix factorization to enhance design efficiency and use vectorized attention models. We then discuss the Transformer-based vision architectures in Sec. <a href="#S3.SS2" title="3.2 Multi-head Self-Attention (Transformers) ‣ 3 Self-Attention &amp; Transformers in Vision ‣ Transformers in Vision: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>.</span></p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span><span id="S3.SS1.1.1" class="ltx_text ltx_font_italic" style="color:#000000;">Single-head Self-Attention</span>
</h3>

<section id="S3.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1 </span>Self-Attention in CNNs</h4>

<div id="S3.SS1.SSS1.p1" class="ltx_para">
<p id="S3.SS1.SSS1.p1.1" class="ltx_p">Inspired by non-local means operation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite> which was mainly designed for image denoising,
Wang <span id="S3.SS1.SSS1.p1.1.1" class="ltx_text ltx_font_italic">et al.<cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS1.SSS1.p1.1.1.1.1" class="ltx_text ltx_font_upright">[</span><a href="#bib.bib70" title="" class="ltx_ref">70</a><span id="S3.SS1.SSS1.p1.1.1.2.2" class="ltx_text ltx_font_upright">]</span></cite></span> proposed a differentiable non-local operation for deep neural networks to capture long-range dependencies both in space and time in a feed-forward fashion. Given a feature map, their proposed operator <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite> computes the response at a position as a weighted sum of the features at all positions in the feature map. This way, the non-local operation is able to capture interactions between any two positions in the feature map regardless of the distance between them. Videos classification is an example of a task where long-range interactions between pixels exist both in space and time. Equipped with the capability to model long-range interactions, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite> demonstrated the superiority of non-local deep neural networks for more accurate video classification on Kinetics dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib71" title="" class="ltx_ref">71</a>]</cite>.</p>
</div>
<div id="S3.SS1.SSS1.p2" class="ltx_para">
<p id="S3.SS1.SSS1.p2.4" class="ltx_p">Although the self-attention allows us to model full-image contextual information, it is both memory and compute intensive. As shown in Fig. <a href="#S3.F5" title="Figure 5 ‣ 3.1.1 Self-Attention in CNNs ‣ 3.1 Single-head Self-Attention ‣ 3 Self-Attention &amp; Transformers in Vision ‣ Transformers in Vision: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>(a), in order to encode global context for a given pixel location, non-local block <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite> computes a <em id="S3.SS1.SSS1.p2.4.1" class="ltx_emph ltx_font_italic">dense</em> attention map (in green). The non-local block <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite> has a high complexity of <math id="S3.SS1.SSS1.p2.1.m1.1" class="ltx_Math" alttext="\mathcal{O}(N^{2})" display="inline"><semantics id="S3.SS1.SSS1.p2.1.m1.1a"><mrow id="S3.SS1.SSS1.p2.1.m1.1.1" xref="S3.SS1.SSS1.p2.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS1.p2.1.m1.1.1.3" xref="S3.SS1.SSS1.p2.1.m1.1.1.3.cmml">𝒪</mi><mo id="S3.SS1.SSS1.p2.1.m1.1.1.2" xref="S3.SS1.SSS1.p2.1.m1.1.1.2.cmml" lspace='0px' rspace='0px'></mo><mrow id="S3.SS1.SSS1.p2.1.m1.1.1.1.1" xref="S3.SS1.SSS1.p2.1.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.SSS1.p2.1.m1.1.1.1.1.2" xref="S3.SS1.SSS1.p2.1.m1.1.1.1.1.1.cmml">(</mo><msup id="S3.SS1.SSS1.p2.1.m1.1.1.1.1.1" xref="S3.SS1.SSS1.p2.1.m1.1.1.1.1.1.cmml"><mi id="S3.SS1.SSS1.p2.1.m1.1.1.1.1.1.2" xref="S3.SS1.SSS1.p2.1.m1.1.1.1.1.1.2.cmml">N</mi><mn id="S3.SS1.SSS1.p2.1.m1.1.1.1.1.1.3" xref="S3.SS1.SSS1.p2.1.m1.1.1.1.1.1.3.cmml">2</mn></msup><mo stretchy="false" id="S3.SS1.SSS1.p2.1.m1.1.1.1.1.3" xref="S3.SS1.SSS1.p2.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p2.1.m1.1b"><apply id="S3.SS1.SSS1.p2.1.m1.1.1.cmml" xref="S3.SS1.SSS1.p2.1.m1.1.1"><times id="S3.SS1.SSS1.p2.1.m1.1.1.2.cmml" xref="S3.SS1.SSS1.p2.1.m1.1.1.2"></times><ci id="S3.SS1.SSS1.p2.1.m1.1.1.3.cmml" xref="S3.SS1.SSS1.p2.1.m1.1.1.3">𝒪</ci><apply id="S3.SS1.SSS1.p2.1.m1.1.1.1.1.1.cmml" xref="S3.SS1.SSS1.p2.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p2.1.m1.1.1.1.1.1.1.cmml" xref="S3.SS1.SSS1.p2.1.m1.1.1.1.1">superscript</csymbol><ci id="S3.SS1.SSS1.p2.1.m1.1.1.1.1.1.2.cmml" xref="S3.SS1.SSS1.p2.1.m1.1.1.1.1.1.2">𝑁</ci><cn type="integer" id="S3.SS1.SSS1.p2.1.m1.1.1.1.1.1.3.cmml" xref="S3.SS1.SSS1.p2.1.m1.1.1.1.1.1.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p2.1.m1.1c">\mathcal{O}(N^{2})</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p2.1.m1.1d">caligraphic_O ( italic_N start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT )</annotation></semantics></math>, where <math id="S3.SS1.SSS1.p2.2.m2.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS1.SSS1.p2.2.m2.1a"><mi id="S3.SS1.SSS1.p2.2.m2.1.1" xref="S3.SS1.SSS1.p2.2.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p2.2.m2.1b"><ci id="S3.SS1.SSS1.p2.2.m2.1.1.cmml" xref="S3.SS1.SSS1.p2.2.m2.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p2.2.m2.1c">N</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p2.2.m2.1d">italic_N</annotation></semantics></math> denotes the number of input feature maps.
To reduce this computational burden, Huang <span id="S3.SS1.SSS1.p2.4.2" class="ltx_text ltx_font_italic">et al.<cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS1.SSS1.p2.4.2.1.1" class="ltx_text ltx_font_upright">[</span><a href="#bib.bib72" title="" class="ltx_ref">72</a><span id="S3.SS1.SSS1.p2.4.2.2.2" class="ltx_text ltx_font_upright">]</span></cite></span> propose the criss-cross attention module that for each pixel position generates a <em id="S3.SS1.SSS1.p2.4.3" class="ltx_emph ltx_font_italic">sparse</em> attention map only on the criss-cross path, as illustrated in Fig. <a href="#S3.F5" title="Figure 5 ‣ 3.1.1 Self-Attention in CNNs ‣ 3.1 Single-head Self-Attention ‣ 3 Self-Attention &amp; Transformers in Vision ‣ Transformers in Vision: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>(b). Further, by applying criss-cross attention recurrently, each pixel position can capture context from all other pixels. Compared to non-local block, the criss-cross uses 11<math id="S3.SS1.SSS1.p2.3.m3.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.SS1.SSS1.p2.3.m3.1a"><mo id="S3.SS1.SSS1.p2.3.m3.1.1" xref="S3.SS1.SSS1.p2.3.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p2.3.m3.1b"><times id="S3.SS1.SSS1.p2.3.m3.1.1.cmml" xref="S3.SS1.SSS1.p2.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p2.3.m3.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p2.3.m3.1d">×</annotation></semantics></math> lesser GPU memory, and has a complexity of <math id="S3.SS1.SSS1.p2.4.m4.1" class="ltx_Math" alttext="\mathcal{O}(2\sqrt{N})" display="inline"><semantics id="S3.SS1.SSS1.p2.4.m4.1a"><mrow id="S3.SS1.SSS1.p2.4.m4.1.1" xref="S3.SS1.SSS1.p2.4.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS1.p2.4.m4.1.1.3" xref="S3.SS1.SSS1.p2.4.m4.1.1.3.cmml">𝒪</mi><mo id="S3.SS1.SSS1.p2.4.m4.1.1.2" xref="S3.SS1.SSS1.p2.4.m4.1.1.2.cmml" lspace='0px' rspace='0px'></mo><mrow id="S3.SS1.SSS1.p2.4.m4.1.1.1.1" xref="S3.SS1.SSS1.p2.4.m4.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.SSS1.p2.4.m4.1.1.1.1.2" xref="S3.SS1.SSS1.p2.4.m4.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS1.SSS1.p2.4.m4.1.1.1.1.1" xref="S3.SS1.SSS1.p2.4.m4.1.1.1.1.1.cmml"><mn id="S3.SS1.SSS1.p2.4.m4.1.1.1.1.1.2" xref="S3.SS1.SSS1.p2.4.m4.1.1.1.1.1.2.cmml">2</mn><mo id="S3.SS1.SSS1.p2.4.m4.1.1.1.1.1.1" xref="S3.SS1.SSS1.p2.4.m4.1.1.1.1.1.1.cmml" lspace='0px' rspace='0px'></mo><msqrt id="S3.SS1.SSS1.p2.4.m4.1.1.1.1.1.3" xref="S3.SS1.SSS1.p2.4.m4.1.1.1.1.1.3.cmml"><mi id="S3.SS1.SSS1.p2.4.m4.1.1.1.1.1.3.2" xref="S3.SS1.SSS1.p2.4.m4.1.1.1.1.1.3.2.cmml">N</mi></msqrt></mrow><mo stretchy="false" id="S3.SS1.SSS1.p2.4.m4.1.1.1.1.3" xref="S3.SS1.SSS1.p2.4.m4.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p2.4.m4.1b"><apply id="S3.SS1.SSS1.p2.4.m4.1.1.cmml" xref="S3.SS1.SSS1.p2.4.m4.1.1"><times id="S3.SS1.SSS1.p2.4.m4.1.1.2.cmml" xref="S3.SS1.SSS1.p2.4.m4.1.1.2"></times><ci id="S3.SS1.SSS1.p2.4.m4.1.1.3.cmml" xref="S3.SS1.SSS1.p2.4.m4.1.1.3">𝒪</ci><apply id="S3.SS1.SSS1.p2.4.m4.1.1.1.1.1.cmml" xref="S3.SS1.SSS1.p2.4.m4.1.1.1.1"><times id="S3.SS1.SSS1.p2.4.m4.1.1.1.1.1.1.cmml" xref="S3.SS1.SSS1.p2.4.m4.1.1.1.1.1.1"></times><cn type="integer" id="S3.SS1.SSS1.p2.4.m4.1.1.1.1.1.2.cmml" xref="S3.SS1.SSS1.p2.4.m4.1.1.1.1.1.2">2</cn><apply id="S3.SS1.SSS1.p2.4.m4.1.1.1.1.1.3.cmml" xref="S3.SS1.SSS1.p2.4.m4.1.1.1.1.1.3"><root id="S3.SS1.SSS1.p2.4.m4.1.1.1.1.1.3a.cmml" xref="S3.SS1.SSS1.p2.4.m4.1.1.1.1.1.3"></root><ci id="S3.SS1.SSS1.p2.4.m4.1.1.1.1.1.3.2.cmml" xref="S3.SS1.SSS1.p2.4.m4.1.1.1.1.1.3.2">𝑁</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p2.4.m4.1c">\mathcal{O}(2\sqrt{N})</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p2.4.m4.1d">caligraphic_O ( 2 square-root start_ARG italic_N end_ARG )</annotation></semantics></math>.
State-of-the-art results are reported <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib72" title="" class="ltx_ref">72</a>]</cite> for the semantic and instance segmentation tasks on several benchmark datasets including Cityscapes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>]</cite>, ADE20K <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib74" title="" class="ltx_ref">74</a>]</cite>, COCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib75" title="" class="ltx_ref">75</a>]</cite>, LIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib76" title="" class="ltx_ref">76</a>]</cite> and CamVid <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib77" title="" class="ltx_ref">77</a>]</cite>.</p>
</div>
<figure id="S3.F5" class="ltx_figure">
<div class="ltx_flex_figure">

<div class="ltx_flex_cell 
                  ltx_flex_size_2">
<figure id="S3.F4.sf1" class="ltx_figure ltx_flex_size_2 ltx_align_center"><img src="/html/2101.01169/assets/Figs/ccnet_a.png" id="S3.F4.sf1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="289" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F4.sf1.3.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S3.F4.sf1.4.2" class="ltx_text" style="font-size:90%;">Non-local block <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite></span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell 
                  ltx_flex_size_2">
<figure id="S3.F4.sf2" class="ltx_figure ltx_flex_size_2 ltx_align_center"><img src="/html/2101.01169/assets/Figs/iccnet_b.png" id="S3.F4.sf2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="248" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F4.sf2.3.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S3.F4.sf2.4.2" class="ltx_text" style="font-size:90%;">Criss-cross attention <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib72" title="" class="ltx_ref">72</a>]</cite> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F5.3.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S3.F5.4.2" class="ltx_text" style="font-size:90%;">Comparison of two different self-attention approaches: Non-local self-attention block <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite> and Criss-cross self-attention module <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib72" title="" class="ltx_ref">72</a>]</cite>. Figure is from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib72" title="" class="ltx_ref">72</a>]</cite>.</span></figcaption>
</figure>
<div id="S3.SS1.SSS1.p3" class="ltx_para">
<p id="S3.SS1.SSS1.p3.1" class="ltx_p">Another shortcoming of the convolutional operator comes from the fact that after training, it applies fixed weights regardless of any changes to the visual input. Hu <span id="S3.SS1.SSS1.p3.1.1" class="ltx_text ltx_font_italic">et al.<cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS1.SSS1.p3.1.1.1.1" class="ltx_text ltx_font_upright">[</span><a href="#bib.bib78" title="" class="ltx_ref">78</a><span id="S3.SS1.SSS1.p3.1.1.2.2" class="ltx_text ltx_font_upright">]</span></cite></span> proposed local relation networks to adaptively compose pixels in a local window. They introduced a new differentiable layer that adapts its weight aggregation based on the compositional relations (similarity) between pixels/features within a local window. Such adaptive weight aggregation introduces geometric priors into the network which are important for the recognition tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite>. Convolution is considered to be a top-down operator as it remains fixed across positions while a non-local operation such as introduced in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite> is a bottom-up method as it aggregates input features over the full image. The local relation layer belongs to the category of bottom-up methods but it is restricted to a fixed window size <em id="S3.SS1.SSS1.p3.1.2" class="ltx_emph ltx_font_italic">e.g.</em>, 7x7 neighborhood.</p>
</div>
<div id="S3.SS1.SSS1.p4" class="ltx_para">
<p id="S3.SS1.SSS1.p4.1" class="ltx_p">Bello <span id="S3.SS1.SSS1.p4.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib79" title="" class="ltx_ref">79</a>]</cite> explore the possibility of employing self-attention as an alternative to convolutional operators. They employ the relative position encoding <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib80" title="" class="ltx_ref">80</a>]</cite> in two dimensions to develop a new self-attention mechanism that maintains translation equivariance, a desirable property for handling images. Although this self-attention provides competitive results as a stand-alone computational primitive, the best performance is obtained in combination with the convolutional operations. Authors show that attention augmentation leads to systematic performance gains in image classification and object detection for different architectures.</p>
</div>
</section>
<section id="S3.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2 </span>Self-Attention as Stand-alone Primitive</h4>

<div id="S3.SS1.SSS2.p1" class="ltx_para">
<p id="S3.SS1.SSS2.p1.1" class="ltx_p">As discussed above, convolutional layers possess translation equivariance but can not scale with a large receptive field, therefore can not capture long-range interactions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib81" title="" class="ltx_ref">81</a>]</cite>. On the other hand, global attention <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> which attend to all spatial locations of the input can be computationally intensive and is preferred on down-sampled small images, image patches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> or augmenting the convolutional features space <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib79" title="" class="ltx_ref">79</a>]</cite>. Ramachandran <span id="S3.SS1.SSS2.p1.1.1" class="ltx_text ltx_font_italic">et al.<cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS1.SSS2.p1.1.1.1.1" class="ltx_text ltx_font_upright">[</span><a href="#bib.bib81" title="" class="ltx_ref">81</a><span id="S3.SS1.SSS2.p1.1.1.2.2" class="ltx_text ltx_font_upright">]</span></cite></span> proposed to replace convolutional layers in deep neural networks with a local self-attention layer which can be applied to small or large inputs without increasing the computational cost. At a basic level, the proposed self-attention layer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib81" title="" class="ltx_ref">81</a>]</cite> considers all pixel positions in a specific window size around a given pixel, compute queries, keys and value vectors for these pixels, and then aggregates the spatial information within this window. The value vectors are aggregated after projecting the softmax score of queries and keys. This process is repeated for all given pixels and the response is concatenated to produce the output pixel. ResNet models with local self-attention layer can solve ImageNet and COCO object detection with fewer parameters as compared to ResNet models based on convolutional layers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib81" title="" class="ltx_ref">81</a>]</cite>.
</p>
</div>
<div id="S3.SS1.SSS2.p2" class="ltx_para">
<p id="S3.SS1.SSS2.p2.1" class="ltx_p">Zhao <span id="S3.SS1.SSS2.p2.1.1" class="ltx_text ltx_font_italic">et al.<cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS1.SSS2.p2.1.1.1.1" class="ltx_text ltx_font_upright">[</span><a href="#bib.bib82" title="" class="ltx_ref">82</a><span id="S3.SS1.SSS2.p2.1.1.2.2" class="ltx_text ltx_font_upright">]</span></cite></span> note that a traditional convolution operator performs feature aggregation and transformation jointly (by applying a filter and then passing it through a non-linearity). In contrast, they propose to perform feature aggregation separately with self-attention followed by transformation using an element-wise perceptron layer. For feature aggregation, they propose two alternate strategies: (a) pairwise self-attention and (b) patch-wise self-attention. The pairwise self-attention is permutation and cardinality invariant operation, while the patch-wise self-attention does not have such invariance properties (similar to convolution). Both pairwise and patch-wise self-attentions are implemented as a <em id="S3.SS1.SSS2.p2.1.2" class="ltx_emph ltx_font_italic">vector</em> attention <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib82" title="" class="ltx_ref">82</a>]</cite> that learns weights for both the spatial and channel dimensions. This provides an alternate approach for attention that is conventionally performed using scalar weights (by taking a dot-product). The pairwise self-attention is a set operator that computes a <em id="S3.SS1.SSS2.p2.1.3" class="ltx_emph ltx_font_italic">vector attention</em> keeping in view the relationships of a particular feature with its neighbors in a given local neighborhood. In contrast, patch-wise self-attention is a generalization of the convolution operator (not a set operator) and looks at all the feature vectors in the local neighbourhood when deriving the attention vectors. Authors show that with considerably fewer parameters, self-attention networks (SAN) can beat ResNet baselines on the ImageNet dataset. They further show robustness against adversarial perturbations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib83" title="" class="ltx_ref">83</a>, <a href="#bib.bib84" title="" class="ltx_ref">84</a>]</cite> and generalization to unseen transformations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib85" title="" class="ltx_ref">85</a>]</cite>. This behaviour is due to the dynamic nature of attention that makes it difficult for the adversary to calculate useful fooling directions.</p>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span><span id="S3.SS2.1.1" class="ltx_text ltx_font_italic" style="color:#000000;">Multi-head Self-Attention (Transformers)</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p"><span id="S3.SS2.p1.1.1" class="ltx_text" style="color:#000000;">Unlike the approaches discussed in Sec. <a href="#S3.SS1" title="3.1 Single-head Self-Attention ‣ 3 Self-Attention &amp; Transformers in Vision ‣ Transformers in Vision: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a> which insert self-attention as a component in CNN inspired architectures, Vision Transformer (ViTs) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> adapts the architecture of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> (see Fig. <a href="#S2.F3" title="Figure 3 ‣ 2 Foundations ‣ Transformers in Vision: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>), which cascades multiple Transformer layers. ViTs have gained significant research attention, and a number of recent approaches have been proposed which build upon ViTs. Below, we discuss these methods by categorizing them into: uniform scale ViTs having single-scale features through all layers (Sec. <a href="#S3.SS2.SSS1" title="3.2.1 Uniform-scale Vision Transformers ‣ 3.2 Multi-head Self-Attention (Transformers) ‣ 3 Self-Attention &amp; Transformers in Vision ‣ Transformers in Vision: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.1</span></a>), multi-scale ViTs that learn hierarchical features which are more suitable for dense prediction tasks (Sec. <a href="#S3.SS2.SSS2" title="3.2.2 Multi-scale Vision Transformers ‣ 3.2 Multi-head Self-Attention (Transformers) ‣ 3 Self-Attention &amp; Transformers in Vision ‣ Transformers in Vision: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.2</span></a>), and hybrid designs having convolution operations within ViTs (Sec. <a href="#S3.SS2.SSS3" title="3.2.3 Hybrid ViTs with Convolutions ‣ 3.2 Multi-head Self-Attention (Transformers) ‣ 3 Self-Attention &amp; Transformers in Vision ‣ Transformers in Vision: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.3</span></a>).</span></p>
</div>
<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span><span id="S3.SS2.SSS1.1.1" class="ltx_text" style="color:#000000;">Uniform-scale Vision Transformers</span>
</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.p1.1" class="ltx_p"><span id="S3.SS2.SSS1.p1.1.1" class="ltx_text" style="color:#000000;">The original Vision Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> model belongs to this family, where the multi-head self-attention is applied to a consistent scale in the input image where the spatial scale is maintained through the network hierarchy. We name such models as the uniform-scale ViTs, as described below.</span></p>
</div>
<div id="S3.SS2.SSS1.p2" class="ltx_para">
<p id="S3.SS2.SSS1.p2.1" class="ltx_p">Vision Transformer (ViT) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> (Fig. <a href="#S3.F6" title="Figure 6 ‣ 3.2.1 Uniform-scale Vision Transformers ‣ 3.2 Multi-head Self-Attention (Transformers) ‣ 3 Self-Attention &amp; Transformers in Vision ‣ Transformers in Vision: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>) is the first work to showcase how Transformers can ‘altogether’ replace standard convolutions in deep neural networks on large-scale image datasets. They applied the original Transformer model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> (with minimal changes) on a sequence of image ’patches’ flattend as vectors. The model was pre-trained on a large propriety dataset (JFT dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite> with 300 million images) and then fine-tuned to downstream recognition benchmarks <em id="S3.SS2.SSS1.p2.1.1" class="ltx_emph ltx_font_italic">e.g.</em>, ImageNet classification. This is an important step since pre-training ViT on a medium-range dataset would not give competitive results, because the CNNs encode prior knowledge about the images (inductive biases <em id="S3.SS2.SSS1.p2.1.2" class="ltx_emph ltx_font_italic">e.g.</em>, translation equivariance) that reduces the need of data as compared to Transformers which must discover such information from very large-scale data. Notably, compared to the iGPT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> model that also applied Transformers to full-sized images but performs training as a generative task, ViT pre-trains the model with a supervised classification task (although a self-supervision variant is also explored which results in a less performance).
</p>
</div>
<figure id="S3.F6" class="ltx_figure"><img src="/html/2101.01169/assets/x2.png" id="S3.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="788" height="395" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F6.3.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S3.F6.4.2" class="ltx_text" style="font-size:90%;">An overview of Vision Transformer (on the <em id="S3.F6.4.2.1" class="ltx_emph ltx_font_italic">left</em>) and the details of Transformer encoder (on the <em id="S3.F6.4.2.2" class="ltx_emph ltx_font_italic">right</em>). The architecture resembles Transformers used in the NLP domain and the image patches are simply fed to the model after flattening. After training, the feature obtained from the first token position is used for classification. Image obtained from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. </span></figcaption>
</figure>
<div id="S3.SS2.SSS1.p3" class="ltx_para">
<p id="S3.SS2.SSS1.p3.1" class="ltx_p">The DeiT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> is the first work to demonstrate that Transformers can be learned on mid-sized datasets (i.e., 1.2 million ImageNet examples compared to 300 million images of JFT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> used in ViT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>) in relatively shorter training episodes.
Besides using augmentation and regularization procedures common in CNNs, the main contribution of DeiT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> is a novel native distillation approach for Transformers which uses a CNN as a teacher model (RegNetY-16GF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib86" title="" class="ltx_ref">86</a>]</cite>) to train the Transformer model. The outputs from the CNN aid the Transformer in efficiently figuring out useful representations for input images. A distillation token is appended with the input patch embeddings and the class token. The self-attention layers operate on these tokens to learn their inter-dependencies and outputs the learned class, patch, and distillation tokens. The network is trained with a cross-entropy loss defined on the output class token and a distillation loss to match the distillation token with the teacher output. Both <em id="S3.SS2.SSS1.p3.1.1" class="ltx_emph ltx_font_italic">soft</em> and <em id="S3.SS2.SSS1.p3.1.2" class="ltx_emph ltx_font_italic">hard</em> label choices were explored for distillation, where the hard distillation was found to perform better. Interestingly, the learned class and distillation tokens do not exhibit a high correlation indicating their complementary nature. The learned representations compare favorably well against top-performing CNN architectures such as EfficientNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib87" title="" class="ltx_ref">87</a>]</cite> and also generalize well for a number of downstream recognition tasks.</p>
</div>
<div id="S3.SS2.SSS1.p4" class="ltx_para">
<p id="S3.SS2.SSS1.p4.2" class="ltx_p">Token to Token (T2T) ViT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> recursively combines neighboring tokens into a single token to reduce tokens length and aggregate spatial context. Transformer in Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib88" title="" class="ltx_ref">88</a>]</cite> computes attention at two levels: patch-level (as done is standard ViTs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>) and local sub-patch-level (<em id="S3.SS2.SSS1.p4.2.1" class="ltx_emph ltx_font_italic">e.g.</em>by subdividing a <math id="S3.SS2.SSS1.p4.1.m1.1" class="ltx_Math" alttext="16\times 16" display="inline"><semantics id="S3.SS2.SSS1.p4.1.m1.1a"><mrow id="S3.SS2.SSS1.p4.1.m1.1.1" xref="S3.SS2.SSS1.p4.1.m1.1.1.cmml"><mn id="S3.SS2.SSS1.p4.1.m1.1.1.2" xref="S3.SS2.SSS1.p4.1.m1.1.1.2.cmml">16</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS1.p4.1.m1.1.1.1" xref="S3.SS2.SSS1.p4.1.m1.1.1.1.cmml">×</mo><mn id="S3.SS2.SSS1.p4.1.m1.1.1.3" xref="S3.SS2.SSS1.p4.1.m1.1.1.3.cmml">16</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p4.1.m1.1b"><apply id="S3.SS2.SSS1.p4.1.m1.1.1.cmml" xref="S3.SS2.SSS1.p4.1.m1.1.1"><times id="S3.SS2.SSS1.p4.1.m1.1.1.1.cmml" xref="S3.SS2.SSS1.p4.1.m1.1.1.1"></times><cn type="integer" id="S3.SS2.SSS1.p4.1.m1.1.1.2.cmml" xref="S3.SS2.SSS1.p4.1.m1.1.1.2">16</cn><cn type="integer" id="S3.SS2.SSS1.p4.1.m1.1.1.3.cmml" xref="S3.SS2.SSS1.p4.1.m1.1.1.3">16</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p4.1.m1.1c">16\times 16</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS1.p4.1.m1.1d">16 × 16</annotation></semantics></math> patch into four <math id="S3.SS2.SSS1.p4.2.m2.1" class="ltx_Math" alttext="4\times 4" display="inline"><semantics id="S3.SS2.SSS1.p4.2.m2.1a"><mrow id="S3.SS2.SSS1.p4.2.m2.1.1" xref="S3.SS2.SSS1.p4.2.m2.1.1.cmml"><mn id="S3.SS2.SSS1.p4.2.m2.1.1.2" xref="S3.SS2.SSS1.p4.2.m2.1.1.2.cmml">4</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS1.p4.2.m2.1.1.1" xref="S3.SS2.SSS1.p4.2.m2.1.1.1.cmml">×</mo><mn id="S3.SS2.SSS1.p4.2.m2.1.1.3" xref="S3.SS2.SSS1.p4.2.m2.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p4.2.m2.1b"><apply id="S3.SS2.SSS1.p4.2.m2.1.1.cmml" xref="S3.SS2.SSS1.p4.2.m2.1.1"><times id="S3.SS2.SSS1.p4.2.m2.1.1.1.cmml" xref="S3.SS2.SSS1.p4.2.m2.1.1.1"></times><cn type="integer" id="S3.SS2.SSS1.p4.2.m2.1.1.2.cmml" xref="S3.SS2.SSS1.p4.2.m2.1.1.2">4</cn><cn type="integer" id="S3.SS2.SSS1.p4.2.m2.1.1.3.cmml" xref="S3.SS2.SSS1.p4.2.m2.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p4.2.m2.1c">4\times 4</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS1.p4.2.m2.1d">4 × 4</annotation></semantics></math> blocks, and computing attention amongst these blocks).
In token labelling ViT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib89" title="" class="ltx_ref">89</a>]</cite>, all patch tokens contribute towards loss calculation, different from regular ViTs that only use classification token in the loss. This process includes auxiliary supervision where each image-patch (token) is labeled using a pre-trained CNN model. Similar to CutMix augmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib90" title="" class="ltx_ref">90</a>]</cite>, tokens from different images are mixed as an augmentation strategy, and the model is trained using the standard classification loss and auxiliary token-label loss. Their model demonstrates excellent performance specially for smaller sized models.</p>
</div>
<div id="S3.SS2.SSS1.p5" class="ltx_para">
<p id="S3.SS2.SSS1.p5.1" class="ltx_p">The quadratic complexity of self-attention hinders its applicability to longer sequences (high-resolution images). Cross-Covariance Image Transformers (XCiT) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib91" title="" class="ltx_ref">91</a>]</cite> incorporate attention across feature-channels instead of tokens, i.e., their cross-covariance attention is given by <math id="S3.SS2.SSS1.p5.1.m1.1" class="ltx_Math" alttext="\mathbf{V}\mathbf{softmax}\left(\frac{\mathbf{K}^{T}\mathbf{Q}^{T}}{\sqrt{\tau}}\right)" display="inline"><semantics id="S3.SS2.SSS1.p5.1.m1.1a"><mrow id="S3.SS2.SSS1.p5.1.m1.1.2" xref="S3.SS2.SSS1.p5.1.m1.1.2.cmml"><mi id="S3.SS2.SSS1.p5.1.m1.1.2.2" xref="S3.SS2.SSS1.p5.1.m1.1.2.2.cmml">𝐕𝐬𝐨𝐟𝐭𝐦𝐚𝐱</mi><mo id="S3.SS2.SSS1.p5.1.m1.1.2.1" xref="S3.SS2.SSS1.p5.1.m1.1.2.1.cmml" lspace='0px' rspace='0px'></mo><mrow id="S3.SS2.SSS1.p5.1.m1.1.2.3.2" xref="S3.SS2.SSS1.p5.1.m1.1.1.cmml"><mo id="S3.SS2.SSS1.p5.1.m1.1.2.3.2.1" xref="S3.SS2.SSS1.p5.1.m1.1.1.cmml">(</mo><mfrac id="S3.SS2.SSS1.p5.1.m1.1.1" xref="S3.SS2.SSS1.p5.1.m1.1.1.cmml"><mrow id="S3.SS2.SSS1.p5.1.m1.1.1.2" xref="S3.SS2.SSS1.p5.1.m1.1.1.2.cmml"><msup id="S3.SS2.SSS1.p5.1.m1.1.1.2.2" xref="S3.SS2.SSS1.p5.1.m1.1.1.2.2.cmml"><mi id="S3.SS2.SSS1.p5.1.m1.1.1.2.2.2" xref="S3.SS2.SSS1.p5.1.m1.1.1.2.2.2.cmml">𝐊</mi><mi id="S3.SS2.SSS1.p5.1.m1.1.1.2.2.3" xref="S3.SS2.SSS1.p5.1.m1.1.1.2.2.3.cmml">T</mi></msup><mo id="S3.SS2.SSS1.p5.1.m1.1.1.2.1" xref="S3.SS2.SSS1.p5.1.m1.1.1.2.1.cmml" lspace='0px' rspace='0px'></mo><msup id="S3.SS2.SSS1.p5.1.m1.1.1.2.3" xref="S3.SS2.SSS1.p5.1.m1.1.1.2.3.cmml"><mi id="S3.SS2.SSS1.p5.1.m1.1.1.2.3.2" xref="S3.SS2.SSS1.p5.1.m1.1.1.2.3.2.cmml">𝐐</mi><mi id="S3.SS2.SSS1.p5.1.m1.1.1.2.3.3" xref="S3.SS2.SSS1.p5.1.m1.1.1.2.3.3.cmml">T</mi></msup></mrow><msqrt id="S3.SS2.SSS1.p5.1.m1.1.1.3" xref="S3.SS2.SSS1.p5.1.m1.1.1.3.cmml"><mi id="S3.SS2.SSS1.p5.1.m1.1.1.3.2" xref="S3.SS2.SSS1.p5.1.m1.1.1.3.2.cmml">τ</mi></msqrt></mfrac><mo id="S3.SS2.SSS1.p5.1.m1.1.2.3.2.2" xref="S3.SS2.SSS1.p5.1.m1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p5.1.m1.1b"><apply id="S3.SS2.SSS1.p5.1.m1.1.2.cmml" xref="S3.SS2.SSS1.p5.1.m1.1.2"><times id="S3.SS2.SSS1.p5.1.m1.1.2.1.cmml" xref="S3.SS2.SSS1.p5.1.m1.1.2.1"></times><ci id="S3.SS2.SSS1.p5.1.m1.1.2.2.cmml" xref="S3.SS2.SSS1.p5.1.m1.1.2.2">𝐕𝐬𝐨𝐟𝐭𝐦𝐚𝐱</ci><apply id="S3.SS2.SSS1.p5.1.m1.1.1.cmml" xref="S3.SS2.SSS1.p5.1.m1.1.2.3.2"><divide id="S3.SS2.SSS1.p5.1.m1.1.1.1.cmml" xref="S3.SS2.SSS1.p5.1.m1.1.2.3.2"></divide><apply id="S3.SS2.SSS1.p5.1.m1.1.1.2.cmml" xref="S3.SS2.SSS1.p5.1.m1.1.1.2"><times id="S3.SS2.SSS1.p5.1.m1.1.1.2.1.cmml" xref="S3.SS2.SSS1.p5.1.m1.1.1.2.1"></times><apply id="S3.SS2.SSS1.p5.1.m1.1.1.2.2.cmml" xref="S3.SS2.SSS1.p5.1.m1.1.1.2.2"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p5.1.m1.1.1.2.2.1.cmml" xref="S3.SS2.SSS1.p5.1.m1.1.1.2.2">superscript</csymbol><ci id="S3.SS2.SSS1.p5.1.m1.1.1.2.2.2.cmml" xref="S3.SS2.SSS1.p5.1.m1.1.1.2.2.2">𝐊</ci><ci id="S3.SS2.SSS1.p5.1.m1.1.1.2.2.3.cmml" xref="S3.SS2.SSS1.p5.1.m1.1.1.2.2.3">𝑇</ci></apply><apply id="S3.SS2.SSS1.p5.1.m1.1.1.2.3.cmml" xref="S3.SS2.SSS1.p5.1.m1.1.1.2.3"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p5.1.m1.1.1.2.3.1.cmml" xref="S3.SS2.SSS1.p5.1.m1.1.1.2.3">superscript</csymbol><ci id="S3.SS2.SSS1.p5.1.m1.1.1.2.3.2.cmml" xref="S3.SS2.SSS1.p5.1.m1.1.1.2.3.2">𝐐</ci><ci id="S3.SS2.SSS1.p5.1.m1.1.1.2.3.3.cmml" xref="S3.SS2.SSS1.p5.1.m1.1.1.2.3.3">𝑇</ci></apply></apply><apply id="S3.SS2.SSS1.p5.1.m1.1.1.3.cmml" xref="S3.SS2.SSS1.p5.1.m1.1.1.3"><root id="S3.SS2.SSS1.p5.1.m1.1.1.3a.cmml" xref="S3.SS2.SSS1.p5.1.m1.1.1.3"></root><ci id="S3.SS2.SSS1.p5.1.m1.1.1.3.2.cmml" xref="S3.SS2.SSS1.p5.1.m1.1.1.3.2">𝜏</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p5.1.m1.1c">\mathbf{V}\mathbf{softmax}\left(\frac{\mathbf{K}^{T}\mathbf{Q}^{T}}{\sqrt{\tau}}\right)</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS1.p5.1.m1.1d">bold_Vsoftmax ( divide start_ARG bold_K start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT bold_Q start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT end_ARG start_ARG square-root start_ARG italic_τ end_ARG end_ARG )</annotation></semantics></math>. The proposed cross-covariance attention has linear complexity (since it depends upon feature dimension instead of the number of tokens). XCiT can therefore handle large resolution images and demonstrate excellent performance across different vision tasks i.e., self-supervised and fully supervised image classification and dense prediction (detection, segmentation). DeepViT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib92" title="" class="ltx_ref">92</a>]</cite> observes that the similarity between attention maps of deeper layer is high and hinders scaling models depth. They propose to re-attend the attention maps in a multi-head block instead of simple aggregation of these attention maps, and show consistent gains over standard multi-head self attention based ViTs.</p>
</div>
</section>
<section id="S3.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Multi-scale Vision Transformers</h4>

<div id="S3.SS2.SSS2.p1" class="ltx_para">
<p id="S3.SS2.SSS2.p1.1" class="ltx_p">In standard ViTs, the number of the tokens and token feature dimension are kept fixed throughout different blocks of the network. This is limiting, since the model is unable to capture fine spatial details at different scales. Initial Transformer based dense prediction methods (e.g., DETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>) therefore have a convolutional backend. Multi-stage hierarchical design for ViTs, where number of tokens is gradually reduced while the token feature dimension is progressively increased, has been shown to produce effective features for dense prediction tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib93" title="" class="ltx_ref">93</a>, <a href="#bib.bib94" title="" class="ltx_ref">94</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib95" title="" class="ltx_ref">95</a>, <a href="#bib.bib96" title="" class="ltx_ref">96</a>]</cite>. These models generally also perform well for recognition tasks. These architectures mostly sparsify tokens by merging neighboring tokens and projecting them to a higher dimensional feature space. Examples of multi-stage ViTs include Pyramid ViT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib93" title="" class="ltx_ref">93</a>, <a href="#bib.bib97" title="" class="ltx_ref">97</a>]</cite>, Twins <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>, CoaT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib98" title="" class="ltx_ref">98</a>]</cite>, Swin Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>, Convolutional vision Transformer (CvT) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib96" title="" class="ltx_ref">96</a>]</cite>, Shuffle Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib95" title="" class="ltx_ref">95</a>]</cite>, CrossFormer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib99" title="" class="ltx_ref">99</a>]</cite>, RegionViT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib100" title="" class="ltx_ref">100</a>]</cite> and Focal Transformer models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib94" title="" class="ltx_ref">94</a>]</cite>. Some of them are hybrid designs (with both convolution and self-attention operations, see Sec. <a href="#S3.SS2.SSS3" title="3.2.3 Hybrid ViTs with Convolutions ‣ 3.2 Multi-head Self-Attention (Transformers) ‣ 3 Self-Attention &amp; Transformers in Vision ‣ Transformers in Vision: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.3</span></a>), while others only employ pure self-attention based design (discussed next).
</p>
</div>
<div id="S3.SS2.SSS2.p2" class="ltx_para">
<p id="S3.SS2.SSS2.p2.1" class="ltx_p">Pyramid ViT (PVT) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib93" title="" class="ltx_ref">93</a>]</cite> is the first hierarchical design for ViT, and proposes a progressive shrinking pyramid and spatial-reduction attention. PVTv2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib97" title="" class="ltx_ref">97</a>]</cite> and SegFormer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib101" title="" class="ltx_ref">101</a>]</cite> improve original PVT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib93" title="" class="ltx_ref">93</a>]</cite> by introducing overlapping patch embedding, depth-wise convolution, and efficient attention. Swin Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> has a multi-stage hierarchical architecture which computes attention within a local window, by partitioning the window into multiple sub-patches. To capture interactions between different windows (image locations), window partitioning is gradually shifted, along the hierarchy of the network, to capture overlapping regions.
Focal Transformer models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib94" title="" class="ltx_ref">94</a>]</cite> is another hierarchical design, where focal self-attention is introduced to simultaneously capture global and local relationships.
Similarly, CrossFormer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib99" title="" class="ltx_ref">99</a>]</cite> has a hierarchical pyramid structure, and introduces cross-scale embedding module, along-with long short distance attention and dynamic position bias to faithfully capture both local and global visual cues. RegionViT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib100" title="" class="ltx_ref">100</a>]</cite> proposes a regional-to-local attention to encode hierarchical features. Multi-Scale Vision Longformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib102" title="" class="ltx_ref">102</a>]</cite> also considers a local context in self-attention, but employs the efficient Longformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib103" title="" class="ltx_ref">103</a>]</cite> design for self-attention. CrossViT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib104" title="" class="ltx_ref">104</a>]</cite> encodes multi-scale features with two branches (each with multiple transformer blocks), by separately processesing smaller and larger image patches. The information from these two multi-scale bracnches is then fused together using a cross-attention module.</p>
</div>
</section>
<section id="S3.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.3 </span>Hybrid ViTs with Convolutions</h4>

<div id="S3.SS2.SSS3.p1" class="ltx_para">
<p id="S3.SS2.SSS3.p1.1" class="ltx_p">Convolutions do an excellent job at capturing low-level local features in images, and have been explored in multiple hybrid ViT designs, specially at the beginning to “patchify and tokenize” an input image. For example, Convolutional vision Transformer (CvT) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib96" title="" class="ltx_ref">96</a>]</cite> incorporate convolution based projection to capture the spatial structure and low-level details, for tokenization of image patches. CvT has a hierarchical design, where number of tokens is progressively reduced while the token-width is increased, thus imitating the impact of spatial downsampling as in CNNs. Convolution enhanced image Transformers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib105" title="" class="ltx_ref">105</a>]</cite> employ convolutions based image-to-token module to extract low-level features. Compact Convolutional Transformer (CCT) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib106" title="" class="ltx_ref">106</a>]</cite> introduces a new sequence pooling scheme, and incorporates convolutional blocks (conv-pool-reshape) for tokenization. CCT can be trained from scratch on smaller datasets, <em id="S3.SS2.SSS3.p1.1.1" class="ltx_emph ltx_font_italic">e.g.</em>, CIFAR10 with <math id="S3.SS2.SSS3.p1.1.m1.1" class="ltx_Math" alttext="\sim 95\%" display="inline"><semantics id="S3.SS2.SSS3.p1.1.m1.1a"><mrow id="S3.SS2.SSS3.p1.1.m1.1.1" xref="S3.SS2.SSS3.p1.1.m1.1.1.cmml"><mi id="S3.SS2.SSS3.p1.1.m1.1.1.2" xref="S3.SS2.SSS3.p1.1.m1.1.1.2.cmml"></mi><mo id="S3.SS2.SSS3.p1.1.m1.1.1.1" xref="S3.SS2.SSS3.p1.1.m1.1.1.1.cmml">∼</mo><mrow id="S3.SS2.SSS3.p1.1.m1.1.1.3" xref="S3.SS2.SSS3.p1.1.m1.1.1.3.cmml"><mn id="S3.SS2.SSS3.p1.1.m1.1.1.3.2" xref="S3.SS2.SSS3.p1.1.m1.1.1.3.2.cmml">95</mn><mo id="S3.SS2.SSS3.p1.1.m1.1.1.3.1" xref="S3.SS2.SSS3.p1.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p1.1.m1.1b"><apply id="S3.SS2.SSS3.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS3.p1.1.m1.1.1"><csymbol cd="latexml" id="S3.SS2.SSS3.p1.1.m1.1.1.1.cmml" xref="S3.SS2.SSS3.p1.1.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S3.SS2.SSS3.p1.1.m1.1.1.2.cmml" xref="S3.SS2.SSS3.p1.1.m1.1.1.2">absent</csymbol><apply id="S3.SS2.SSS3.p1.1.m1.1.1.3.cmml" xref="S3.SS2.SSS3.p1.1.m1.1.1.3"><csymbol cd="latexml" id="S3.SS2.SSS3.p1.1.m1.1.1.3.1.cmml" xref="S3.SS2.SSS3.p1.1.m1.1.1.3.1">percent</csymbol><cn type="integer" id="S3.SS2.SSS3.p1.1.m1.1.1.3.2.cmml" xref="S3.SS2.SSS3.p1.1.m1.1.1.3.2">95</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p1.1.m1.1c">\sim 95\%</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS3.p1.1.m1.1d">∼ 95 %</annotation></semantics></math> accuracy, which is a remarkable property not possible with the traditional ViTs.</p>
</div>
<div id="S3.SS2.SSS3.p2" class="ltx_para">
<p id="S3.SS2.SSS3.p2.4" class="ltx_p">LocalViT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib107" title="" class="ltx_ref">107</a>]</cite> introduces depthwise convolutions to enhance local features modeling capability of ViTs. LeViT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib108" title="" class="ltx_ref">108</a>]</cite> (name inspired from LeNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib109" title="" class="ltx_ref">109</a>]</cite>) applies a four-layered CNN block (with <math id="S3.SS2.SSS3.p2.1.m1.1" class="ltx_Math" alttext="3\times 3" display="inline"><semantics id="S3.SS2.SSS3.p2.1.m1.1a"><mrow id="S3.SS2.SSS3.p2.1.m1.1.1" xref="S3.SS2.SSS3.p2.1.m1.1.1.cmml"><mn id="S3.SS2.SSS3.p2.1.m1.1.1.2" xref="S3.SS2.SSS3.p2.1.m1.1.1.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS3.p2.1.m1.1.1.1" xref="S3.SS2.SSS3.p2.1.m1.1.1.1.cmml">×</mo><mn id="S3.SS2.SSS3.p2.1.m1.1.1.3" xref="S3.SS2.SSS3.p2.1.m1.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p2.1.m1.1b"><apply id="S3.SS2.SSS3.p2.1.m1.1.1.cmml" xref="S3.SS2.SSS3.p2.1.m1.1.1"><times id="S3.SS2.SSS3.p2.1.m1.1.1.1.cmml" xref="S3.SS2.SSS3.p2.1.m1.1.1.1"></times><cn type="integer" id="S3.SS2.SSS3.p2.1.m1.1.1.2.cmml" xref="S3.SS2.SSS3.p2.1.m1.1.1.2">3</cn><cn type="integer" id="S3.SS2.SSS3.p2.1.m1.1.1.3.cmml" xref="S3.SS2.SSS3.p2.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p2.1.m1.1c">3\times 3</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS3.p2.1.m1.1d">3 × 3</annotation></semantics></math> convolutions) at the beginning with progressively increasing channels (3,32,64,128,256). For a <math id="S3.SS2.SSS3.p2.2.m2.1" class="ltx_Math" alttext="3\times 224\times 224" display="inline"><semantics id="S3.SS2.SSS3.p2.2.m2.1a"><mrow id="S3.SS2.SSS3.p2.2.m2.1.1" xref="S3.SS2.SSS3.p2.2.m2.1.1.cmml"><mn id="S3.SS2.SSS3.p2.2.m2.1.1.2" xref="S3.SS2.SSS3.p2.2.m2.1.1.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS3.p2.2.m2.1.1.1" xref="S3.SS2.SSS3.p2.2.m2.1.1.1.cmml">×</mo><mn id="S3.SS2.SSS3.p2.2.m2.1.1.3" xref="S3.SS2.SSS3.p2.2.m2.1.1.3.cmml">224</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS3.p2.2.m2.1.1.1a" xref="S3.SS2.SSS3.p2.2.m2.1.1.1.cmml">×</mo><mn id="S3.SS2.SSS3.p2.2.m2.1.1.4" xref="S3.SS2.SSS3.p2.2.m2.1.1.4.cmml">224</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p2.2.m2.1b"><apply id="S3.SS2.SSS3.p2.2.m2.1.1.cmml" xref="S3.SS2.SSS3.p2.2.m2.1.1"><times id="S3.SS2.SSS3.p2.2.m2.1.1.1.cmml" xref="S3.SS2.SSS3.p2.2.m2.1.1.1"></times><cn type="integer" id="S3.SS2.SSS3.p2.2.m2.1.1.2.cmml" xref="S3.SS2.SSS3.p2.2.m2.1.1.2">3</cn><cn type="integer" id="S3.SS2.SSS3.p2.2.m2.1.1.3.cmml" xref="S3.SS2.SSS3.p2.2.m2.1.1.3">224</cn><cn type="integer" id="S3.SS2.SSS3.p2.2.m2.1.1.4.cmml" xref="S3.SS2.SSS3.p2.2.m2.1.1.4">224</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p2.2.m2.1c">3\times 224\times 224</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS3.p2.2.m2.1d">3 × 224 × 224</annotation></semantics></math> input image, the resulting <math id="S3.SS2.SSS3.p2.3.m3.1" class="ltx_Math" alttext="256\times 14\times 14" display="inline"><semantics id="S3.SS2.SSS3.p2.3.m3.1a"><mrow id="S3.SS2.SSS3.p2.3.m3.1.1" xref="S3.SS2.SSS3.p2.3.m3.1.1.cmml"><mn id="S3.SS2.SSS3.p2.3.m3.1.1.2" xref="S3.SS2.SSS3.p2.3.m3.1.1.2.cmml">256</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS3.p2.3.m3.1.1.1" xref="S3.SS2.SSS3.p2.3.m3.1.1.1.cmml">×</mo><mn id="S3.SS2.SSS3.p2.3.m3.1.1.3" xref="S3.SS2.SSS3.p2.3.m3.1.1.3.cmml">14</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS3.p2.3.m3.1.1.1a" xref="S3.SS2.SSS3.p2.3.m3.1.1.1.cmml">×</mo><mn id="S3.SS2.SSS3.p2.3.m3.1.1.4" xref="S3.SS2.SSS3.p2.3.m3.1.1.4.cmml">14</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p2.3.m3.1b"><apply id="S3.SS2.SSS3.p2.3.m3.1.1.cmml" xref="S3.SS2.SSS3.p2.3.m3.1.1"><times id="S3.SS2.SSS3.p2.3.m3.1.1.1.cmml" xref="S3.SS2.SSS3.p2.3.m3.1.1.1"></times><cn type="integer" id="S3.SS2.SSS3.p2.3.m3.1.1.2.cmml" xref="S3.SS2.SSS3.p2.3.m3.1.1.2">256</cn><cn type="integer" id="S3.SS2.SSS3.p2.3.m3.1.1.3.cmml" xref="S3.SS2.SSS3.p2.3.m3.1.1.3">14</cn><cn type="integer" id="S3.SS2.SSS3.p2.3.m3.1.1.4.cmml" xref="S3.SS2.SSS3.p2.3.m3.1.1.4">14</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p2.3.m3.1c">256\times 14\times 14</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS3.p2.3.m3.1d">256 × 14 × 14</annotation></semantics></math> output from the CNN block becomes input to a hierarchical ViT. By virtue of its design, LeViT is <math id="S3.SS2.SSS3.p2.4.m4.1" class="ltx_math_unparsed" alttext="5\times" display="inline"><semantics id="S3.SS2.SSS3.p2.4.m4.1a"><mrow id="S3.SS2.SSS3.p2.4.m4.1b"><mn id="S3.SS2.SSS3.p2.4.m4.1.1">5</mn><mo lspace="0.222em" id="S3.SS2.SSS3.p2.4.m4.1.2">×</mo></mrow><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p2.4.m4.1c">5\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS3.p2.4.m4.1d">5 ×</annotation></semantics></math> faster than EfficientNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib87" title="" class="ltx_ref">87</a>]</cite> on CPU, at inference. ResT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib110" title="" class="ltx_ref">110</a>]</cite> is another hierarchical architecture which applies a CNN block at the beginning for patch-embedding. It incorporates depth-wise convolutions and adaptive position encoding to tackle varying image sizes. A recent approach NesT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib111" title="" class="ltx_ref">111</a>]</cite> proposes a simple technique to introduce hierarchy in ViTs. NesT divides an image into non-overlapping blocks (each block is further split into patches). It first separately applies local self-attention on patches within each block, and then enables global interaction between blocks by aggregating them into an image space and applying convolution operation, followed by downsampling. The number of blocks is gradually reduced along the hierarchy of the model, while number of local-patches is kept fixed. This simple scheme performs favorably compared with more sophisticated designs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib97" title="" class="ltx_ref">97</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>, and enables training NesT on smaller datasets (e.g., CIFAR-10) from scratch.</p>
</div>
<div id="S3.SS2.SSS3.p3" class="ltx_para">
<p id="S3.SS2.SSS3.p3.1" class="ltx_p">Depthwise Convolution and self-Attention Networks (CoAtNets) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib112" title="" class="ltx_ref">112</a>]</cite> introduce a relative attention module (which combines depthwise convolutions and self-attention), and vertically stack convolution and attention layers. CoAtNets demonstrate an impressive <math id="S3.SS2.SSS3.p3.1.m1.1" class="ltx_Math" alttext="86\%" display="inline"><semantics id="S3.SS2.SSS3.p3.1.m1.1a"><mrow id="S3.SS2.SSS3.p3.1.m1.1.1" xref="S3.SS2.SSS3.p3.1.m1.1.1.cmml"><mn id="S3.SS2.SSS3.p3.1.m1.1.1.2" xref="S3.SS2.SSS3.p3.1.m1.1.1.2.cmml">86</mn><mo id="S3.SS2.SSS3.p3.1.m1.1.1.1" xref="S3.SS2.SSS3.p3.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p3.1.m1.1b"><apply id="S3.SS2.SSS3.p3.1.m1.1.1.cmml" xref="S3.SS2.SSS3.p3.1.m1.1.1"><csymbol cd="latexml" id="S3.SS2.SSS3.p3.1.m1.1.1.1.cmml" xref="S3.SS2.SSS3.p3.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S3.SS2.SSS3.p3.1.m1.1.1.2.cmml" xref="S3.SS2.SSS3.p3.1.m1.1.1.2">86</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p3.1.m1.1c">86\%</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS3.p3.1.m1.1d">86 %</annotation></semantics></math> ImageNet top-1 accuracy without extra data (i.e. trained only on ImageNet-1k). Shuffle Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib95" title="" class="ltx_ref">95</a>]</cite> performs self-attention within a window and has depth-wise convolutions between the window-based multi-head self-attention and MLP. It introduces a shuffle operation to build stronger cross-patch connections.
Co-scale conv-attentional image Transformers (CoaT) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib98" title="" class="ltx_ref">98</a>]</cite>, is a hybrid hierarchical pyramid design, with serial and parallel blocks, where the serial block is similar to standard transformer block except for the attention layer replaced with depthwise convolution. The parallel blocks is applied on the output of serial blocks and encodes relationships between tokens at multiple scales using cross-attention.
Twins <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> builds upon PVT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib93" title="" class="ltx_ref">93</a>]</cite> (an attention only pyramid design), by replacing the absolute position embedding in PVT with relative conditional position embedding <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib113" title="" class="ltx_ref">113</a>]</cite>, and incorporating the separable depth-wise convolutions instead of the standard spatial attention, to capture local and global context of the image. In this sense, the hybrid designs tend to combine the strengths of both convolution and transformer models. TransCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib114" title="" class="ltx_ref">114</a>]</cite> propose a hierarchical multi-head self attention block, which first learns interactions within small grids (tokens) using self-attention, and then gradually merges the smaller grids into larger grids. The proposed block can then be plugged into existing CNN architectures.</p>
</div>
</section>
<section id="S3.SS2.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.4 </span>Self-Supervised Vision Transformers</h4>

<div id="S3.SS2.SSS4.p1" class="ltx_para">
<p id="S3.SS2.SSS4.p1.1" class="ltx_p">Contrastive learning based self-supervised approaches, which have gained significant success for CNN based vision tasks, have also been investigated for ViTs. Chen <span id="S3.SS2.SSS4.p1.1.1" class="ltx_text ltx_font_italic">et al.<cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS2.SSS4.p1.1.1.1.1" class="ltx_text ltx_font_upright">[</span><a href="#bib.bib115" title="" class="ltx_ref">115</a><span id="S3.SS2.SSS4.p1.1.1.2.2" class="ltx_text ltx_font_upright">]</span></cite></span> evaluate different self-supervised frameworks and propose practical strategies including MoCo v3 (extended from v1/v2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib116" title="" class="ltx_ref">116</a>, <a href="#bib.bib117" title="" class="ltx_ref">117</a>]</cite>) for stabilized training of self-supervised ViTs. Xie <span id="S3.SS2.SSS4.p1.1.2" class="ltx_text ltx_font_italic">et al.<cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS2.SSS4.p1.1.2.1.1" class="ltx_text ltx_font_upright">[</span><a href="#bib.bib118" title="" class="ltx_ref">118</a><span id="S3.SS2.SSS4.p1.1.2.2.2" class="ltx_text ltx_font_upright">]</span></cite></span> combine MoCo v2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib117" title="" class="ltx_ref">117</a>]</cite> and BYOL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib119" title="" class="ltx_ref">119</a>]</cite> to train DeiT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> and SwinTransformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>. They demonstrate generalization of self-supervised SwinTransformer for dense prediction tasks of detection and segmentation. Self distillation with no labels (DINO) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib120" title="" class="ltx_ref">120</a>]</cite> demonstrate that self-supervised ViTs can automatically segment the background pixels of an image, even though they were never trained using pixel-level supervision, a phenomena otherwise not observed in CNNs or fully supervised ViTs. Efficient self-supervised vision transformer (EsViT) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib121" title="" class="ltx_ref">121</a>]</cite> propose a multi-stage design, where neighboring tokens are gradually merged along the hierarchy of the network, and use DINO for self-supervision. Apart from standard image-level self-supervision as in DINO, they incorporate additional patch-level self-supervision in which correspondence is promoted between similar patches within augmented versions of an image. EsViT demonstrates excellent performance under self-supervision settings, and its off-the-shelf features transfer better than supervised SwinTransformer on 17 out of 18 evaluated datasets.</p>
</div>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span><span id="S3.SS3.1.1" class="ltx_text ltx_font_italic">Transformers for Object Detection</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p"><span id="S3.SS3.p1.1.1" class="ltx_text" style="color:#000000;">Transformers based modules have been used for object detection in the following manner: (a) Transformer backbones for feature extraction, with a R-CNN based head for detection (see Sec. <a href="#S3.SS2.SSS2" title="3.2.2 Multi-scale Vision Transformers ‣ 3.2 Multi-head Self-Attention (Transformers) ‣ 3 Self-Attention &amp; Transformers in Vision ‣ Transformers in Vision: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.2</span></a>), (b) CNN backbone for visual features and a Transformer based decoder for object detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib122" title="" class="ltx_ref">122</a>, <a href="#bib.bib123" title="" class="ltx_ref">123</a>]</cite> (see Sec. <a href="#S3.SS3.SSS1" title="3.3.1 Detection Transformers with CNN Backbone ‣ 3.3 Transformers for Object Detection ‣ 3 Self-Attention &amp; Transformers in Vision ‣ Transformers in Vision: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3.1</span></a>, and (c) a purely transformer based design for end-to-end object detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib124" title="" class="ltx_ref">124</a>]</cite> (see Sec. <a href="#S3.SS3.SSS2" title="3.3.2 Detection with Pure Transformers ‣ 3.3 Transformers for Object Detection ‣ 3 Self-Attention &amp; Transformers in Vision ‣ Transformers in Vision: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3.2</span></a>). </span></p>
</div>
<section id="S3.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.1 </span>Detection Transformers with CNN Backbone</h4>

<div id="S3.SS3.SSS1.p1" class="ltx_para">
<p id="S3.SS3.SSS1.p1.1" class="ltx_p">Detection Transformer (DETR) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> treats object detection as a set prediction task i.e., given a set of image features, the objective is to predict the set of object bounding boxes. The Transformer model enables the prediction of a set of objects (in a single shot) and also allows modeling their relationships. DETR adapts a set loss function which allows bipartite matching between predictions and ground-truth boxes. The main advantage of DETR is that it removes the dependence on hand-crafted modules and operations, such as the RPN (region proposal network) and NMS (non-maximal suppression) commonly used in object detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib125" title="" class="ltx_ref">125</a>, <a href="#bib.bib126" title="" class="ltx_ref">126</a>, <a href="#bib.bib127" title="" class="ltx_ref">127</a>, <a href="#bib.bib128" title="" class="ltx_ref">128</a>, <a href="#bib.bib129" title="" class="ltx_ref">129</a>]</cite>. In this manner, the dependence on prior knowledge and careful engineering design is relaxed for complex structured tasks like object detection.</p>
</div>
<figure id="S3.F7" class="ltx_figure"><img src="/html/2101.01169/assets/Figs/DETR.png" id="S3.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="111" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F7.3.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="S3.F7.4.2" class="ltx_text" style="font-size:90%;">Detection Transformer (DETR) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> treats the object detection task as a set prediction problem and uses the Transformer network to encode relationships between set elements. A bipartite set loss is used to uniquely match the box predictions with the ground-truth boxes (shown on the <em id="S3.F7.4.2.1" class="ltx_emph ltx_font_italic">right</em> two columns). In case of no match, a ’<em id="S3.F7.4.2.2" class="ltx_emph ltx_font_italic">no object</em>’ class prediction is selected. Its simple design with minimal problem-specific modifications can beat a carefully built and popular Faster R-CNN model. Figure from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>.</span></figcaption>
</figure>
<div id="S3.SS3.SSS1.p2" class="ltx_para">
<p id="S3.SS3.SSS1.p2.4" class="ltx_p">Given spatial feature maps from the CNN backbone, the encoder first flattens the spatial dimensions (see Fig. <a href="#S3.F7" title="Figure 7 ‣ 3.3.1 Detection Transformers with CNN Backbone ‣ 3.3 Transformers for Object Detection ‣ 3 Self-Attention &amp; Transformers in Vision ‣ Transformers in Vision: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>). This gives a sequence of features <math id="S3.SS3.SSS1.p2.1.m1.1" class="ltx_Math" alttext="d\times n" display="inline"><semantics id="S3.SS3.SSS1.p2.1.m1.1a"><mrow id="S3.SS3.SSS1.p2.1.m1.1.1" xref="S3.SS3.SSS1.p2.1.m1.1.1.cmml"><mi id="S3.SS3.SSS1.p2.1.m1.1.1.2" xref="S3.SS3.SSS1.p2.1.m1.1.1.2.cmml">d</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.SSS1.p2.1.m1.1.1.1" xref="S3.SS3.SSS1.p2.1.m1.1.1.1.cmml">×</mo><mi id="S3.SS3.SSS1.p2.1.m1.1.1.3" xref="S3.SS3.SSS1.p2.1.m1.1.1.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p2.1.m1.1b"><apply id="S3.SS3.SSS1.p2.1.m1.1.1.cmml" xref="S3.SS3.SSS1.p2.1.m1.1.1"><times id="S3.SS3.SSS1.p2.1.m1.1.1.1.cmml" xref="S3.SS3.SSS1.p2.1.m1.1.1.1"></times><ci id="S3.SS3.SSS1.p2.1.m1.1.1.2.cmml" xref="S3.SS3.SSS1.p2.1.m1.1.1.2">𝑑</ci><ci id="S3.SS3.SSS1.p2.1.m1.1.1.3.cmml" xref="S3.SS3.SSS1.p2.1.m1.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p2.1.m1.1c">d\times n</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p2.1.m1.1d">italic_d × italic_n</annotation></semantics></math>, where <math id="S3.SS3.SSS1.p2.2.m2.1" class="ltx_Math" alttext="d" display="inline"><semantics id="S3.SS3.SSS1.p2.2.m2.1a"><mi id="S3.SS3.SSS1.p2.2.m2.1.1" xref="S3.SS3.SSS1.p2.2.m2.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p2.2.m2.1b"><ci id="S3.SS3.SSS1.p2.2.m2.1.1.cmml" xref="S3.SS3.SSS1.p2.2.m2.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p2.2.m2.1c">d</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p2.2.m2.1d">italic_d</annotation></semantics></math> is the feature dimension and <math id="S3.SS3.SSS1.p2.3.m3.1" class="ltx_Math" alttext="n=h\times w" display="inline"><semantics id="S3.SS3.SSS1.p2.3.m3.1a"><mrow id="S3.SS3.SSS1.p2.3.m3.1.1" xref="S3.SS3.SSS1.p2.3.m3.1.1.cmml"><mi id="S3.SS3.SSS1.p2.3.m3.1.1.2" xref="S3.SS3.SSS1.p2.3.m3.1.1.2.cmml">n</mi><mo id="S3.SS3.SSS1.p2.3.m3.1.1.1" xref="S3.SS3.SSS1.p2.3.m3.1.1.1.cmml">=</mo><mrow id="S3.SS3.SSS1.p2.3.m3.1.1.3" xref="S3.SS3.SSS1.p2.3.m3.1.1.3.cmml"><mi id="S3.SS3.SSS1.p2.3.m3.1.1.3.2" xref="S3.SS3.SSS1.p2.3.m3.1.1.3.2.cmml">h</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.SSS1.p2.3.m3.1.1.3.1" xref="S3.SS3.SSS1.p2.3.m3.1.1.3.1.cmml">×</mo><mi id="S3.SS3.SSS1.p2.3.m3.1.1.3.3" xref="S3.SS3.SSS1.p2.3.m3.1.1.3.3.cmml">w</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p2.3.m3.1b"><apply id="S3.SS3.SSS1.p2.3.m3.1.1.cmml" xref="S3.SS3.SSS1.p2.3.m3.1.1"><eq id="S3.SS3.SSS1.p2.3.m3.1.1.1.cmml" xref="S3.SS3.SSS1.p2.3.m3.1.1.1"></eq><ci id="S3.SS3.SSS1.p2.3.m3.1.1.2.cmml" xref="S3.SS3.SSS1.p2.3.m3.1.1.2">𝑛</ci><apply id="S3.SS3.SSS1.p2.3.m3.1.1.3.cmml" xref="S3.SS3.SSS1.p2.3.m3.1.1.3"><times id="S3.SS3.SSS1.p2.3.m3.1.1.3.1.cmml" xref="S3.SS3.SSS1.p2.3.m3.1.1.3.1"></times><ci id="S3.SS3.SSS1.p2.3.m3.1.1.3.2.cmml" xref="S3.SS3.SSS1.p2.3.m3.1.1.3.2">ℎ</ci><ci id="S3.SS3.SSS1.p2.3.m3.1.1.3.3.cmml" xref="S3.SS3.SSS1.p2.3.m3.1.1.3.3">𝑤</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p2.3.m3.1c">n=h\times w</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p2.3.m3.1d">italic_n = italic_h × italic_w</annotation></semantics></math> with <math id="S3.SS3.SSS1.p2.4.m4.2" class="ltx_Math" alttext="h,w" display="inline"><semantics id="S3.SS3.SSS1.p2.4.m4.2a"><mrow id="S3.SS3.SSS1.p2.4.m4.2.3.2" xref="S3.SS3.SSS1.p2.4.m4.2.3.1.cmml"><mi id="S3.SS3.SSS1.p2.4.m4.1.1" xref="S3.SS3.SSS1.p2.4.m4.1.1.cmml">h</mi><mo id="S3.SS3.SSS1.p2.4.m4.2.3.2.1" xref="S3.SS3.SSS1.p2.4.m4.2.3.1.cmml">,</mo><mi id="S3.SS3.SSS1.p2.4.m4.2.2" xref="S3.SS3.SSS1.p2.4.m4.2.2.cmml">w</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p2.4.m4.2b"><list id="S3.SS3.SSS1.p2.4.m4.2.3.1.cmml" xref="S3.SS3.SSS1.p2.4.m4.2.3.2"><ci id="S3.SS3.SSS1.p2.4.m4.1.1.cmml" xref="S3.SS3.SSS1.p2.4.m4.1.1">ℎ</ci><ci id="S3.SS3.SSS1.p2.4.m4.2.2.cmml" xref="S3.SS3.SSS1.p2.4.m4.2.2">𝑤</ci></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p2.4.m4.2c">h,w</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p2.4.m4.2d">italic_h , italic_w</annotation></semantics></math> being the height and width of the spatial feature maps. These features are then encoded and decoded using multi-head self-attention modules as in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. The main difference in the decoding stage is that all boxes are predicted in parallel while <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> uses an RNN to predict sequence elements one by one. Since the encoder and decoder are permutation invariant, learned positional encodings are used as the object queries by the decoder to generate different boxes. Note that the spatial structure in a CNN detector (e.g., Faster R-CNN) automatically encodes the positional information. DETR obtains performance comparable to the popular Faster R-CNN model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib125" title="" class="ltx_ref">125</a>]</cite> which is an impressive feat given its simple design. The DETR has also been extended to interesting applications in other domains, e.g., Cell-DETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib130" title="" class="ltx_ref">130</a>]</cite> extends it for instance segmentation of biological cells. A dedicated attention branch is added to obtain instance-wise segmentations in addition box predictions that are enhanced with a CNN decoder to generate accurate instance masks.
</p>
</div>
<div id="S3.SS3.SSS1.p3" class="ltx_para">
<p id="S3.SS3.SSS1.p3.5" class="ltx_p">The DETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> model successfully combines convolutional networks with Transformers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> to remove hand-crafted design requirements and achieves an end-to-end trainable object detection pipeline. However, it struggles to detect small objects and suffers from slow convergence and a relatively high computational cost <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. DETR maps images to features space before using the Transformer for the relation modeling. Thus, the computational cost of self-attention grows quadratically with the spatial size of the feature map i.e., <math id="S3.SS3.SSS1.p3.1.m1.1" class="ltx_Math" alttext="\mathcal{O}(H^{2}W^{2}C)" display="inline"><semantics id="S3.SS3.SSS1.p3.1.m1.1a"><mrow id="S3.SS3.SSS1.p3.1.m1.1.1" xref="S3.SS3.SSS1.p3.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.SSS1.p3.1.m1.1.1.3" xref="S3.SS3.SSS1.p3.1.m1.1.1.3.cmml">𝒪</mi><mo id="S3.SS3.SSS1.p3.1.m1.1.1.2" xref="S3.SS3.SSS1.p3.1.m1.1.1.2.cmml" lspace='0px' rspace='0px'></mo><mrow id="S3.SS3.SSS1.p3.1.m1.1.1.1.1" xref="S3.SS3.SSS1.p3.1.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS3.SSS1.p3.1.m1.1.1.1.1.2" xref="S3.SS3.SSS1.p3.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS3.SSS1.p3.1.m1.1.1.1.1.1" xref="S3.SS3.SSS1.p3.1.m1.1.1.1.1.1.cmml"><msup id="S3.SS3.SSS1.p3.1.m1.1.1.1.1.1.2" xref="S3.SS3.SSS1.p3.1.m1.1.1.1.1.1.2.cmml"><mi id="S3.SS3.SSS1.p3.1.m1.1.1.1.1.1.2.2" xref="S3.SS3.SSS1.p3.1.m1.1.1.1.1.1.2.2.cmml">H</mi><mn id="S3.SS3.SSS1.p3.1.m1.1.1.1.1.1.2.3" xref="S3.SS3.SSS1.p3.1.m1.1.1.1.1.1.2.3.cmml">2</mn></msup><mo id="S3.SS3.SSS1.p3.1.m1.1.1.1.1.1.1" xref="S3.SS3.SSS1.p3.1.m1.1.1.1.1.1.1.cmml" lspace='0px' rspace='0px'></mo><msup id="S3.SS3.SSS1.p3.1.m1.1.1.1.1.1.3" xref="S3.SS3.SSS1.p3.1.m1.1.1.1.1.1.3.cmml"><mi id="S3.SS3.SSS1.p3.1.m1.1.1.1.1.1.3.2" xref="S3.SS3.SSS1.p3.1.m1.1.1.1.1.1.3.2.cmml">W</mi><mn id="S3.SS3.SSS1.p3.1.m1.1.1.1.1.1.3.3" xref="S3.SS3.SSS1.p3.1.m1.1.1.1.1.1.3.3.cmml">2</mn></msup><mo id="S3.SS3.SSS1.p3.1.m1.1.1.1.1.1.1a" xref="S3.SS3.SSS1.p3.1.m1.1.1.1.1.1.1.cmml" lspace='0px' rspace='0px'></mo><mi id="S3.SS3.SSS1.p3.1.m1.1.1.1.1.1.4" xref="S3.SS3.SSS1.p3.1.m1.1.1.1.1.1.4.cmml">C</mi></mrow><mo stretchy="false" id="S3.SS3.SSS1.p3.1.m1.1.1.1.1.3" xref="S3.SS3.SSS1.p3.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p3.1.m1.1b"><apply id="S3.SS3.SSS1.p3.1.m1.1.1.cmml" xref="S3.SS3.SSS1.p3.1.m1.1.1"><times id="S3.SS3.SSS1.p3.1.m1.1.1.2.cmml" xref="S3.SS3.SSS1.p3.1.m1.1.1.2"></times><ci id="S3.SS3.SSS1.p3.1.m1.1.1.3.cmml" xref="S3.SS3.SSS1.p3.1.m1.1.1.3">𝒪</ci><apply id="S3.SS3.SSS1.p3.1.m1.1.1.1.1.1.cmml" xref="S3.SS3.SSS1.p3.1.m1.1.1.1.1"><times id="S3.SS3.SSS1.p3.1.m1.1.1.1.1.1.1.cmml" xref="S3.SS3.SSS1.p3.1.m1.1.1.1.1.1.1"></times><apply id="S3.SS3.SSS1.p3.1.m1.1.1.1.1.1.2.cmml" xref="S3.SS3.SSS1.p3.1.m1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p3.1.m1.1.1.1.1.1.2.1.cmml" xref="S3.SS3.SSS1.p3.1.m1.1.1.1.1.1.2">superscript</csymbol><ci id="S3.SS3.SSS1.p3.1.m1.1.1.1.1.1.2.2.cmml" xref="S3.SS3.SSS1.p3.1.m1.1.1.1.1.1.2.2">𝐻</ci><cn type="integer" id="S3.SS3.SSS1.p3.1.m1.1.1.1.1.1.2.3.cmml" xref="S3.SS3.SSS1.p3.1.m1.1.1.1.1.1.2.3">2</cn></apply><apply id="S3.SS3.SSS1.p3.1.m1.1.1.1.1.1.3.cmml" xref="S3.SS3.SSS1.p3.1.m1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p3.1.m1.1.1.1.1.1.3.1.cmml" xref="S3.SS3.SSS1.p3.1.m1.1.1.1.1.1.3">superscript</csymbol><ci id="S3.SS3.SSS1.p3.1.m1.1.1.1.1.1.3.2.cmml" xref="S3.SS3.SSS1.p3.1.m1.1.1.1.1.1.3.2">𝑊</ci><cn type="integer" id="S3.SS3.SSS1.p3.1.m1.1.1.1.1.1.3.3.cmml" xref="S3.SS3.SSS1.p3.1.m1.1.1.1.1.1.3.3">2</cn></apply><ci id="S3.SS3.SSS1.p3.1.m1.1.1.1.1.1.4.cmml" xref="S3.SS3.SSS1.p3.1.m1.1.1.1.1.1.4">𝐶</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p3.1.m1.1c">\mathcal{O}(H^{2}W^{2}C)</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p3.1.m1.1d">caligraphic_O ( italic_H start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_W start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_C )</annotation></semantics></math>, where <math id="S3.SS3.SSS1.p3.2.m2.1" class="ltx_Math" alttext="H" display="inline"><semantics id="S3.SS3.SSS1.p3.2.m2.1a"><mi id="S3.SS3.SSS1.p3.2.m2.1.1" xref="S3.SS3.SSS1.p3.2.m2.1.1.cmml">H</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p3.2.m2.1b"><ci id="S3.SS3.SSS1.p3.2.m2.1.1.cmml" xref="S3.SS3.SSS1.p3.2.m2.1.1">𝐻</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p3.2.m2.1c">H</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p3.2.m2.1d">italic_H</annotation></semantics></math> and <math id="S3.SS3.SSS1.p3.3.m3.1" class="ltx_Math" alttext="W" display="inline"><semantics id="S3.SS3.SSS1.p3.3.m3.1a"><mi id="S3.SS3.SSS1.p3.3.m3.1.1" xref="S3.SS3.SSS1.p3.3.m3.1.1.cmml">W</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p3.3.m3.1b"><ci id="S3.SS3.SSS1.p3.3.m3.1.1.cmml" xref="S3.SS3.SSS1.p3.3.m3.1.1">𝑊</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p3.3.m3.1c">W</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p3.3.m3.1d">italic_W</annotation></semantics></math> represent the height and width of the feature map. This inherently puts a limitation on the use of multi-scale hierarchical features <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib131" title="" class="ltx_ref">131</a>]</cite> in DETR training framework which is ultimately important to detect small objects. Furthermore, at the beginning of training, the attention module simply projects uniform attention to all the locations of the feature map and requires a large number of training epochs to tune attention weights to converge to meaningfully sparse locations. This approach contributes to a slow convergence rate of DETR. To mitigate the above-mentioned issues, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> proposed a deformable attention module to process the feature maps. Inspired from deformable convolutions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>, deformable attention module <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> only attends to sparse set of elements from the whole feature map regardless of its spatial size. This further allows cross-scale aggregation of feature maps with the help of multi-scale attention modules without increasing the computational cost significantly. Deformable DETR not only performs better but its training time also remains 10<math id="S3.SS3.SSS1.p3.4.m4.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.SS3.SSS1.p3.4.m4.1a"><mo id="S3.SS3.SSS1.p3.4.m4.1.1" xref="S3.SS3.SSS1.p3.4.m4.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p3.4.m4.1b"><times id="S3.SS3.SSS1.p3.4.m4.1.1.cmml" xref="S3.SS3.SSS1.p3.4.m4.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p3.4.m4.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p3.4.m4.1d">×</annotation></semantics></math> lower than the original DETR model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>.
<span id="S3.SS3.SSS1.p3.5.1" class="ltx_text" style="color:#000000;">Anchor DETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib122" title="" class="ltx_ref">122</a>]</cite> replaces the learnable query tokens in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> with anchor-point based queries, such that each query focuses on predicting the object near the anchor point. The anchor points can be fixed on 2D grid, or learned from uniformly distributed points. Anchor DETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib122" title="" class="ltx_ref">122</a>]</cite> requires 10 <math id="S3.SS3.SSS1.p3.5.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.SS3.SSS1.p3.5.1.m1.1a"><mo mathcolor="#000000" id="S3.SS3.SSS1.p3.5.1.m1.1.1" xref="S3.SS3.SSS1.p3.5.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p3.5.1.m1.1b"><times id="S3.SS3.SSS1.p3.5.1.m1.1.1.cmml" xref="S3.SS3.SSS1.p3.5.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p3.5.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p3.5.1.m1.1d">×</annotation></semantics></math> fewer training epochs with comparable performance.</span>
<span id="S3.SS3.SSS1.p3.5.2" class="ltx_text" style="color:#000000;">Pix2Seq <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib123" title="" class="ltx_ref">123</a>]</cite> is a generic Transformer-based framework, without any specialized task-specific modules, and learns to directly produce a sequence of tokens with object descriptions (bounding-boxes and class-labels). A quantization and serialization scheme first converts bounding boxes and class-labels into a sequence of discrete tokens. A generic Transformer based encoder-decoder network is then used to generate these tokens in an auto-regressive manner conditioned on previous predictions and image features.</span></p>
</div>
</section>
<section id="S3.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.2 </span>Detection with Pure Transformers</h4>

<div id="S3.SS3.SSS2.p1" class="ltx_para">
<p id="S3.SS3.SSS2.p1.1" class="ltx_p"><span id="S3.SS3.SSS2.p1.1.1" class="ltx_text" style="color:#000000;">You Only Look at One Sequence (YOLOS) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib124" title="" class="ltx_ref">124</a>]</cite> is a simple, attention-only architecture directly built upon the ViT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib132" title="" class="ltx_ref">132</a>, <a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. It replaces the class-token in ViT with multiple learnable object query tokens, and the bipartite matching loss is used for object detection similar to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. YOLOS demonstrates the flexibility of ViTs to object detection, in a pure sequence-to-sequence learning manner, with minimal image related 2D inductive biases. In similar spirit, PVT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib93" title="" class="ltx_ref">93</a>]</cite> is combined with DETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> to perform object detection with an end-to-end transformer pipeline. We note that it is feasible to combine other recent ViTs with transformer based detection heads as well to create pure ViT based designs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib124" title="" class="ltx_ref">124</a>]</cite>, and we hope to see more such efforts in future. </span>
</p>
</div>
<figure id="S3.F8" class="ltx_figure"><img src="/html/2101.01169/assets/Figs/AxialAttention.png" id="S3.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="184" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F8.3.1.1" class="ltx_text" style="font-size:90%;">Figure 8</span>: </span><span id="S3.F8.4.2" class="ltx_text" style="font-size:90%;">Axial attention module <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib133" title="" class="ltx_ref">133</a>]</cite> that sequentially applies multi-head axial attention operations along height and width axes. Image from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib133" title="" class="ltx_ref">133</a>]</cite>.</span></figcaption>
</figure>
</section>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span><span id="S3.SS4.1.1" class="ltx_text ltx_font_italic">Transformers for Segmentation</span>
</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">Self-attention can be leveraged for dense prediction tasks like image segmentation that requires modeling rich interactions between pixels. Below, we discuss axial self-attention <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib133" title="" class="ltx_ref">133</a>]</cite>, a cross-modal approach <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> that can segment regions corresponding to a given language expression, and ViTs based segmentation architectures <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib134" title="" class="ltx_ref">134</a>, <a href="#bib.bib101" title="" class="ltx_ref">101</a>, <a href="#bib.bib135" title="" class="ltx_ref">135</a>]</cite>.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.1" class="ltx_p">Panoptic segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib136" title="" class="ltx_ref">136</a>]</cite> aims to jointly solve the otherwise distinct tasks of semantic segmentation and instance segmentation by assigning each pixel a semantic label and an instance id. Global context can provide useful cues to deal with such a complex visual understanding task. Self-attention is effective at modeling long-range contextual information, albeit applying it to large inputs for a dense prediction task like panoptic segmentation is prohibitively expensive. A naive solution is to apply self-attention either to downsampled inputs or to limited regions around each pixel <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib81" title="" class="ltx_ref">81</a>]</cite>. Even after introducing these constraints, the self-attention still has quadratic complexity and sacrifices the global context.
To tackle these issues, Wang <span id="S3.SS4.p2.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib133" title="" class="ltx_ref">133</a>]</cite> propose the position-sensitive axial-attention where the 2D self-attention mechanism is reformulated as two 1D axial-attention layers, applied to height-axis and width-axis sequentially (see Fig. <a href="#S3.F8" title="Figure 8 ‣ 3.3.2 Detection with Pure Transformers ‣ 3.3 Transformers for Object Detection ‣ 3 Self-Attention &amp; Transformers in Vision ‣ Transformers in Vision: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>). The axial-attention is compute efficient and enables models to capture the full-image context. It achieves competitive performance for the panoptic segmentation task on COCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib75" title="" class="ltx_ref">75</a>]</cite>, Mapillary Vistas <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib137" title="" class="ltx_ref">137</a>]</cite>, and Cityscapes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>]</cite> benchmarks and for the image classification on ImageNet dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib138" title="" class="ltx_ref">138</a>]</cite>.</p>
</div>
<div id="S3.SS4.p3" class="ltx_para">
<p id="S3.SS4.p3.1" class="ltx_p">Cross-modal Self-attention (CMSA) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> encodes long-range multi-modal dependencies between linguistic and visual features for <span id="S3.SS4.p3.1.1" class="ltx_text ltx_font_italic">referring image segmentation task</span>, that aims to segment entities in an image referred by a language description.For this purpose, a set of cross-modal features is obtained by concatenating image features with each word embedding and the spatial coordinate features. The self-attention operates on these features and generates attention over the image corresponding to each word in the sentence. The segmentation network then performs self-attention at multiple spatial levels and uses a gated multi-level fusion module to refine segmentation masks via information exchange across multi-resolution features. A binary CE loss is used to train the overall model that achieves good improvements on UNC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib139" title="" class="ltx_ref">139</a>]</cite>, G-Ref <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib140" title="" class="ltx_ref">140</a>]</cite> and ReferIt <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib141" title="" class="ltx_ref">141</a>]</cite> datasets.</p>
</div>
<div id="S3.SS4.p4" class="ltx_para">
<p id="S3.SS4.p4.1" class="ltx_p"><span id="S3.SS4.p4.1.1" class="ltx_text" style="color:#000000;">While the segmentation approaches discussed above insert self-attention in their CNN based architectures, some recent works have proposed transformer based encoder-decoder architectures. Segmentation Transformer (SETR) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib134" title="" class="ltx_ref">134</a>]</cite> has a ViT encoder, and two decoder designs based upon progressive upsampling, and multi-level feature aggregation.
SegFormer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib101" title="" class="ltx_ref">101</a>]</cite> has a hierarchical pyramid ViT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib93" title="" class="ltx_ref">93</a>]</cite> (without position encoding) as an encoder, and a simple MLP based decoder with upsampling operation to get the segmentation mask.
Segmenter <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib135" title="" class="ltx_ref">135</a>]</cite> uses ViT encoder to extract image features, and the decoder is a mask Transformer module which predicts segmentation masks, using learnable mask tokens and image-patch tokens as inputs. The authors also propose a baseline linear decoder which projects the patch-embeddings to classification space, thus producing coarse patch-level labels.
</span></p>
</div>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span><span id="S3.SS5.1.1" class="ltx_text ltx_font_italic">Transformers for Image and Scene Generation</span>
</h3>

<div id="S3.SS5.p1" class="ltx_para">
<p id="S3.SS5.p1.1" class="ltx_p">Here, we discuss Transformer-based architectures <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib142" title="" class="ltx_ref">142</a>, <a href="#bib.bib143" title="" class="ltx_ref">143</a>, <a href="#bib.bib144" title="" class="ltx_ref">144</a>, <a href="#bib.bib145" title="" class="ltx_ref">145</a>, <a href="#bib.bib146" title="" class="ltx_ref">146</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> for image synthesis, which is interesting from the perspective of generative modeling and learning unsupervised representations for down-stream tasks.</p>
</div>
<div id="S3.SS5.p2" class="ltx_para">
<p id="S3.SS5.p2.1" class="ltx_p">Parmar <span id="S3.SS5.p2.1.1" class="ltx_text ltx_font_italic">et al.<cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS5.p2.1.1.1.1" class="ltx_text ltx_font_upright">[</span><a href="#bib.bib142" title="" class="ltx_ref">142</a><span id="S3.SS5.p2.1.1.2.2" class="ltx_text ltx_font_upright">]</span></cite></span> develop an image generation model that can sequentially predict each pixel of an output image given its previously generated pixels (Fig. <a href="#S3.F9" title="Figure 9 ‣ 3.5 Transformers for Image and Scene Generation ‣ 3 Self-Attention &amp; Transformers in Vision ‣ Transformers in Vision: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>). Their approach models the joint distribution of the image pixels by factorizing it as a product of pixel-wise conditional distributions. Previously developed auto-regressive models for this task, such as the PixelCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib147" title="" class="ltx_ref">147</a>]</cite>, suffer from a limited receptive field which hinders in modeling long term relationships in an image <em id="S3.SS5.p2.1.2" class="ltx_emph ltx_font_italic">e.g.</em>, part relationships or occlusions. Using self-attention, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib142" title="" class="ltx_ref">142</a>]</cite> enhances the receptive field without incurring a high computational cost (<em id="S3.SS5.p2.1.3" class="ltx_emph ltx_font_italic">e.g.</em>, effective receptive field up to 256 pixels can be achieved as compared to 25 pixels of PixelCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib147" title="" class="ltx_ref">147</a>]</cite>). The generative pipeline was also tested on conditional generation tasks <em id="S3.SS5.p2.1.4" class="ltx_emph ltx_font_italic">e.g.</em>, image super-resolution, image completion, and denoising.</p>
</div>
<figure id="S3.F9" class="ltx_figure"><img src="/html/2101.01169/assets/x3.png" id="S3.F9.g1" class="ltx_graphics ltx_centering ltx_img_square" width="598" height="547" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F9.8.4.1" class="ltx_text" style="font-size:90%;">Figure 9</span>: </span><span id="S3.F9.6.3" class="ltx_text" style="font-size:90%;">(a) Self-attention block in Image Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib142" title="" class="ltx_ref">142</a>]</cite>. Given one channel for a pixel <math id="S3.F9.4.1.m1.1" class="ltx_Math" alttext="q" display="inline"><semantics id="S3.F9.4.1.m1.1b"><mi id="S3.F9.4.1.m1.1.1" xref="S3.F9.4.1.m1.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S3.F9.4.1.m1.1c"><ci id="S3.F9.4.1.m1.1.1.cmml" xref="S3.F9.4.1.m1.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F9.4.1.m1.1d">q</annotation><annotation encoding="application/x-llamapun" id="S3.F9.4.1.m1.1e">italic_q</annotation></semantics></math>, the block attends to the memory of previous synthesized pixels (<math id="S3.F9.5.2.m2.1" class="ltx_Math" alttext="m_{i}" display="inline"><semantics id="S3.F9.5.2.m2.1b"><msub id="S3.F9.5.2.m2.1.1" xref="S3.F9.5.2.m2.1.1.cmml"><mi id="S3.F9.5.2.m2.1.1.2" xref="S3.F9.5.2.m2.1.1.2.cmml">m</mi><mi id="S3.F9.5.2.m2.1.1.3" xref="S3.F9.5.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.F9.5.2.m2.1c"><apply id="S3.F9.5.2.m2.1.1.cmml" xref="S3.F9.5.2.m2.1.1"><csymbol cd="ambiguous" id="S3.F9.5.2.m2.1.1.1.cmml" xref="S3.F9.5.2.m2.1.1">subscript</csymbol><ci id="S3.F9.5.2.m2.1.1.2.cmml" xref="S3.F9.5.2.m2.1.1.2">𝑚</ci><ci id="S3.F9.5.2.m2.1.1.3.cmml" xref="S3.F9.5.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F9.5.2.m2.1d">m_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.F9.5.2.m2.1e">italic_m start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>), followed by a feed-forward sub-network. Positional encodings <math id="S3.F9.6.3.m3.1" class="ltx_Math" alttext="p_{i}" display="inline"><semantics id="S3.F9.6.3.m3.1b"><msub id="S3.F9.6.3.m3.1.1" xref="S3.F9.6.3.m3.1.1.cmml"><mi id="S3.F9.6.3.m3.1.1.2" xref="S3.F9.6.3.m3.1.1.2.cmml">p</mi><mi id="S3.F9.6.3.m3.1.1.3" xref="S3.F9.6.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.F9.6.3.m3.1c"><apply id="S3.F9.6.3.m3.1.1.cmml" xref="S3.F9.6.3.m3.1.1"><csymbol cd="ambiguous" id="S3.F9.6.3.m3.1.1.1.cmml" xref="S3.F9.6.3.m3.1.1">subscript</csymbol><ci id="S3.F9.6.3.m3.1.1.2.cmml" xref="S3.F9.6.3.m3.1.1.2">𝑝</ci><ci id="S3.F9.6.3.m3.1.1.3.cmml" xref="S3.F9.6.3.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F9.6.3.m3.1d">p_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.F9.6.3.m3.1e">italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> are added in the first layer. (b) The operation performed in Local Self-Attention (example of a 2D case is shown). The image is partitioned into a grid of spatial blocks known as query blocks. In the self-attention operation, each pixel in a query block attends to all pixels in the memory block (shown in cyan rectangle). White grid locations show masked inputs that have zero-contribution towards the self-attention.</span></figcaption>
</figure>
<div id="S3.SS5.p3" class="ltx_para">
<p id="S3.SS5.p3.2" class="ltx_p">Inspired by the success of GPT model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> in the language domain, image GPT (iGPT) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib143" title="" class="ltx_ref">143</a>]</cite> demonstrated that such models can be directly used for image generation tasks, and to learn strong features for downstream vision tasks (e.g., image classification). Specifically, iGPT trains GPT v2 model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> on flattened image sequences (1D pixel arrays) and shows that it can generate plausible image outputs without any external supervision. The generated samples depict the model’s ability to understand spatial relationships between pixels and high-level attributes such as object classes, texture, and scale. Notably, the design does not use any image-specific knowledge in the design (e.g., the 2D position embeddings used in Image Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib142" title="" class="ltx_ref">142</a>]</cite>).
The features learned with iGPT’s unsupervised training mechanism compete impressively against other unsupervised approaches, achieving state-of-the-art performance on CIFAR-10/100 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib148" title="" class="ltx_ref">148</a>]</cite> and STL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib149" title="" class="ltx_ref">149</a>]</cite> datasets while performing comparably to SimCLR (a contrastive learning approach) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib150" title="" class="ltx_ref">150</a>]</cite> on ImageNet dataset. This is an astounding result, since the iGPT architecture is exactly the same as used for language modeling tasks, and therefore it does not incorporate any prior domain-specific knowledge. Notably, the competing unsupervised CNN based solutions widely adopt such priors in the form of architectural design, attention mechanisms, loss functions, and regularization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib151" title="" class="ltx_ref">151</a>, <a href="#bib.bib152" title="" class="ltx_ref">152</a>, <a href="#bib.bib117" title="" class="ltx_ref">117</a>, <a href="#bib.bib153" title="" class="ltx_ref">153</a>, <a href="#bib.bib154" title="" class="ltx_ref">154</a>]</cite>. However, on the downside, iGPT has a high compute cost <em id="S3.SS5.p3.2.1" class="ltx_emph ltx_font_italic">e.g.</em>, iGPT-L version has roughly <math id="S3.SS5.p3.1.m1.1" class="ltx_math_unparsed" alttext="36\times" display="inline"><semantics id="S3.SS5.p3.1.m1.1a"><mrow id="S3.SS5.p3.1.m1.1b"><mn id="S3.SS5.p3.1.m1.1.1">36</mn><mo lspace="0.222em" id="S3.SS5.p3.1.m1.1.2">×</mo></mrow><annotation encoding="application/x-tex" id="S3.SS5.p3.1.m1.1c">36\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p3.1.m1.1d">36 ×</annotation></semantics></math> high training cost compared to MoCo <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib117" title="" class="ltx_ref">117</a>]</cite> which is a state of the art self-supervised feature learning approach. For this reason, the training was generally limited to low-resolution of <math id="S3.SS5.p3.2.m2.1" class="ltx_Math" alttext="\leq 64\times 64" display="inline"><semantics id="S3.SS5.p3.2.m2.1a"><mrow id="S3.SS5.p3.2.m2.1.1" xref="S3.SS5.p3.2.m2.1.1.cmml"><mi id="S3.SS5.p3.2.m2.1.1.2" xref="S3.SS5.p3.2.m2.1.1.2.cmml"></mi><mo id="S3.SS5.p3.2.m2.1.1.1" xref="S3.SS5.p3.2.m2.1.1.1.cmml">≤</mo><mrow id="S3.SS5.p3.2.m2.1.1.3" xref="S3.SS5.p3.2.m2.1.1.3.cmml"><mn id="S3.SS5.p3.2.m2.1.1.3.2" xref="S3.SS5.p3.2.m2.1.1.3.2.cmml">64</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS5.p3.2.m2.1.1.3.1" xref="S3.SS5.p3.2.m2.1.1.3.1.cmml">×</mo><mn id="S3.SS5.p3.2.m2.1.1.3.3" xref="S3.SS5.p3.2.m2.1.1.3.3.cmml">64</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.p3.2.m2.1b"><apply id="S3.SS5.p3.2.m2.1.1.cmml" xref="S3.SS5.p3.2.m2.1.1"><leq id="S3.SS5.p3.2.m2.1.1.1.cmml" xref="S3.SS5.p3.2.m2.1.1.1"></leq><csymbol cd="latexml" id="S3.SS5.p3.2.m2.1.1.2.cmml" xref="S3.SS5.p3.2.m2.1.1.2">absent</csymbol><apply id="S3.SS5.p3.2.m2.1.1.3.cmml" xref="S3.SS5.p3.2.m2.1.1.3"><times id="S3.SS5.p3.2.m2.1.1.3.1.cmml" xref="S3.SS5.p3.2.m2.1.1.3.1"></times><cn type="integer" id="S3.SS5.p3.2.m2.1.1.3.2.cmml" xref="S3.SS5.p3.2.m2.1.1.3.2">64</cn><cn type="integer" id="S3.SS5.p3.2.m2.1.1.3.3.cmml" xref="S3.SS5.p3.2.m2.1.1.3.3">64</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p3.2.m2.1c">\leq 64\times 64</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p3.2.m2.1d">≤ 64 × 64</annotation></semantics></math>, while convolutional architectures can effectively learn from high-resolution inputs.</p>
</div>
<div id="S3.SS5.p4" class="ltx_para">
<p id="S3.SS5.p4.1" class="ltx_p">Transformers typically incur a high compute cost when applied on high-dimensional sequences. To overcome this limitation, Esser <span id="S3.SS5.p4.1.1" class="ltx_text ltx_font_italic">et al.<cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS5.p4.1.1.1.1" class="ltx_text ltx_font_upright">[</span><a href="#bib.bib144" title="" class="ltx_ref">144</a><span id="S3.SS5.p4.1.1.2.2" class="ltx_text ltx_font_upright">]</span></cite></span> proposed to include inductive biases (commonly used in the CNNs) alongside Transformers to improve their efficiency. Specifically, local connectivity and spatial invariance biases inbuilt in the CNN structure are leveraged by learning a rich dictionary of visual patterns (using a Generative Adversarial approach). A Transformer is then used to learn the long-range interactions between the dictionary items to generate the outputs. In turn, they develop a conditional image generation model capable of producing very high-resolution images (up to megapixel range) using Transformers. This is the first work that demonstrates the application of Transformers to generate such high-resolution images.</p>
</div>
<div id="S3.SS5.p5" class="ltx_para">
<p id="S3.SS5.p5.1" class="ltx_p">Generative Adversarial Networks (GANs) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite> with CNNs as default backbone have been very successful for visually appealing image synthesis <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib155" title="" class="ltx_ref">155</a>, <a href="#bib.bib156" title="" class="ltx_ref">156</a>, <a href="#bib.bib157" title="" class="ltx_ref">157</a>]</cite>. TransGAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib145" title="" class="ltx_ref">145</a>]</cite> builds a strong GAN model, free of any convolution operation, with both generator and discriminator based upon the Transformer model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. The architecture of both generator and discriminator is based upon the encoder in original Transformer model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. For memory efficiency, the generator contains multiple stages, with up-sampling modules in-between, which gradually increase the resolution of feature maps (input sequence length) while reducing the embedding dimension. The discriminator of TransGAN takes flattened image-patches as tokens similar to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib132" title="" class="ltx_ref">132</a>]</cite>.
Authors introduce different training techniques including data augmentation, training with an auxiliary task and injecting locality to self-attention to scale-up their model for high quality image synthesis <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib144" title="" class="ltx_ref">144</a>]</cite>. The TransGAN model achieves state-of-the-art results in terms of Inception Score and Fréchet Inception Distance (FID) on STL-10 and performs favorably compared with their CNN-based GAN counterparts on other datasets.</p>
</div>
<div id="S3.SS5.p6" class="ltx_para">
<p id="S3.SS5.p6.1" class="ltx_p">Unlike previous image generation methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib142" title="" class="ltx_ref">142</a>, <a href="#bib.bib143" title="" class="ltx_ref">143</a>, <a href="#bib.bib144" title="" class="ltx_ref">144</a>]</cite>, which directly predict image outputs, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> learns to generate parameters of 3D objects to be placed in a given scene. Specifically, SceneFormer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> studies the 3D room layout conditioned scene generation task. Given the empty room shape, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> can propose new object configurations in the room while maintaining realism. Remarkably, the model does not use any appearance information and only learns to generate new scenes by modeling the inter-object relationships using self-attention in Transformers. Similar to how a Transformer operates on a sentence, it is applied to a sequence of objects to predict the next suitable object in a scene. Specifically, the size, pose, location, and category of the next object is predicted by the Transformer model. A start token indicates the initiation of inference and the number of output token indicate the objects generated by the model in a sequence. The authors also explore generating new scenes given a textual description of the room layout. The independence from the appearance makes the approach efficient, enabling interactive scene generation.</p>
</div>
<div id="S3.SS5.p7" class="ltx_para">
<p id="S3.SS5.p7.2" class="ltx_p">The task of generating realistic images from text is interesting and practically valuable (<em id="S3.SS5.p7.2.1" class="ltx_emph ltx_font_italic">e.g.</em>, for artistic content creation), but at the same time highly challenging. Prior text-to-image synthesis approaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib158" title="" class="ltx_ref">158</a>, <a href="#bib.bib159" title="" class="ltx_ref">159</a>, <a href="#bib.bib160" title="" class="ltx_ref">160</a>, <a href="#bib.bib161" title="" class="ltx_ref">161</a>]</cite> are mostly based on GANs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>. Although these methods produce encouraging results, they are far from being photo-realistic.
Ramesh <span id="S3.SS5.p7.2.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> recently proposed DALL·E which is a Transformer model capable of generating high-fidelity images from a given text description.
DALL·E model has 12 billion parameters and it is trained on a large set of text-image pairs taken from the internet.
Before training, images are first resized to 256<math id="S3.SS5.p7.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.SS5.p7.1.m1.1a"><mo id="S3.SS5.p7.1.m1.1.1" xref="S3.SS5.p7.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS5.p7.1.m1.1b"><times id="S3.SS5.p7.1.m1.1.1.cmml" xref="S3.SS5.p7.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p7.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p7.1.m1.1d">×</annotation></semantics></math>256 resolution, and subsequently compressed to a 32<math id="S3.SS5.p7.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.SS5.p7.2.m2.1a"><mo id="S3.SS5.p7.2.m2.1.1" xref="S3.SS5.p7.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS5.p7.2.m2.1b"><times id="S3.SS5.p7.2.m2.1.1.cmml" xref="S3.SS5.p7.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p7.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p7.2.m2.1d">×</annotation></semantics></math>32 grid of latent codes using a pre-trained discrete variational autoencoder <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib162" title="" class="ltx_ref">162</a>, <a href="#bib.bib163" title="" class="ltx_ref">163</a>]</cite>.
DALL·E takes as input a single stream of 1280 tokens (256 for the text and 1024 for the image), and is trained to generate all other tokens autoregressively (one after another). It provides flexibility to generate images either from scratch (Fig. <a href="#S3.F9.sf1" title="9(a) ‣ Figure 10 ‣ 3.5 Transformers for Image and Scene Generation ‣ 3 Self-Attention &amp; Transformers in Vision ‣ Transformers in Vision: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9(a)</span></a>) or by extending existing images (Fig. <a href="#S3.F9.sf2" title="9(b) ‣ Figure 10 ‣ 3.5 Transformers for Image and Scene Generation ‣ 3 Self-Attention &amp; Transformers in Vision ‣ Transformers in Vision: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9(b)</span></a>), while staying faithful to the text caption.</p>
</div>
<figure id="S3.F10" class="ltx_figure">
<div class="ltx_flex_figure">

<div class="ltx_flex_cell 
                  ltx_flex_size_many">
<figure id="S3.F9.sf1" class="ltx_figure ltx_flex_size_7 ltx_flex_size_many ltx_align_center"><img src="/html/2101.01169/assets/Figs/scratch1.png" id="S3.F9.sf1.g1" class="ltx_graphics ltx_img_square" width="598" height="620" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F9.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell 
                  ltx_flex_size_many">
<figure id="S3.F9.sf2" class="ltx_figure ltx_flex_size_7 ltx_flex_size_many ltx_align_center"><img src="/html/2101.01169/assets/Figs/img_comp.png" id="S3.F9.sf2.g1" class="ltx_graphics ltx_img_square" width="598" height="586" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F9.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell 
                  ltx_flex_size_many">
<figure id="S3.F9.sf3" class="ltx_figure ltx_flex_size_7 ltx_flex_size_many ltx_align_center"><img src="/html/2101.01169/assets/Figs/multi-attributes.png" id="S3.F9.sf3.g1" class="ltx_graphics ltx_img_square" width="598" height="600" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F9.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell 
                  ltx_flex_size_many">
<figure id="S3.F9.sf4" class="ltx_figure ltx_flex_size_7 ltx_flex_size_many ltx_align_center"><img src="/html/2101.01169/assets/Figs/viewpoint.png" id="S3.F9.sf4.g1" class="ltx_graphics ltx_img_square" width="598" height="595" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F9.sf4.2.1.1" class="ltx_text" style="font-size:90%;">(d)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell 
                  ltx_flex_size_many">
<figure id="S3.F9.sf5" class="ltx_figure ltx_flex_size_7 ltx_flex_size_many ltx_align_center"><img src="/html/2101.01169/assets/Figs/internel_structure.png" id="S3.F9.sf5.g1" class="ltx_graphics ltx_img_square" width="598" height="587" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F9.sf5.2.1.1" class="ltx_text" style="font-size:90%;">(e)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell 
                  ltx_flex_size_many">
<figure id="S3.F9.sf6" class="ltx_figure ltx_flex_size_7 ltx_flex_size_many ltx_align_center"><img src="/html/2101.01169/assets/Figs/unrelated_concepts.png" id="S3.F9.sf6.g1" class="ltx_graphics ltx_img_square" width="598" height="595" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F9.sf6.2.1.1" class="ltx_text" style="font-size:90%;">(f)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell 
                  ltx_flex_size_many">
<figure id="S3.F9.sf7" class="ltx_figure ltx_flex_size_7 ltx_flex_size_many ltx_align_center"><img src="/html/2101.01169/assets/Figs/img2img.png" id="S3.F9.sf7.g1" class="ltx_graphics ltx_img_square" width="598" height="608" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F9.sf7.2.1.1" class="ltx_text" style="font-size:90%;">(g)</span> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F10.3.1.1" class="ltx_text" style="font-size:90%;">Figure 10</span>: </span><span id="S3.F10.4.2" class="ltx_text" style="font-size:90%;">Images generated by DALL·E <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> from the following text prompts. (a) <em id="S3.F10.4.2.1" class="ltx_emph ltx_font_italic">An armchair in the shape of an avocado.</em> (b) <em id="S3.F10.4.2.2" class="ltx_emph ltx_font_italic">A photo of San Francisco’s golden gate bridge.</em> Given a part of the image (in green box), DALL·E performs the image completion. (c) <em id="S3.F10.4.2.3" class="ltx_emph ltx_font_italic">An emoji of a baby penguin wearing a blue hat, red gloves, green shirt, and yellow pants.</em> (d) <em id="S3.F10.4.2.4" class="ltx_emph ltx_font_italic">An extreme close-up view of a capybara sitting in a field.</em> (e) <span id="S3.F10.4.2.5" class="ltx_text ltx_font_italic"> A cross-section view of a pomegranate.</span> (f) <em id="S3.F10.4.2.6" class="ltx_emph ltx_font_italic">A penguin made of watermelon.</em> (g) <em id="S3.F10.4.2.7" class="ltx_emph ltx_font_italic">The exact same cat on the top as a sketch on the bottom.</em> </span></figcaption>
</figure>
<div id="S3.SS5.p8" class="ltx_para">
<p id="S3.SS5.p8.1" class="ltx_p">The authors demonstrate the effectiveness of DALL·E by creating images from text describing a wide variety of real and fictional concepts. While generating images purely from textural captions, DALL·E shows impressive performance at controlling multiple objects and their attributes (Fig. <a href="#S3.F9.sf3" title="9(c) ‣ Figure 10 ‣ 3.5 Transformers for Image and Scene Generation ‣ 3 Self-Attention &amp; Transformers in Vision ‣ Transformers in Vision: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9(c)</span></a>), rendering certain viewpoint (Fig. <a href="#S3.F9.sf4" title="9(d) ‣ Figure 10 ‣ 3.5 Transformers for Image and Scene Generation ‣ 3 Self-Attention &amp; Transformers in Vision ‣ Transformers in Vision: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9(d)</span></a>), capturing object’s internal structure (Fig. <a href="#S3.F9.sf5" title="9(e) ‣ Figure 10 ‣ 3.5 Transformers for Image and Scene Generation ‣ 3 Self-Attention &amp; Transformers in Vision ‣ Transformers in Vision: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9(e)</span></a>), and combining unrelated objects (Fig. <a href="#S3.F9.sf6" title="9(f) ‣ Figure 10 ‣ 3.5 Transformers for Image and Scene Generation ‣ 3 Self-Attention &amp; Transformers in Vision ‣ Transformers in Vision: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9(f)</span></a>).
Furthermore, DALL·E can perform image-to-image translation (Fig. <a href="#S3.F9.sf7" title="9(g) ‣ Figure 10 ‣ 3.5 Transformers for Image and Scene Generation ‣ 3 Self-Attention &amp; Transformers in Vision ‣ Transformers in Vision: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9(g)</span></a>) guided by the input text.</p>
</div>
</section>
<section id="S3.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.6 </span><span id="S3.SS6.1.1" class="ltx_text ltx_font_italic">Transformers for Low-level Vision</span>
</h3>

<div id="S3.SS6.p1" class="ltx_para">
<p id="S3.SS6.p1.1" class="ltx_p">After witnessing the success of Transformer models in high-level vision problems, numerous Transformer-based methods have been proposed for low-level vision tasks, including image super-resolution <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib164" title="" class="ltx_ref">164</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, denoising <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib165" title="" class="ltx_ref">165</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, deraining <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib165" title="" class="ltx_ref">165</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, and colorization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>.
Image restoration requires pixel-to-pixel correspondence from the input to the output images. One major goal of restoration algorithms is to preserve desired fine image details (such as edges and texture) in the restored images. CNNs achieve this by employing a single-scale architecture design that does not involve any downsampling operation. Since the computational complexity of self-attention in Transformer models increases quadratically with number of image patches, it is infeasible to develop Transformer model that can operate on single-scale feature processing pipeline. Consequently, these Transformer-based image restoration models make use of various strategies to reduce the computational burden, such as computing attention on local image windows <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib164" title="" class="ltx_ref">164</a>]</cite>, performing spatial reduction attention <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib166" title="" class="ltx_ref">166</a>]</cite>, and employing encoder-decoder design <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib165" title="" class="ltx_ref">165</a>]</cite>. Here, we briefly discuss a few image restoration Transformer models.</p>
</div>
<section id="S3.SS6.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.6.1 </span>Transformers for Image Processing Tasks</h4>

<div id="S3.SS6.SSS1.p1" class="ltx_para">
<p id="S3.SS6.SSS1.p1.1" class="ltx_p">Top performing algorithms for high-level computer vision tasks such as object detection and semantic segmentation often employ backbone models that are pre-trained on large-scale datasets <em id="S3.SS6.SSS1.p1.1.1" class="ltx_emph ltx_font_italic">e.g.</em>, ImageNet. In contrast, algorithms for low-level vision tasks such as image denoising, super-resolution, and deraining are directly trained on task-specific data, thereby suffer from these limitations: <span id="S3.SS6.SSS1.p1.1.2" class="ltx_text ltx_font_bold">(i)</span> small number of images available in task-specific datasets (<em id="S3.SS6.SSS1.p1.1.3" class="ltx_emph ltx_font_italic">e.g.</em>, the commonly used DIV2K dataset for image super-resolution contains only 2000 images), <span id="S3.SS6.SSS1.p1.1.4" class="ltx_text ltx_font_bold">(ii)</span> the model trained for one image processing task does not adapt well to other related tasks.</p>
</div>
<div id="S3.SS6.SSS1.p2" class="ltx_para">
<p id="S3.SS6.SSS1.p2.1" class="ltx_p">Chen <span id="S3.SS6.SSS1.p2.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> propose a pre-trained model based on Transformer architecture, named as Image Processing Transformer (IPT). It is capable of performing various image restoration tasks such as super-resolution, denoising, and deraining.
The overall architecture of IPT consists of multi-heads and multi-tails to deal with different tasks separately, and a shared encoder-decoder Transformer body.
Since exploiting Transformers at full potential requires training on large-scale data, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> takes the clean (ground-truth) images from the ImageNet benchmark and synthesize their degraded versions for different tasks. For example, bicubic interpolation is used for generating low-resolution images, additive white Gaussian noise is added to prepare noisy data, and hand-crafted rain streaks are applied to obtain rainy images. In total, 10 million images are used to pre-train the IPT model. During training, each task-specific head takes as input a degraded image and generates visual features. These feature maps are divided into small crops and subsequently flattened before feeding them to the Transformer encoder (whose architecture is the same as <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>). The outputs of the encoder along with the task-specific embeddings are given as input to the Transformer decoder. The features from the decoder output are reshaped and passed to the multi-tail that yields restored images. The IPT model is optimized with L<math id="S3.SS6.SSS1.p2.1.m1.1" class="ltx_Math" alttext="{}_{1}" display="inline"><semantics id="S3.SS6.SSS1.p2.1.m1.1a"><msub id="S3.SS6.SSS1.p2.1.m1.1.1" xref="S3.SS6.SSS1.p2.1.m1.1.1.cmml"><mi id="S3.SS6.SSS1.p2.1.m1.1.1a" xref="S3.SS6.SSS1.p2.1.m1.1.1.cmml"></mi><mn id="S3.SS6.SSS1.p2.1.m1.1.1.1" xref="S3.SS6.SSS1.p2.1.m1.1.1.1.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS6.SSS1.p2.1.m1.1b"><apply id="S3.SS6.SSS1.p2.1.m1.1.1.cmml" xref="S3.SS6.SSS1.p2.1.m1.1.1"><cn type="integer" id="S3.SS6.SSS1.p2.1.m1.1.1.1.cmml" xref="S3.SS6.SSS1.p2.1.m1.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.SSS1.p2.1.m1.1c">{}_{1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS6.SSS1.p2.1.m1.1d">start_FLOATSUBSCRIPT 1 end_FLOATSUBSCRIPT</annotation></semantics></math> loss. Experimental results show that the pre-trained IPT model, when fine-tuned for a specific low-level vision task, can provide significant performance gains over the state-of-the-art methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib167" title="" class="ltx_ref">167</a>, <a href="#bib.bib168" title="" class="ltx_ref">168</a>, <a href="#bib.bib169" title="" class="ltx_ref">169</a>]</cite>.</p>
</div>
</section>
<section id="S3.SS6.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.6.2 </span>Transformers for Super-Resolution</h4>

<div id="S3.SS6.SSS2.p1" class="ltx_para">
<p id="S3.SS6.SSS2.p1.1" class="ltx_p">Recent years have seen major performance breakthroughs for super-resolution (SR) due to convolutional neural networks (CNNs). Principally, the quality of super-resolved images generated by CNNs is dependent on the choice of optimization objective. While the SR methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib170" title="" class="ltx_ref">170</a>, <a href="#bib.bib171" title="" class="ltx_ref">171</a>, <a href="#bib.bib172" title="" class="ltx_ref">172</a>, <a href="#bib.bib167" title="" class="ltx_ref">167</a>, <a href="#bib.bib173" title="" class="ltx_ref">173</a>]</cite> that are based on pixel-wise loss functions (<em id="S3.SS6.SSS2.p1.1.1" class="ltx_emph ltx_font_italic">e.g.</em>, L1, MSE, etc.) yield impressive results in terms of image fidelity metrics such as PSNR and SSIM, they struggle to recover fine texture details and often produce images that are overly-smooth and perceptually less pleasant.
Further, <em id="S3.SS6.SSS2.p1.1.2" class="ltx_emph ltx_font_italic">perceptual</em> SR approaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib174" title="" class="ltx_ref">174</a>, <a href="#bib.bib175" title="" class="ltx_ref">175</a>, <a href="#bib.bib176" title="" class="ltx_ref">176</a>, <a href="#bib.bib177" title="" class="ltx_ref">177</a>, <a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite>, in addition to per-pixel loss, employ adversarial loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite> and perceptual loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib178" title="" class="ltx_ref">178</a>]</cite> based on deep features extracted from pre-trained CNNs. While these methods generate images that are sharp, visually pleasant, and perceptually plausible, they show a substantial decrease in reconstruction accuracy measured in PSNR/SSIM. Moreover, the perceptual SR algorithms have a tendency to hallucinate fake textures and cause artifacts. The above mentioned SR approaches follow two distinct (but conflicting) research directions: one maximizing the reconstruction accuracy and the other maximizing the perceptual quality, but never both.</p>
</div>
<figure id="S3.F11" class="ltx_figure"><img src="/html/2101.01169/assets/Figs/texture_transformer.png" id="S3.F11.g1" class="ltx_graphics ltx_centering ltx_img_square" width="449" height="458" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F11.16.8.1" class="ltx_text" style="font-size:90%;">Figure 11</span>: </span><span id="S3.F11.14.7" class="ltx_text" style="font-size:90%;">Diagram of the texture Transformer module. <math id="S3.F11.8.1.m1.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S3.F11.8.1.m1.1b"><mi id="S3.F11.8.1.m1.1.1" xref="S3.F11.8.1.m1.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.F11.8.1.m1.1c"><ci id="S3.F11.8.1.m1.1.1.cmml" xref="S3.F11.8.1.m1.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F11.8.1.m1.1d">Q</annotation><annotation encoding="application/x-llamapun" id="S3.F11.8.1.m1.1e">italic_Q</annotation></semantics></math> (query), <math id="S3.F11.9.2.m2.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S3.F11.9.2.m2.1b"><mi id="S3.F11.9.2.m2.1.1" xref="S3.F11.9.2.m2.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.F11.9.2.m2.1c"><ci id="S3.F11.9.2.m2.1.1.cmml" xref="S3.F11.9.2.m2.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F11.9.2.m2.1d">K</annotation><annotation encoding="application/x-llamapun" id="S3.F11.9.2.m2.1e">italic_K</annotation></semantics></math> (key) and <math id="S3.F11.10.3.m3.1" class="ltx_Math" alttext="V" display="inline"><semantics id="S3.F11.10.3.m3.1b"><mi id="S3.F11.10.3.m3.1.1" xref="S3.F11.10.3.m3.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S3.F11.10.3.m3.1c"><ci id="S3.F11.10.3.m3.1.1.cmml" xref="S3.F11.10.3.m3.1.1">𝑉</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F11.10.3.m3.1d">V</annotation><annotation encoding="application/x-llamapun" id="S3.F11.10.3.m3.1e">italic_V</annotation></semantics></math> (value) represent texture features extracted from a (bicubic upsampled) low-resolution image, a sequentially down/upsampled reference image, and an original reference image, respectively. The relevance embedding aims to estimate similarity between low-resolution and reference images. <math id="S3.F11.11.4.m4.1" class="ltx_Math" alttext="H" display="inline"><semantics id="S3.F11.11.4.m4.1b"><mi id="S3.F11.11.4.m4.1.1" xref="S3.F11.11.4.m4.1.1.cmml">H</mi><annotation-xml encoding="MathML-Content" id="S3.F11.11.4.m4.1c"><ci id="S3.F11.11.4.m4.1.1.cmml" xref="S3.F11.11.4.m4.1.1">𝐻</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F11.11.4.m4.1d">H</annotation><annotation encoding="application/x-llamapun" id="S3.F11.11.4.m4.1e">italic_H</annotation></semantics></math> and <math id="S3.F11.12.5.m5.1" class="ltx_Math" alttext="S" display="inline"><semantics id="S3.F11.12.5.m5.1b"><mi id="S3.F11.12.5.m5.1.1" xref="S3.F11.12.5.m5.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S3.F11.12.5.m5.1c"><ci id="S3.F11.12.5.m5.1.1.cmml" xref="S3.F11.12.5.m5.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F11.12.5.m5.1d">S</annotation><annotation encoding="application/x-llamapun" id="S3.F11.12.5.m5.1e">italic_S</annotation></semantics></math> respectively denote hard and soft attentions computed from relevance embedding. <math id="S3.F11.13.6.m6.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S3.F11.13.6.m6.1b"><mi id="S3.F11.13.6.m6.1.1" xref="S3.F11.13.6.m6.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.F11.13.6.m6.1c"><ci id="S3.F11.13.6.m6.1.1.cmml" xref="S3.F11.13.6.m6.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F11.13.6.m6.1d">T</annotation><annotation encoding="application/x-llamapun" id="S3.F11.13.6.m6.1e">italic_T</annotation></semantics></math> indicates high-resolution texture features that are then transferred to the features <math id="S3.F11.14.7.m7.1" class="ltx_Math" alttext="F" display="inline"><semantics id="S3.F11.14.7.m7.1b"><mi id="S3.F11.14.7.m7.1.1" xref="S3.F11.14.7.m7.1.1.cmml">F</mi><annotation-xml encoding="MathML-Content" id="S3.F11.14.7.m7.1c"><ci id="S3.F11.14.7.m7.1.1.cmml" xref="S3.F11.14.7.m7.1.1">𝐹</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F11.14.7.m7.1d">F</annotation><annotation encoding="application/x-llamapun" id="S3.F11.14.7.m7.1e">italic_F</annotation></semantics></math> of low-resolution image. Figure is from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>.</span></figcaption>
</figure>
<div id="S3.SS6.SSS2.p2" class="ltx_para">
<p id="S3.SS6.SSS2.p2.4" class="ltx_p">To alleviate the trade-off between perceptual reproduction and accurate reproduction, Yang <span id="S3.SS6.SSS2.p2.4.1" class="ltx_text ltx_font_italic">et al.<cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS6.SSS2.p2.4.1.1.1" class="ltx_text ltx_font_upright">[</span><a href="#bib.bib16" title="" class="ltx_ref">16</a><span id="S3.SS6.SSS2.p2.4.1.2.2" class="ltx_text ltx_font_upright">]</span></cite></span> propose a Transformer network (TTSR) for super-resolution.
During training, TTSR uses paired LR-HR images, as well as reference (Ref) images with similar content as of LR images.
TTSR learns to search relevant regions in the Ref image and transfers rich textures to help super-resolving the input LR image.
The texture Transformer module of TTSR method (see Fig. <a href="#S3.F11" title="Figure 11 ‣ 3.6.2 Transformers for Super-Resolution ‣ 3.6 Transformers for Low-level Vision ‣ 3 Self-Attention &amp; Transformers in Vision ‣ Transformers in Vision: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>) consists of four core components: (1) <em id="S3.SS6.SSS2.p2.4.2" class="ltx_emph ltx_font_italic">Learnable texture extractor:</em> takes as input LR<math id="S3.SS6.SSS2.p2.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S3.SS6.SSS2.p2.1.m1.1a"><mo stretchy="false" id="S3.SS6.SSS2.p2.1.m1.1.1" xref="S3.SS6.SSS2.p2.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.SS6.SSS2.p2.1.m1.1b"><ci id="S3.SS6.SSS2.p2.1.m1.1.1.cmml" xref="S3.SS6.SSS2.p2.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.SSS2.p2.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S3.SS6.SSS2.p2.1.m1.1d">↑</annotation></semantics></math>, Ref<math id="S3.SS6.SSS2.p2.2.m2.1" class="ltx_Math" alttext="\downarrow\uparrow" display="inline"><semantics id="S3.SS6.SSS2.p2.2.m2.1a"><mrow id="S3.SS6.SSS2.p2.2.m2.1.1" xref="S3.SS6.SSS2.p2.2.m2.1.1.cmml"><mi id="S3.SS6.SSS2.p2.2.m2.1.1.2" xref="S3.SS6.SSS2.p2.2.m2.1.1.2.cmml"></mi><mo rspace="0em" stretchy="false" id="S3.SS6.SSS2.p2.2.m2.1.1.1" xref="S3.SS6.SSS2.p2.2.m2.1.1.1.cmml">↓</mo><mo lspace="0em" stretchy="false" id="S3.SS6.SSS2.p2.2.m2.1.1.3" xref="S3.SS6.SSS2.p2.2.m2.1.1.3.cmml">↑</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS6.SSS2.p2.2.m2.1b"><apply id="S3.SS6.SSS2.p2.2.m2.1.1.cmml" xref="S3.SS6.SSS2.p2.2.m2.1.1"><ci id="S3.SS6.SSS2.p2.2.m2.1.1.1.cmml" xref="S3.SS6.SSS2.p2.2.m2.1.1.1">↓</ci><csymbol cd="latexml" id="S3.SS6.SSS2.p2.2.m2.1.1.2.cmml" xref="S3.SS6.SSS2.p2.2.m2.1.1.2">absent</csymbol><ci id="S3.SS6.SSS2.p2.2.m2.1.1.3.cmml" xref="S3.SS6.SSS2.p2.2.m2.1.1.3">↑</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.SSS2.p2.2.m2.1c">\downarrow\uparrow</annotation><annotation encoding="application/x-llamapun" id="S3.SS6.SSS2.p2.2.m2.1d">↓ ↑</annotation></semantics></math>, and Ref images, and generates texture features query (Q), key (K), and value (V), respectively. Here, <math id="S3.SS6.SSS2.p2.3.m3.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S3.SS6.SSS2.p2.3.m3.1a"><mo stretchy="false" id="S3.SS6.SSS2.p2.3.m3.1.1" xref="S3.SS6.SSS2.p2.3.m3.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.SS6.SSS2.p2.3.m3.1b"><ci id="S3.SS6.SSS2.p2.3.m3.1.1.cmml" xref="S3.SS6.SSS2.p2.3.m3.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.SSS2.p2.3.m3.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S3.SS6.SSS2.p2.3.m3.1d">↑</annotation></semantics></math> denotes bicubic upsampling operation, and <math id="S3.SS6.SSS2.p2.4.m4.1" class="ltx_Math" alttext="\downarrow\uparrow" display="inline"><semantics id="S3.SS6.SSS2.p2.4.m4.1a"><mrow id="S3.SS6.SSS2.p2.4.m4.1.1" xref="S3.SS6.SSS2.p2.4.m4.1.1.cmml"><mi id="S3.SS6.SSS2.p2.4.m4.1.1.2" xref="S3.SS6.SSS2.p2.4.m4.1.1.2.cmml"></mi><mo rspace="0em" stretchy="false" id="S3.SS6.SSS2.p2.4.m4.1.1.1" xref="S3.SS6.SSS2.p2.4.m4.1.1.1.cmml">↓</mo><mo lspace="0em" stretchy="false" id="S3.SS6.SSS2.p2.4.m4.1.1.3" xref="S3.SS6.SSS2.p2.4.m4.1.1.3.cmml">↑</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS6.SSS2.p2.4.m4.1b"><apply id="S3.SS6.SSS2.p2.4.m4.1.1.cmml" xref="S3.SS6.SSS2.p2.4.m4.1.1"><ci id="S3.SS6.SSS2.p2.4.m4.1.1.1.cmml" xref="S3.SS6.SSS2.p2.4.m4.1.1.1">↓</ci><csymbol cd="latexml" id="S3.SS6.SSS2.p2.4.m4.1.1.2.cmml" xref="S3.SS6.SSS2.p2.4.m4.1.1.2">absent</csymbol><ci id="S3.SS6.SSS2.p2.4.m4.1.1.3.cmml" xref="S3.SS6.SSS2.p2.4.m4.1.1.3">↑</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.SSS2.p2.4.m4.1c">\downarrow\uparrow</annotation><annotation encoding="application/x-llamapun" id="S3.SS6.SSS2.p2.4.m4.1d">↓ ↑</annotation></semantics></math> represents bicubic down-sampling followed by an upsampling operation. (2) <em id="S3.SS6.SSS2.p2.4.3" class="ltx_emph ltx_font_italic">Relevance embedding:</em> first unfolds Q and K into patches and then computes the similarity of each patch in Q with each patch in K in order to generate hard and soft attention maps. (3) <em id="S3.SS6.SSS2.p2.4.4" class="ltx_emph ltx_font_italic">Hard-attention:</em> transfers HR texture features from V to (LR features) Q using the hard attention map. (4) <em id="S3.SS6.SSS2.p2.4.5" class="ltx_emph ltx_font_italic">Soft-attention:</em> further enhances relevant features while suppressing less relevant ones.</p>
</div>
<div id="S3.SS6.SSS2.p3" class="ltx_para">
<p id="S3.SS6.SSS2.p3.1" class="ltx_p">While TTSR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> method deals with reference-based image super-resolution, most of the research is conducted on single image super-resolution problem in which only LR-HR paired images are available. Since the computational complexity of the original self-attention operation is prohibitively high for high-resolution images, recently a few efficient transformer models have been proposed that employ window-based attention (SwinIR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib164" title="" class="ltx_ref">164</a>]</cite>) and spatial resolution reduction operation in attention module (ESRT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib166" title="" class="ltx_ref">166</a>]</cite>) to perform super-resolution.</p>
</div>
</section>
<section id="S3.SS6.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.6.3 </span>Colorization Transformer</h4>

<div id="S3.SS6.SSS3.p1" class="ltx_para">
<p id="S3.SS6.SSS3.p1.1" class="ltx_p">Given a grayscale image, colorization seeks to produce the corresponding colorized sample. It is a one-to-many task as for a given grayscale input, there exist many possibilities in the colorized output space. The challenging nature of this task requires probabilistic models capable of producing multiple colorized output samples. Colorization Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> is a probabilistic model based on conditional attention mechanism <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib179" title="" class="ltx_ref">179</a>]</cite>. It divides the image colorization task into three sub-problems and proposes to solve each task sequentially by a different Transformer network. The authors first train a Transformer network to map a low-resolution grey-scale image to a 3-bit low-resolution colored image. Low-resolution images in turn allow training of larger models. The 3-bit low-resolution colored image is then upsampled to an 8-bit RGB sample by another Transformer network in the second stage of training. Finally, a third stage Transformer is trained to increase the spatial resolution of the 8-bit RGB sample produced by the second-stage Transformer. Self-attention used in the colorization Transformer is based on row/column attention layers introduced in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib179" title="" class="ltx_ref">179</a>]</cite>. These layers capture the interaction between each pixel of an input image while being computationally less costly. The row-wise attention layer applies self-attention to all pixels in a given row, while the column-wise attention layer considers pixels only in a given column of an image. This work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> is the first successful application of Transformers trained to colorize grey-scale images at high (256<math id="S3.SS6.SSS3.p1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.SS6.SSS3.p1.1.m1.1a"><mo id="S3.SS6.SSS3.p1.1.m1.1.1" xref="S3.SS6.SSS3.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS6.SSS3.p1.1.m1.1b"><times id="S3.SS6.SSS3.p1.1.m1.1.1.cmml" xref="S3.SS6.SSS3.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.SSS3.p1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS6.SSS3.p1.1.m1.1d">×</annotation></semantics></math>256) resolution.</p>
</div>
<figure id="S3.F12" class="ltx_figure"><img src="/html/2101.01169/assets/Figs/Multi-modal_Archs_border.png" id="S3.F12.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="403" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F12.3.1.1" class="ltx_text" style="font-size:90%;">Figure 12</span>: </span><span id="S3.F12.4.2" class="ltx_text" style="font-size:90%;">An overview of Transformer models used for multi-modal tasks in computer vision. The Transformer designs in this category can be grouped into single-stream (UNITER <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>, OSCAR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>, VideoBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, Unicoder-VL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib180" title="" class="ltx_ref">180</a>]</cite>, VisualBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite> and VL-BERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>) and dual-stream architectures (LXMERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, ViLBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib181" title="" class="ltx_ref">181</a>]</cite> and PEMT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib182" title="" class="ltx_ref">182</a>]</cite>). A key distinction between models is the choice of loss functions. While most of the multi-modal methods are focused on images as visual data, VideoBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> and PEMT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib182" title="" class="ltx_ref">182</a>]</cite> are designed to work on video streams and leverage unique modalities e.g., audio signals in videos <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib182" title="" class="ltx_ref">182</a>]</cite>.</span></figcaption>
</figure>
</section>
</section>
<section id="S3.SS7" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.7 </span><span id="S3.SS7.1.1" class="ltx_text ltx_font_italic">Transformers for Multi-Modal Tasks</span>
</h3>

<div id="S3.SS7.p1" class="ltx_para">
<p id="S3.SS7.p1.1" class="ltx_p">Transformer models have also been extensively used for vision-language tasks such as visual question answering (VQA) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib183" title="" class="ltx_ref">183</a>]</cite>, visual commonsense reasoning (VSR) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib184" title="" class="ltx_ref">184</a>]</cite>, cross-modal retrieval <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib185" title="" class="ltx_ref">185</a>]</cite> and image captioning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>. Several works in this direction target effective vision-language pre-training (VLP) on large-scale multi-modal datasets to learn generic representations that effectively encode cross-modality relationships (<em id="S3.SS7.p1.1.1" class="ltx_emph ltx_font_italic">e.g.</em>, grounding semantic attributes of a person in a given image). These representations can then be transferred to downstream tasks, often obtaining state of the art results. <span id="S3.SS7.p1.1.2" class="ltx_text" style="color:#000000;">Notably, several of these models still use CNNs as vision backbone to extract visual features while Transformers are used mainly used to encode text followed by the fusion of language and visual features.</span> Such models generally apply the vanilla multi-layer Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> with multi-modal inputs and do not introduce fundamental changes to the core attention block. However, their main distinction is in the configuration of Transformers and the loss functions, based on which we categorize them into: (a) Multi-stream Transformers (see Sec. <a href="#S3.SS7.SSS1" title="3.7.1 Multi-stream Transformers ‣ 3.7 Transformers for Multi-Modal Tasks ‣ 3 Self-Attention &amp; Transformers in Vision ‣ Transformers in Vision: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.7.1</span></a>) and (b) Single-stream Transformers (see Sec. <a href="#S3.SS7.SSS2" title="3.7.2 Single-stream Transformers ‣ 3.7 Transformers for Multi-Modal Tasks ‣ 3 Self-Attention &amp; Transformers in Vision ‣ Transformers in Vision: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.7.2</span></a>). The <em id="S3.SS7.p1.1.3" class="ltx_emph ltx_font_italic">single-stream</em> designs feed the <em id="S3.SS7.p1.1.4" class="ltx_emph ltx_font_italic">multi-modal</em> inputs to a single Transformer while the multi-stream designs first use independent Transformers for each modality and later learn cross-modal representations using another Transformer (see Fig. <a href="#S3.F12" title="Figure 12 ‣ 3.6.3 Colorization Transformer ‣ 3.6 Transformers for Low-level Vision ‣ 3 Self-Attention &amp; Transformers in Vision ‣ Transformers in Vision: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a>). Besides these vision language pretraining methods, we also explain visual grounding approaches towards the end of this section (see Sec. <a href="#S3.SS7.SSS3" title="3.7.3 Transformers for Visual Grounding ‣ 3.7 Transformers for Multi-Modal Tasks ‣ 3 Self-Attention &amp; Transformers in Vision ‣ Transformers in Vision: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.7.3</span></a>).</p>
</div>
<section id="S3.SS7.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.7.1 </span>Multi-stream Transformers</h4>

<div id="S3.SS7.SSS1.p1" class="ltx_para">
<p id="S3.SS7.SSS1.p1.1" class="ltx_p">Vision and Language BERT (ViLBERT) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite> was the first extension of the BERT model to the multi-modal domain. The goal was to learn representations that can jointly model images and natural language. For this purpose, ViLBERT developed a two-stream architecture where each stream is dedicated to model the vision or language inputs (Fig. <a href="#S3.F12" title="Figure 12 ‣ 3.6.3 Colorization Transformer ‣ 3.6 Transformers for Low-level Vision ‣ 3 Self-Attention &amp; Transformers in Vision ‣ Transformers in Vision: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a>-h). The architecture of both parallel streams is a series of Transformer blocks similar to the BERT model. Subsequently, co-attentional Transformer layers are applied to learn cross-modal relationships. The co-attentional framework is very simple. Query, key, and value matrices are computed for each modality in the standard way <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> and then key-value pairs for one modality are passed on to the other modality’s attention head.</p>
</div>
<div id="S3.SS7.SSS1.p2" class="ltx_para">
<p id="S3.SS7.SSS1.p2.1" class="ltx_p">ViLBERT applies VLP on a set of proxy tasks defined on the Conceptual Concepts dataset (with 3.3M images with weak captions) and later fine-tune the model on downstream tasks such as VQA. The pre-training phase operates in a self-supervised manner, i.e., pretext tasks are created without manual labeling on the large-scale unlabelled dataset. These pretext tasks include predicting whether the text and image inputs are related and predicting the semantics of masked image regions and textual inputs (<em id="S3.SS7.SSS1.p2.1.1" class="ltx_emph ltx_font_italic">e.g.</em>, similar to reconstructing masked words in text in the BERT model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>). This way, the model learns the inherent structure in the data during pre-training and also models cross-domain associations. With evaluations on several tasks, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> demonstrated that a two-stream model can perform better than a single-stream model that uses shared parameters to model both language and vision domains <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>.</p>
</div>
<div id="S3.SS7.SSS1.p3" class="ltx_para">
<p id="S3.SS7.SSS1.p3.1" class="ltx_p">Similar to ViLBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib181" title="" class="ltx_ref">181</a>]</cite>, Learning Cross-Modality Encoder Representations from Transformers (LXMERT) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> also uses a two-stream architecture based on BERT framework. The main difference lies in the object-relationship encoder that is used to model the visual features instead of simple image-level features used in ViLBERT. The information in two streams is then fused across modalities using cross-attention blocks similar to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib181" title="" class="ltx_ref">181</a>]</cite>.</p>
</div>
<div id="S3.SS7.SSS1.p4" class="ltx_para">
<p id="S3.SS7.SSS1.p4.1" class="ltx_p">Compared to two pre-texts tasks used for VLP in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib181" title="" class="ltx_ref">181</a>]</cite>, LXMERT uses five pre-training tasks including masked object and language prediction, cross-modality matching, and visual question answering (Fig. <a href="#S3.F12" title="Figure 12 ‣ 3.6.3 Colorization Transformer ‣ 3.6 Transformers for Low-level Vision ‣ 3 Self-Attention &amp; Transformers in Vision ‣ Transformers in Vision: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a>-g). The pre-trained model is fine-tuned on the VQA task, however, a high similarity between pre-training and fine-tuned tasks raises questions on the generalizability of the learned representations to new tasks. To this end, the authors conducted generalization experiments on Visual Reasoning for Real (NLVR) task <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib186" title="" class="ltx_ref">186</a>]</cite> demonstrating impressive improvements on novel tasks.</p>
</div>
<div id="S3.SS7.SSS1.p5" class="ltx_para">
<p id="S3.SS7.SSS1.p5.1" class="ltx_p">Lee <span id="S3.SS7.SSS1.p5.1.1" class="ltx_text ltx_font_italic">et al.<cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS7.SSS1.p5.1.1.1.1" class="ltx_text ltx_font_upright">[</span><a href="#bib.bib182" title="" class="ltx_ref">182</a><span id="S3.SS7.SSS1.p5.1.1.2.2" class="ltx_text ltx_font_upright">]</span></cite></span> note that the multi-modal representation learning approaches like VideoBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> and ViLBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib181" title="" class="ltx_ref">181</a>]</cite> generally keep the language processing part fixed to a pre-trained model (<em id="S3.SS7.SSS1.p5.1.2" class="ltx_emph ltx_font_italic">e.g.</em>, BERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>) to reduce training complexity. For the first time in the literature, they propose to learn an end-to-end multi-modal bidirectional Transformer model called PEMT on audio-visual data from unlabeled videos. First, short-term (<em id="S3.SS7.SSS1.p5.1.3" class="ltx_emph ltx_font_italic">e.g.</em>, 1-3 seconds) video dynamics are encoded using CNNs, followed by a modality-specific Transformer (audio/visual) to model long-term dependencies (<em id="S3.SS7.SSS1.p5.1.4" class="ltx_emph ltx_font_italic">e.g.</em>, 30 seconds). A multi-modal Transformer is then applied to the modality-specific Transformer outputs to exchange information across visual-linguistic domains. However, learning such a model in a naive form would incur huge memory requirements. To reduce parametric complexity, the parameters are shared across layers within each Transformer which leads upto 80% parameter reduction.
The Transformer is trained using a contrastive learning approach based on a content-aware negative sampling (Fig. <a href="#S3.F12" title="Figure 12 ‣ 3.6.3 Colorization Transformer ‣ 3.6 Transformers for Low-level Vision ‣ 3 Self-Attention &amp; Transformers in Vision ‣ Transformers in Vision: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a>-i). Specifically, the model uses the features obtained from CNNs learned during the training phase to select negative samples that are visually similar to the positive instances. This work also compares various fusion strategies adopted in earlier works such as early (VideoBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> and VL-BERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>), mid-level (ViL-BERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib181" title="" class="ltx_ref">181</a>]</cite> and LXMERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>) and late fusion mechanisms and shows that the mid-level fusion is the optimal choice. The proposed model is pre-trained on Kinetics-700 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib187" title="" class="ltx_ref">187</a>]</cite> dataset and later fine-tuned on downstream video classification tasks such as short video classification on UCF101 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib188" title="" class="ltx_ref">188</a>]</cite>, audio classification on ESC50 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib189" title="" class="ltx_ref">189</a>]</cite> and long-term action recognition on Charades <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib190" title="" class="ltx_ref">190</a>]</cite> and Kinetics-Sounds <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref">65</a>]</cite> datasets.</p>
</div>
<div id="S3.SS7.SSS1.p6" class="ltx_para">
<p id="S3.SS7.SSS1.p6.1" class="ltx_p">Tan and Bansal
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib191" title="" class="ltx_ref">191</a>]</cite> introduce the concept of ‘<em id="S3.SS7.SSS1.p6.1.1" class="ltx_emph ltx_font_italic">vokens</em>’ (images related to language tokens extracted from sentences). The vokens (visualized tokens) provide visual supervision to the language model to learn better features. The motivation is that humans learn languages by correlating visual information with semantic concepts. In a similar spirit to other self-supervised language representation learning methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib181" title="" class="ltx_ref">181</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, they learn representations by defining an auxiliary task of voken-prediction task.
Since the existing datasets encode limited visually grounded tokens, they propose a vokenization method to map language tokens to visual vokens, as illustrated in Fig. <a href="#S3.F13" title="Figure 13 ‣ 3.7.2 Single-stream Transformers ‣ 3.7 Transformers for Multi-Modal Tasks ‣ 3 Self-Attention &amp; Transformers in Vision ‣ Transformers in Vision: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a>.
The approach uses language-based retrieval for such a mapping and transfers a model trained on a small labeled dataset (MS-COCO) to a large dataset (Wikipedia). Furthermore, it was ensured that the sentence-wide context is considered to obtain the token-voken mapping. The resulting model trained using generated tokens outperforms the state of the art BERT model on a diverse set of NLP tasks. In this sense, the proposed model does not evaluate vision tasks, however, uses vision as a useful grounding cue to train the language model, hence we include it in the multi-modal representation learning group.</p>
</div>
<div id="S3.SS7.SSS1.p7" class="ltx_para">
<p id="S3.SS7.SSS1.p7.1" class="ltx_p">Vision-and-Language Navigation (VLN) aims to predict a navigation plan on a map based on the vision and language inputs. Transformer models were used earlier in
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib192" title="" class="ltx_ref">192</a>, <a href="#bib.bib193" title="" class="ltx_ref">193</a>]</cite> for VLN task. These works first pre-train a cross-modal Transformer using self-supervision on vision and language pairs and subsequently fine-tune on the specific VLN tasks. While these works learn attention between image region and language, Chen <span id="S3.SS7.SSS1.p7.1.1" class="ltx_text ltx_font_italic">et al.<cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS7.SSS1.p7.1.1.1.1" class="ltx_text ltx_font_upright">[</span><a href="#bib.bib194" title="" class="ltx_ref">194</a><span id="S3.SS7.SSS1.p7.1.1.2.2" class="ltx_text ltx_font_upright">]</span></cite></span> propose to learn cross-modal attention between language inputs and spatial topological maps (to represent an agent’s environment as a graph whose nodes denote places and the edges denote their connectivity). Given the topological map and natural language inputs, a VLN task using the Transformer model bears resemblance to sequence prediction in NLP. Specifically, at each time instance, the cross-modal Transformer predicts a single node of the topological map in the navigation plan. The individual language and map encodings are first processed using uni-modal encoders and later a cross-modal encoder (similar to LXMERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>) is applied to aggregate information across modalities. To denote positions in the map, a learned trajectory position encoding is appended with the map features. Based on this Transformer setup, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib194" title="" class="ltx_ref">194</a>]</cite> reports a full navigation system that can freely explore the environment and intelligently plan its actions.</p>
</div>
<div id="S3.SS7.SSS1.p8" class="ltx_para">
<p id="S3.SS7.SSS1.p8.3" class="ltx_p"><span id="S3.SS7.SSS1.p8.3.3" class="ltx_text" style="color:#000000;">CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib195" title="" class="ltx_ref">195</a>]</cite> is a contrastive approach to learn image representations from text, with a learning objective which maximizes similarity of correct text-image pairs embeddings in a large batch size. Specifically, given a batch of <math id="S3.SS7.SSS1.p8.1.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS7.SSS1.p8.1.1.m1.1a"><mi mathcolor="#000000" id="S3.SS7.SSS1.p8.1.1.m1.1.1" xref="S3.SS7.SSS1.p8.1.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS7.SSS1.p8.1.1.m1.1b"><ci id="S3.SS7.SSS1.p8.1.1.m1.1.1.cmml" xref="S3.SS7.SSS1.p8.1.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS7.SSS1.p8.1.1.m1.1c">N</annotation><annotation encoding="application/x-llamapun" id="S3.SS7.SSS1.p8.1.1.m1.1d">italic_N</annotation></semantics></math> image-text pairs, CLIP learns a multi-modal embedding space, by jointly training an image-encoder and a text-encoder, such that the cosine similarity of the valid <math id="S3.SS7.SSS1.p8.2.2.m2.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS7.SSS1.p8.2.2.m2.1a"><mi mathcolor="#000000" id="S3.SS7.SSS1.p8.2.2.m2.1.1" xref="S3.SS7.SSS1.p8.2.2.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS7.SSS1.p8.2.2.m2.1b"><ci id="S3.SS7.SSS1.p8.2.2.m2.1.1.cmml" xref="S3.SS7.SSS1.p8.2.2.m2.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS7.SSS1.p8.2.2.m2.1c">N</annotation><annotation encoding="application/x-llamapun" id="S3.SS7.SSS1.p8.2.2.m2.1d">italic_N</annotation></semantics></math> image-text pairs is maximized, while the remaining <math id="S3.SS7.SSS1.p8.3.3.m3.1" class="ltx_Math" alttext="N^{2}-N" display="inline"><semantics id="S3.SS7.SSS1.p8.3.3.m3.1a"><mrow id="S3.SS7.SSS1.p8.3.3.m3.1.1" xref="S3.SS7.SSS1.p8.3.3.m3.1.1.cmml"><msup id="S3.SS7.SSS1.p8.3.3.m3.1.1.2" xref="S3.SS7.SSS1.p8.3.3.m3.1.1.2.cmml"><mi mathcolor="#000000" id="S3.SS7.SSS1.p8.3.3.m3.1.1.2.2" xref="S3.SS7.SSS1.p8.3.3.m3.1.1.2.2.cmml">N</mi><mn mathcolor="#000000" id="S3.SS7.SSS1.p8.3.3.m3.1.1.2.3" xref="S3.SS7.SSS1.p8.3.3.m3.1.1.2.3.cmml">2</mn></msup><mo mathcolor="#000000" id="S3.SS7.SSS1.p8.3.3.m3.1.1.1" xref="S3.SS7.SSS1.p8.3.3.m3.1.1.1.cmml">−</mo><mi mathcolor="#000000" id="S3.SS7.SSS1.p8.3.3.m3.1.1.3" xref="S3.SS7.SSS1.p8.3.3.m3.1.1.3.cmml">N</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS7.SSS1.p8.3.3.m3.1b"><apply id="S3.SS7.SSS1.p8.3.3.m3.1.1.cmml" xref="S3.SS7.SSS1.p8.3.3.m3.1.1"><minus id="S3.SS7.SSS1.p8.3.3.m3.1.1.1.cmml" xref="S3.SS7.SSS1.p8.3.3.m3.1.1.1"></minus><apply id="S3.SS7.SSS1.p8.3.3.m3.1.1.2.cmml" xref="S3.SS7.SSS1.p8.3.3.m3.1.1.2"><csymbol cd="ambiguous" id="S3.SS7.SSS1.p8.3.3.m3.1.1.2.1.cmml" xref="S3.SS7.SSS1.p8.3.3.m3.1.1.2">superscript</csymbol><ci id="S3.SS7.SSS1.p8.3.3.m3.1.1.2.2.cmml" xref="S3.SS7.SSS1.p8.3.3.m3.1.1.2.2">𝑁</ci><cn type="integer" id="S3.SS7.SSS1.p8.3.3.m3.1.1.2.3.cmml" xref="S3.SS7.SSS1.p8.3.3.m3.1.1.2.3">2</cn></apply><ci id="S3.SS7.SSS1.p8.3.3.m3.1.1.3.cmml" xref="S3.SS7.SSS1.p8.3.3.m3.1.1.3">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS7.SSS1.p8.3.3.m3.1c">N^{2}-N</annotation><annotation encoding="application/x-llamapun" id="S3.SS7.SSS1.p8.3.3.m3.1d">italic_N start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT - italic_N</annotation></semantics></math> pairs is minimized. The authors consider ResNet-50 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite> and Vision Transformer (ViT) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib132" title="" class="ltx_ref">132</a>]</cite> for encoding images. The modified Transformer model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> as in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> is employed for encoding text. CLIP is trained on a large corpus of 400 million image-text pairs
and demonstrates excellent zero-shot transfer capabilities. At inference, the names of classes are used as input to the text-encoder, and similarity of the encoded image is computed with all encoded texts (classes) to find the image-text pair with highest match. The CLIP achieves an astounding zero-shot classification accuracy of 75% on ImageNet, without using an supervision from ImageNet training set. The authors further demonstrate zero-shot transfer capabilities of the CLIP model on 30 different computer vision benchmarks. Note that
CLIP with ResNet took 18 days to train on 592 V100 GPUs while CLIP with ViT took 12 days on 256 V100 GPUs. This highlights the computational cost of CLIP.</span></p>
</div>
</section>
<section id="S3.SS7.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.7.2 </span>Single-stream Transformers</h4>

<div id="S3.SS7.SSS2.p1" class="ltx_para">
<p id="S3.SS7.SSS2.p1.1" class="ltx_p">Different from two-stream networks like ViLBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib181" title="" class="ltx_ref">181</a>]</cite> and LXMERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, VisualBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite> uses a single stack of Transformers to model both the domains (images and text). The input sequence of text (<em id="S3.SS7.SSS2.p1.1.1" class="ltx_emph ltx_font_italic">e.g.</em>, caption) and the visual features corresponding to the object proposals are fed to the Transformer that automatically discovers relations between the two domains. Notably, VisualBERT architecture is somewhat similar to VideoBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> (explained in Sec. <a href="#S3.SS8" title="3.8 Video Understanding ‣ 3 Self-Attention &amp; Transformers in Vision ‣ Transformers in Vision: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.8</span></a>), but instead of only focusing on cooking videos, VisualBERT evaluates on various visual-linguistic tasks (<em id="S3.SS7.SSS2.p1.1.2" class="ltx_emph ltx_font_italic">e.g.</em>, VCR, NLVR, VQA, and visual grounding).
The VisualBERT model first applies task-agnostic pre-training using two objectives (Fig. <a href="#S3.F12" title="Figure 12 ‣ 3.6.3 Colorization Transformer ‣ 3.6 Transformers for Low-level Vision ‣ 3 Self-Attention &amp; Transformers in Vision ‣ Transformers in Vision: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a>-e). The first objective simply attempts to predict missing text tokens using the image features and remaining textual tokens. The second objective attempts to differentiate between the true and false caption of a given image. After task-agnostic pre-training, the authors propose to perform task-specific pre-training to bridge the domain gap before the final fine-tuning to the downstream task.</p>
</div>
<div id="S3.SS7.SSS2.p2" class="ltx_para">
<p id="S3.SS7.SSS2.p2.1" class="ltx_p">Su <span id="S3.SS7.SSS2.p2.1.1" class="ltx_text ltx_font_italic">et al.<cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS7.SSS2.p2.1.1.1.1" class="ltx_text ltx_font_upright">[</span><a href="#bib.bib22" title="" class="ltx_ref">22</a><span id="S3.SS7.SSS2.p2.1.1.2.2" class="ltx_text ltx_font_upright">]</span></cite></span> propose a multi-modal pre-training approach to learn features that are generalizable to multi-modal downstream tasks such as Visual Commonsense Reasoning and Visual Question Answering. This endeavor requires adequately aligning the visual and linguistic cues so that an effective composite representation is learned. To the end, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> builds on the BERT model and inputs both the visual and language features. The language features correspond to the token in the input sentence and the visual features correspond to the region of interest (RoI) from the input image (obtained via a standard Faster R-CNN).
Specifically, the model is pre-trained on both the visual-lingual dataset (Conceptual Captions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib196" title="" class="ltx_ref">196</a>]</cite>) as well as the language-only datasets (<em id="S3.SS7.SSS2.p2.1.2" class="ltx_emph ltx_font_italic">e.g.</em>, Wikipedia). The loss function is identical to BERT, where the model is trained to predict the masked out words or visual ROIs (Fig. <a href="#S3.F12" title="Figure 12 ‣ 3.6.3 Colorization Transformer ‣ 3.6 Transformers for Low-level Vision ‣ 3 Self-Attention &amp; Transformers in Vision ‣ Transformers in Vision: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a>-f). In contrary to other works such as UNITER <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>, VL-BERT claims that the visual-linguistic matching tasks are not useful during pre-training, which is in contrast to evidence from later efforts <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib180" title="" class="ltx_ref">180</a>]</cite>. Their results on several multi-modal tasks show their benefit over the language-only pre-training (<em id="S3.SS7.SSS2.p2.1.3" class="ltx_emph ltx_font_italic">e.g.</em>, in BERT).
</p>
</div>
<div id="S3.SS7.SSS2.p3" class="ltx_para">
<p id="S3.SS7.SSS2.p3.1" class="ltx_p">Universal Encoder for Vision and Language (Unicoder-VL) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib180" title="" class="ltx_ref">180</a>]</cite> learns multi-modal representations using large-scale image-caption pairs. The language and image inputs are fed to a single Transformer model (with multiple successive encoders) to learn joint embeddings. To this end, it uses masked word prediction, masked object classification, and visual-linguistic matching as self-supervision tasks during pre-training (Fig. <a href="#S3.F12" title="Figure 12 ‣ 3.6.3 Colorization Transformer ‣ 3.6 Transformers for Low-level Vision ‣ 3 Self-Attention &amp; Transformers in Vision ‣ Transformers in Vision: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a>-d). Notably, the visual-linguistic matching is carried out only at the global level (i.e., image-sentence alignment). The model is evaluated on image-text retrieval, zero-shot learning, and visual commonsense reasoning where it performs better than the previous models such as ViLBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib181" title="" class="ltx_ref">181</a>]</cite> and VisualBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite>. This shows the significance of rich self-supervised tasks and advocates for a unified Transformer architecture to learn multi-modal features in a common framework.</p>
</div>
<div id="S3.SS7.SSS2.p4" class="ltx_para">
<p id="S3.SS7.SSS2.p4.1" class="ltx_p">The Unified Vision-Language Pre-training (VLP) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib197" title="" class="ltx_ref">197</a>]</cite> model uses a single Transformer network for both encoding and decoding stages. This stands in contrast to BERT inspired VLP models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib198" title="" class="ltx_ref">198</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite> which use independent encoder and decoder networks.
Joint modeling of encoding and decoding stages allows the Unified VLP model to perform well for both image captioning and visual-question answering tasks, when fine-tuned on these individual tasks.
The intuition for shared modeling of encoding and decoding stage stems from the need to better share cross-task information during pre-training. The unified model consists of a stack of 12 Transformer blocks, each with a self-attention layer followed by a feed-forward module.
The self-supervised objectives used for pre-training include masked vision-language predictions. Here, the authors explore two variants i.e., bidirectional and sequence-to-sequence prediction of masked works where different context encodings are used for both types of objectives.
The proposed approach is evaluated on COCO Captions, Flick 30K Captions and VQA 2.0 and obtains encouraging results compared to previous methods on image captioning and VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib199" title="" class="ltx_ref">199</a>]</cite>.</p>
</div>
<div id="S3.SS7.SSS2.p5" class="ltx_para">
<p id="S3.SS7.SSS2.p5.1" class="ltx_p">Universal image-text representation (UNITER) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> performs pre-training on four large-scale visual-linguistic datasets (MS-COCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib75" title="" class="ltx_ref">75</a>]</cite>, Visual Genome <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib200" title="" class="ltx_ref">200</a>]</cite>, Conceptual Captions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib196" title="" class="ltx_ref">196</a>]</cite> and SBU Captions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib201" title="" class="ltx_ref">201</a>]</cite>). The learned representations transfer well on downstream tasks such as VQA, Multi-modal retrieval, Visual Commonsense reasoning, and NLVR. In order to emphasize on learning the relationships between visual and language domains, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> specifically designs pre-training tasks to predict masked visual or text region conditioned on the other domain input, and align language and visual inputs on both the global (image-text) and local (word-region) levels (Fig. <a href="#S3.F12" title="Figure 12 ‣ 3.6.3 Colorization Transformer ‣ 3.6 Transformers for Low-level Vision ‣ 3 Self-Attention &amp; Transformers in Vision ‣ Transformers in Vision: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a>-a). These tasks are beside the conventional masked language modeling task used in BERT and explicitly include fine-grained word-region alignment alongside conditional masking of inputs that were not considered in the earlier works such as VL-BERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, Visual-BERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite>, Vilbert <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib181" title="" class="ltx_ref">181</a>]</cite> and Unicoder-VL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib180" title="" class="ltx_ref">180</a>]</cite>. Common to the other approaches, they adopt the Transformer architecture proposed in BERT that operates on both the visual and language embeddings. In contrast to applying independent Transformers to the language and visual inputs (as in ViLBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib181" title="" class="ltx_ref">181</a>]</cite> and LXMERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>), UNITER adopts a single Transformer applied to the textual and image inputs like <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib180" title="" class="ltx_ref">180</a>, <a href="#bib.bib63" title="" class="ltx_ref">63</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>.</p>
</div>
<div id="S3.SS7.SSS2.p6" class="ltx_para">
<p id="S3.SS7.SSS2.p6.1" class="ltx_p">VisualBert <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite>, Uniter <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>, VL-BERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, VilBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib181" title="" class="ltx_ref">181</a>]</cite>, and Unicoder-VL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib180" title="" class="ltx_ref">180</a>]</cite> models for VLP concatenate image and text features and leave it to the self-attention to automatically discover cross-modal relationships. This can complicate the visual grounding of semantic concepts in an image. To address this problem, Object-Semantics Aligned Pre-Training (Oscar) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> first uses an object detector to obtain object tags (labels), which are then subsequently used as a mechanism to align relevant visual features with the semantic information (Fig. <a href="#S3.F12" title="Figure 12 ‣ 3.6.3 Colorization Transformer ‣ 3.6 Transformers for Low-level Vision ‣ 3 Self-Attention &amp; Transformers in Vision ‣ Transformers in Vision: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a>-b). The motivation is that the textual content generally pertains to major objects in the image, therefore by explicitly adding those image labels to the input, visual features can be better attended.
Similar to BERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, Oscar uses a Masked Token Loss for VLP, where different tokens in the textual input and image tags are randomly masked and the model predicts these missing tokens. Further, it also uses a contrastive loss that discriminates between the original and noisy/fake image-tag pairs. The representations thus learned are fine-tuned on VQA, cross-modality retrieval, natural language reasoning, and image captioning tasks to obtain better performances compared to VLP methods that do not use object tags. <span id="S3.SS7.SSS2.p6.1.1" class="ltx_text" style="color:#000000;">The recent VinVL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib202" title="" class="ltx_ref">202</a>]</cite> approach extends Oscar for the object detection task and learns object instance-centered relationships between visual and language domains using an adapted pretraining scheme. The model is trained on a collection of datasets (MS-COCO, OpenImages, Visual Genome and Objects365) and was demonstrated to precisely relate semantic attributes with the visual information and provided better transferability to the downstream visual comprehension tasks. </span></p>
</div>
<figure id="S3.F13" class="ltx_figure"><img src="/html/2101.01169/assets/Figs/Vokenizer.png" id="S3.F13.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="354" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F13.3.1.1" class="ltx_text" style="font-size:90%;">Figure 13</span>: </span><span id="S3.F13.4.2" class="ltx_text" style="font-size:90%;">Visualized tokens (Vokens) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib191" title="" class="ltx_ref">191</a>]</cite>: A language model is visually supervised using closely related images that leads to better feature representations from the pretrained model. Figure from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib191" title="" class="ltx_ref">191</a>]</cite>. </span></figcaption>
</figure>
</section>
<section id="S3.SS7.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsubsection">3.7.3 </span>Transformers for Visual Grounding</h4>

<div id="S3.SS7.SSS3.p1" class="ltx_para">
<p id="S3.SS7.SSS3.p1.1" class="ltx_p"><span id="S3.SS7.SSS3.p1.1.1" class="ltx_text" style="color:#000000;">Modulated DETR (MDETR) </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS7.SSS3.p1.1.2.1" class="ltx_text" style="color:#000000;">[</span><a href="#bib.bib203" title="" class="ltx_ref">203</a><span id="S3.SS7.SSS3.p1.1.3.2" class="ltx_text" style="color:#000000;">]</span></cite><span id="S3.SS7.SSS3.p1.1.4" class="ltx_text" style="color:#000000;"> has a CNN and BERT backbone to extract features from image and text inputs, respectively. The visual and text features are then separately linearly projected to a shared space, concatenated and fed to a transformer model (with an architecture similar to DETR) to predict the bounding boxes for objects corresponding to the queries in the grounding text. The model is trained by using a loss which predicts a uniform distribution over all relevant text query tokens specific to the predicted bounding boxes. An additional contrastive loss term ensures correspondence between visual and text embedding.
TransVG </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS7.SSS3.p1.1.5.1" class="ltx_text" style="color:#000000;">[</span><a href="#bib.bib204" title="" class="ltx_ref">204</a><span id="S3.SS7.SSS3.p1.1.6.2" class="ltx_text" style="color:#000000;">]</span></cite><span id="S3.SS7.SSS3.p1.1.7" class="ltx_text" style="color:#000000;"> is a simple design, where visual and text features are fused together in a transformer module, and the bounding-box corresponding to the query is directly regressed using a learnable token (input to the Transformer module, along-with visual and text features). Referring Transformer </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS7.SSS3.p1.1.8.1" class="ltx_text" style="color:#000000;">[</span><a href="#bib.bib205" title="" class="ltx_ref">205</a><span id="S3.SS7.SSS3.p1.1.9.2" class="ltx_text" style="color:#000000;">]</span></cite><span id="S3.SS7.SSS3.p1.1.10" class="ltx_text" style="color:#000000;"> is also a simple one stage design where the text and image features are fused in a Transformer encoder, and the Transformer based decoder then directly regresses bounding boxes or segmentation masks.
Visual Grounding with Transformer </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS7.SSS3.p1.1.11.1" class="ltx_text" style="color:#000000;">[</span><a href="#bib.bib206" title="" class="ltx_ref">206</a><span id="S3.SS7.SSS3.p1.1.12.2" class="ltx_text" style="color:#000000;">]</span></cite><span id="S3.SS7.SSS3.p1.1.13" class="ltx_text" style="color:#000000;"> has an encoder-decoder architecture, where visual tokens (features extracted from a pretrained CNN model) and text tokens (parsed through an RNN module) are processed in parallel with two distinct branches in the encoder, with cross-modality attention to generate text-guided visual features. The decoder then computes attention between the text queries and visual features and predicts query-specific bounding boxes.</span><span id="S3.SS7.SSS3.p1.1.14" class="ltx_text"></span></p>
</div>
</section>
</section>
<section id="S3.SS8" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.8 </span><span id="S3.SS8.1.1" class="ltx_text ltx_font_italic">Video Understanding</span>
</h3>

<div id="S3.SS8.p1" class="ltx_para">
<p id="S3.SS8.p1.1" class="ltx_p">Existing approaches for audio-video data analysis generally learn representations on short-length videos (up to a few seconds long), that allow them to encode only short-range dependencies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>. Long-range dependency modeling is desirable in various uni-modal and multi-modal learning tasks such as activity recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib187" title="" class="ltx_ref">187</a>, <a href="#bib.bib71" title="" class="ltx_ref">71</a>, <a href="#bib.bib207" title="" class="ltx_ref">207</a>, <a href="#bib.bib208" title="" class="ltx_ref">208</a>, <a href="#bib.bib209" title="" class="ltx_ref">209</a>]</cite>. Below, we explain recent approaches that seek to resolve this challenge using the expressivity of Transformer networks. <span id="S3.SS8.p1.1.1" class="ltx_text" style="color:#000000;">It is important to note that several of these works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib210" title="" class="ltx_ref">210</a>, <a href="#bib.bib182" title="" class="ltx_ref">182</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> still employ (pretrained) CNNs to encode image/frame-level features in the videos on top of which Transformers are applied to model wide context. A few exceptions include <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib211" title="" class="ltx_ref">211</a>, <a href="#bib.bib212" title="" class="ltx_ref">212</a>, <a href="#bib.bib209" title="" class="ltx_ref">209</a>, <a href="#bib.bib213" title="" class="ltx_ref">213</a>]</cite> which obtain frame-level features also using the ViT based backbones.</span></p>
</div>
<section id="S3.SS8.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.8.1 </span>Joint Video and Language Modeling</h4>

<div id="S3.SS8.SSS1.p1" class="ltx_para">
<p id="S3.SS8.SSS1.p1.1" class="ltx_p">The VideoBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> model leverages Transformer networks and the strength of self-supervised learning to learn effective multi-modal representations. Specifically, VideoBERT uses the prediction of masked visual and linguistic tokens as a pretext task (Fig. <a href="#S3.F12" title="Figure 12 ‣ 3.6.3 Colorization Transformer ‣ 3.6 Transformers for Low-level Vision ‣ 3 Self-Attention &amp; Transformers in Vision ‣ Transformers in Vision: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a>-c). This allows modeling high-level semantics and long-range temporal dependencies, important for video understanding tasks. Given a video, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> converts speech to text using off-the-shelf speech recognition systems and applies vector quantization (clustering) to obtain visual features from pre-trained video classification models. The BERT model is then directly applied to these concatenated sequences of language and visual tokens to learn their joint distribution.
The model can be trained with only-text, video-only, and video+text domains. The resulting model showcases interesting capabilities for cross-modal predictions such as video generation from a given textual input (<em id="S3.SS8.SSS1.p1.1.1" class="ltx_emph ltx_font_italic">e.g.</em>, captions or cooking recipe) and (video-based) future forecasting. The video+text model uses a visual-linguistic alignment task to learn cross-modality relationships. The definition of this pre-text task is simple, given the latent state of the <span id="S3.SS8.SSS1.p1.1.2" class="ltx_text ltx_font_typewriter">[cls]</span> token, the task is to predict whether the sentence is temporally aligned with the sequence of visual tokens. Further, the learned representations are shown to be very useful for downstream tasks such as action classification, zero-shot classification, and video captioning.</p>
</div>
<div id="S3.SS8.SSS1.p2" class="ltx_para">
<p id="S3.SS8.SSS1.p2.1" class="ltx_p">Zhou <span id="S3.SS8.SSS1.p2.1.1" class="ltx_text ltx_font_italic">et al.<cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS8.SSS1.p2.1.1.1.1" class="ltx_text ltx_font_upright">[</span><a href="#bib.bib210" title="" class="ltx_ref">210</a><span id="S3.SS8.SSS1.p2.1.1.2.2" class="ltx_text ltx_font_upright">]</span></cite></span> explore Masked Transformers for dense video captioning. This requires generating language descriptions for all events occurring in a video. Existing works on this problem generally operate sequentially i.e., first detect events and then generate captions in separate sub-blocks. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib210" title="" class="ltx_ref">210</a>]</cite> proposes a unified Transformer network to tackle both tasks jointly, thereby seamlessly integrating the multi-modal tasks of event detection and captioning. First, a video encoder is used to obtain frame-wise representations followed by two decoder blocks focused on proposing the video events and the captions. Since untrimmed videos are considered, a masking network is used in the captioning decoder to focus on describing a single event proposal. Remarkably, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib210" title="" class="ltx_ref">210</a>]</cite> was the first approach to target dense video captioning using non-recurrent models and used self-attention in the encoder(applied on CNN derived features) to model broad range context between video frames. Experiments on ActivityNet Captions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib214" title="" class="ltx_ref">214</a>]</cite> and YouCookII <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib215" title="" class="ltx_ref">215</a>]</cite> datasets showed good improvements over previous recurrent network and two-stage based approaches.</p>
</div>
</section>
<section id="S3.SS8.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.8.2 </span>Video Action Recognition</h4>

<div id="S3.SS8.SSS2.p1" class="ltx_para">
<p id="S3.SS8.SSS2.p1.2" class="ltx_p">The traditional CNN based methods in video classification generally perform 3D spatio-temporal processing over limited intervals to understand videos. Neimark <em id="S3.SS8.SSS2.p1.2.1" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib211" title="" class="ltx_ref">211</a>]</cite> propose Video Transformer Network (VTN) that first obtains frame-wise features using 2D CNN and apply a Transformer encoder (Longformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib103" title="" class="ltx_ref">103</a>]</cite>) on top to learn temporal relationships. Longformer is an attractive choice to process long sequences (with an arbitrary length <math id="S3.SS8.SSS2.p1.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.SS8.SSS2.p1.1.m1.1a"><mi id="S3.SS8.SSS2.p1.1.m1.1.1" xref="S3.SS8.SSS2.p1.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS8.SSS2.p1.1.m1.1b"><ci id="S3.SS8.SSS2.p1.1.m1.1.1.cmml" xref="S3.SS8.SSS2.p1.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS8.SSS2.p1.1.m1.1c">n</annotation><annotation encoding="application/x-llamapun" id="S3.SS8.SSS2.p1.1.m1.1d">italic_n</annotation></semantics></math>) due to its <math id="S3.SS8.SSS2.p1.2.m2.1" class="ltx_Math" alttext="\mathcal{O}(n)" display="inline"><semantics id="S3.SS8.SSS2.p1.2.m2.1a"><mrow id="S3.SS8.SSS2.p1.2.m2.1.2" xref="S3.SS8.SSS2.p1.2.m2.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS8.SSS2.p1.2.m2.1.2.2" xref="S3.SS8.SSS2.p1.2.m2.1.2.2.cmml">𝒪</mi><mo id="S3.SS8.SSS2.p1.2.m2.1.2.1" xref="S3.SS8.SSS2.p1.2.m2.1.2.1.cmml" lspace='0px' rspace='0px'></mo><mrow id="S3.SS8.SSS2.p1.2.m2.1.2.3.2" xref="S3.SS8.SSS2.p1.2.m2.1.2.cmml"><mo stretchy="false" id="S3.SS8.SSS2.p1.2.m2.1.2.3.2.1" xref="S3.SS8.SSS2.p1.2.m2.1.2.cmml">(</mo><mi id="S3.SS8.SSS2.p1.2.m2.1.1" xref="S3.SS8.SSS2.p1.2.m2.1.1.cmml">n</mi><mo stretchy="false" id="S3.SS8.SSS2.p1.2.m2.1.2.3.2.2" xref="S3.SS8.SSS2.p1.2.m2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS8.SSS2.p1.2.m2.1b"><apply id="S3.SS8.SSS2.p1.2.m2.1.2.cmml" xref="S3.SS8.SSS2.p1.2.m2.1.2"><times id="S3.SS8.SSS2.p1.2.m2.1.2.1.cmml" xref="S3.SS8.SSS2.p1.2.m2.1.2.1"></times><ci id="S3.SS8.SSS2.p1.2.m2.1.2.2.cmml" xref="S3.SS8.SSS2.p1.2.m2.1.2.2">𝒪</ci><ci id="S3.SS8.SSS2.p1.2.m2.1.1.cmml" xref="S3.SS8.SSS2.p1.2.m2.1.1">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS8.SSS2.p1.2.m2.1c">\mathcal{O}(n)</annotation><annotation encoding="application/x-llamapun" id="S3.SS8.SSS2.p1.2.m2.1d">caligraphic_O ( italic_n )</annotation></semantics></math> complexity.
The classification token is passed through a fully connected layer to recognize actions or events. The advantage of using Transformer encoder on top of spatial features is two fold: (a) it allows processing a complete video in a single pass, and (b) considerably improves training and inference efficiency by avoiding the expensive 3D convolutions.
This makes VTN particularly suitable for modeling long videos where interactions between entities are spread throughout the video length. Their experiments on Kinetics-400 dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib71" title="" class="ltx_ref">71</a>]</cite> with various backbones (ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite>, ViT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> and DeiT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>) shows competitive performance.</p>
</div>
<div id="S3.SS8.SSS2.p2" class="ltx_para">
<p id="S3.SS8.SSS2.p2.13" class="ltx_p">Girdhar <span id="S3.SS8.SSS2.p2.13.1" class="ltx_text ltx_font_italic">et al.<cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS8.SSS2.p2.13.1.1.1" class="ltx_text ltx_font_upright">[</span><a href="#bib.bib18" title="" class="ltx_ref">18</a><span id="S3.SS8.SSS2.p2.13.1.2.2" class="ltx_text ltx_font_upright">]</span></cite></span> use a variant of Transformer architecture to aggregate person-specific contextual cues in a video for action classification and localization. Initially, the model uses a Faster-RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib125" title="" class="ltx_ref">125</a>]</cite> style processing where a backbone model generates features that are forwarded to the Region Proposal Network to obtain object proposals. Then RoI pooling is applied to generate object-specific features. Multi-head self-attention <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> is then applied on top of the object features as a cascade of self-attention layers. In each Transformer unit, a particular person feature is treated as the ‘query’ (Q), while the features from the neighboring video clip are used as ‘key’ (K) and ‘value’ (V). The location information is explicitly encoded in the input feature map from which K, V and Q are derived, thus incorporating the positional information in the self-attention. For a given <math id="S3.SS8.SSS2.p2.1.m1.1" class="ltx_Math" alttext="400" display="inline"><semantics id="S3.SS8.SSS2.p2.1.m1.1a"><mn id="S3.SS8.SSS2.p2.1.m1.1.1" xref="S3.SS8.SSS2.p2.1.m1.1.1.cmml">400</mn><annotation-xml encoding="MathML-Content" id="S3.SS8.SSS2.p2.1.m1.1b"><cn type="integer" id="S3.SS8.SSS2.p2.1.m1.1.1.cmml" xref="S3.SS8.SSS2.p2.1.m1.1.1">400</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS8.SSS2.p2.1.m1.1c">400</annotation><annotation encoding="application/x-llamapun" id="S3.SS8.SSS2.p2.1.m1.1d">400</annotation></semantics></math><math id="S3.SS8.SSS2.p2.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.SS8.SSS2.p2.2.m2.1a"><mo id="S3.SS8.SSS2.p2.2.m2.1.1" xref="S3.SS8.SSS2.p2.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS8.SSS2.p2.2.m2.1b"><times id="S3.SS8.SSS2.p2.2.m2.1.1.cmml" xref="S3.SS8.SSS2.p2.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS8.SSS2.p2.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS8.SSS2.p2.2.m2.1d">×</annotation></semantics></math><math id="S3.SS8.SSS2.p2.3.m3.1" class="ltx_Math" alttext="400" display="inline"><semantics id="S3.SS8.SSS2.p2.3.m3.1a"><mn id="S3.SS8.SSS2.p2.3.m3.1.1" xref="S3.SS8.SSS2.p2.3.m3.1.1.cmml">400</mn><annotation-xml encoding="MathML-Content" id="S3.SS8.SSS2.p2.3.m3.1b"><cn type="integer" id="S3.SS8.SSS2.p2.3.m3.1.1.cmml" xref="S3.SS8.SSS2.p2.3.m3.1.1">400</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS8.SSS2.p2.3.m3.1c">400</annotation><annotation encoding="application/x-llamapun" id="S3.SS8.SSS2.p2.3.m3.1d">400</annotation></semantics></math><math id="S3.SS8.SSS2.p2.4.m4.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.SS8.SSS2.p2.4.m4.1a"><mo id="S3.SS8.SSS2.p2.4.m4.1.1" xref="S3.SS8.SSS2.p2.4.m4.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS8.SSS2.p2.4.m4.1b"><times id="S3.SS8.SSS2.p2.4.m4.1.1.cmml" xref="S3.SS8.SSS2.p2.4.m4.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS8.SSS2.p2.4.m4.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS8.SSS2.p2.4.m4.1d">×</annotation></semantics></math><math id="S3.SS8.SSS2.p2.5.m5.1" class="ltx_Math" alttext="64" display="inline"><semantics id="S3.SS8.SSS2.p2.5.m5.1a"><mn id="S3.SS8.SSS2.p2.5.m5.1.1" xref="S3.SS8.SSS2.p2.5.m5.1.1.cmml">64</mn><annotation-xml encoding="MathML-Content" id="S3.SS8.SSS2.p2.5.m5.1b"><cn type="integer" id="S3.SS8.SSS2.p2.5.m5.1.1.cmml" xref="S3.SS8.SSS2.p2.5.m5.1.1">64</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS8.SSS2.p2.5.m5.1c">64</annotation><annotation encoding="application/x-llamapun" id="S3.SS8.SSS2.p2.5.m5.1d">64</annotation></semantics></math> video clip, the key and value tensors are of size <math id="S3.SS8.SSS2.p2.6.m6.1" class="ltx_Math" alttext="16" display="inline"><semantics id="S3.SS8.SSS2.p2.6.m6.1a"><mn id="S3.SS8.SSS2.p2.6.m6.1.1" xref="S3.SS8.SSS2.p2.6.m6.1.1.cmml">16</mn><annotation-xml encoding="MathML-Content" id="S3.SS8.SSS2.p2.6.m6.1b"><cn type="integer" id="S3.SS8.SSS2.p2.6.m6.1.1.cmml" xref="S3.SS8.SSS2.p2.6.m6.1.1">16</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS8.SSS2.p2.6.m6.1c">16</annotation><annotation encoding="application/x-llamapun" id="S3.SS8.SSS2.p2.6.m6.1d">16</annotation></semantics></math><math id="S3.SS8.SSS2.p2.7.m7.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.SS8.SSS2.p2.7.m7.1a"><mo id="S3.SS8.SSS2.p2.7.m7.1.1" xref="S3.SS8.SSS2.p2.7.m7.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS8.SSS2.p2.7.m7.1b"><times id="S3.SS8.SSS2.p2.7.m7.1.1.cmml" xref="S3.SS8.SSS2.p2.7.m7.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS8.SSS2.p2.7.m7.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS8.SSS2.p2.7.m7.1d">×</annotation></semantics></math><math id="S3.SS8.SSS2.p2.8.m8.1" class="ltx_Math" alttext="25" display="inline"><semantics id="S3.SS8.SSS2.p2.8.m8.1a"><mn id="S3.SS8.SSS2.p2.8.m8.1.1" xref="S3.SS8.SSS2.p2.8.m8.1.1.cmml">25</mn><annotation-xml encoding="MathML-Content" id="S3.SS8.SSS2.p2.8.m8.1b"><cn type="integer" id="S3.SS8.SSS2.p2.8.m8.1.1.cmml" xref="S3.SS8.SSS2.p2.8.m8.1.1">25</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS8.SSS2.p2.8.m8.1c">25</annotation><annotation encoding="application/x-llamapun" id="S3.SS8.SSS2.p2.8.m8.1d">25</annotation></semantics></math><math id="S3.SS8.SSS2.p2.9.m9.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.SS8.SSS2.p2.9.m9.1a"><mo id="S3.SS8.SSS2.p2.9.m9.1.1" xref="S3.SS8.SSS2.p2.9.m9.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS8.SSS2.p2.9.m9.1b"><times id="S3.SS8.SSS2.p2.9.m9.1.1.cmml" xref="S3.SS8.SSS2.p2.9.m9.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS8.SSS2.p2.9.m9.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS8.SSS2.p2.9.m9.1d">×</annotation></semantics></math><math id="S3.SS8.SSS2.p2.10.m10.1" class="ltx_Math" alttext="25" display="inline"><semantics id="S3.SS8.SSS2.p2.10.m10.1a"><mn id="S3.SS8.SSS2.p2.10.m10.1.1" xref="S3.SS8.SSS2.p2.10.m10.1.1.cmml">25</mn><annotation-xml encoding="MathML-Content" id="S3.SS8.SSS2.p2.10.m10.1b"><cn type="integer" id="S3.SS8.SSS2.p2.10.m10.1.1.cmml" xref="S3.SS8.SSS2.p2.10.m10.1.1">25</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS8.SSS2.p2.10.m10.1c">25</annotation><annotation encoding="application/x-llamapun" id="S3.SS8.SSS2.p2.10.m10.1d">25</annotation></semantics></math><math id="S3.SS8.SSS2.p2.11.m11.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.SS8.SSS2.p2.11.m11.1a"><mo id="S3.SS8.SSS2.p2.11.m11.1.1" xref="S3.SS8.SSS2.p2.11.m11.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS8.SSS2.p2.11.m11.1b"><times id="S3.SS8.SSS2.p2.11.m11.1.1.cmml" xref="S3.SS8.SSS2.p2.11.m11.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS8.SSS2.p2.11.m11.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS8.SSS2.p2.11.m11.1d">×</annotation></semantics></math><math id="S3.SS8.SSS2.p2.12.m12.1" class="ltx_Math" alttext="128" display="inline"><semantics id="S3.SS8.SSS2.p2.12.m12.1a"><mn id="S3.SS8.SSS2.p2.12.m12.1.1" xref="S3.SS8.SSS2.p2.12.m12.1.1.cmml">128</mn><annotation-xml encoding="MathML-Content" id="S3.SS8.SSS2.p2.12.m12.1b"><cn type="integer" id="S3.SS8.SSS2.p2.12.m12.1.1.cmml" xref="S3.SS8.SSS2.p2.12.m12.1.1">128</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS8.SSS2.p2.12.m12.1c">128</annotation><annotation encoding="application/x-llamapun" id="S3.SS8.SSS2.p2.12.m12.1d">128</annotation></semantics></math>, while the query is <math id="S3.SS8.SSS2.p2.13.m13.1" class="ltx_Math" alttext="128" display="inline"><semantics id="S3.SS8.SSS2.p2.13.m13.1a"><mn id="S3.SS8.SSS2.p2.13.m13.1.1" xref="S3.SS8.SSS2.p2.13.m13.1.1.cmml">128</mn><annotation-xml encoding="MathML-Content" id="S3.SS8.SSS2.p2.13.m13.1b"><cn type="integer" id="S3.SS8.SSS2.p2.13.m13.1.1.cmml" xref="S3.SS8.SSS2.p2.13.m13.1.1">128</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS8.SSS2.p2.13.m13.1c">128</annotation><annotation encoding="application/x-llamapun" id="S3.SS8.SSS2.p2.13.m13.1d">128</annotation></semantics></math> dimensional vector. Although <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> uses only RGB stream, additional modalities like optical flow and audio signal (as in competing works) would further increase the compute complexity. Further, the Transformer model was found to be sub-optimal for action localization, perhaps due to its tendency to incorporate global information. Therefore, it is important to achieve the right trade-off between the global and local context for problems that demand precise delineation (<em id="S3.SS8.SSS2.p2.13.2" class="ltx_emph ltx_font_italic">e.g.</em>, action localization and segmentation).</p>
</div>
<div id="S3.SS8.SSS2.p3" class="ltx_para">
<p id="S3.SS8.SSS2.p3.1" class="ltx_p">Human action recognition based on skeleton representation requires understanding relationships between different joints of a body in a given frame as well as between different frames of a video. Plizzari <span id="S3.SS8.SSS2.p3.1.1" class="ltx_text ltx_font_italic">et al.<cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS8.SSS2.p3.1.1.1.1" class="ltx_text ltx_font_upright">[</span><a href="#bib.bib216" title="" class="ltx_ref">216</a><span id="S3.SS8.SSS2.p3.1.1.2.2" class="ltx_text ltx_font_upright">]</span></cite></span> proposed a two-stream Transformer network to model such relationships. They introduced spatial self-attention (SSA) to model relations between different body-joints (Fig. <a href="#S3.F13.sf1" title="13(a) ‣ Figure 14 ‣ 3.8.3 Video Instance Segmentation ‣ 3.8 Video Understanding ‣ 3 Self-Attention &amp; Transformers in Vision ‣ Transformers in Vision: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13(a)</span></a>)
while temporal self-attention (TSA) to capture long-range inter-frame dependencies (Fig. <a href="#S3.F13.sf2" title="13(b) ‣ Figure 14 ‣ 3.8.3 Video Instance Segmentation ‣ 3.8 Video Understanding ‣ 3 Self-Attention &amp; Transformers in Vision ‣ Transformers in Vision: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13(b)</span></a>).
They first used a small residual network to extract features from skeleton data and then used SSA and TSA modules to process those feature maps. SSA finds the correlation between each pair of joints independently, while TSA focuses on how features of a certain joint change between frames along the temporal dimension. The purpose of SSA is to discover relationships among the surrounding joints in the same way as the Transformer relates different words in a phrase. On the other hand, TSA finds long-range relations between frames, similar to how relations among phrases are built in NLP. The two streamed model achieves state-of-the-art results on NTU-RGB+D 60 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib217" title="" class="ltx_ref">217</a>]</cite> and NTU-RGB+D 120 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib218" title="" class="ltx_ref">218</a>]</cite> datasets.</p>
</div>
<div id="S3.SS8.SSS2.p4" class="ltx_para">
<p id="S3.SS8.SSS2.p4.1" class="ltx_p"><span id="S3.SS8.SSS2.p4.1.1" class="ltx_text" style="color:#000000;">Multiscale Vision Transformers (MViT) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib219" title="" class="ltx_ref">219</a>]</cite> build a feature hierarchy by progressively expanding the channel capacity and reducing the spatio-temporal resolution in videos. They introduce multi-head pooling attention to gradually change the visual resolution in their pyramid structure. TimeSFormer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib213" title="" class="ltx_ref">213</a>]</cite> extends ViTs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib132" title="" class="ltx_ref">132</a>]</cite> to videos, by considering the video as a sequence of patches extracted from individual frames. To capture spatio-temporal relationships, they propose divided attention i.e., spatial and temporal attentions are separately applied within each block. TimeSFormer demonstrates SoTA performance on action recognition, and can be applied to clips over one minute. Another notable pure-transformer based model is the Video Vision Transformer (ViViT) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib212" title="" class="ltx_ref">212</a>]</cite>. First, the spatio-temporal tokens are extracted and then efficient factorised versions of self-attention are applied to encode relationships between tokens. However, they require initialization with image-pretrained models to effectively learn the ViT models. There has also been concurrent work on learning sound pretrained models using self-supervised learning with ViTs. An important recent effort is the long-short contrastive learning (LSTCL) framework <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib220" title="" class="ltx_ref">220</a>]</cite>, which reconstructs representations from different time-scales (narrow and broad) as auxiliary learning tasks and demonstrates good down-stream performance.
</span></p>
</div>
</section>
<section id="S3.SS8.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.8.3 </span>Video Instance Segmentation</h4>

<div id="S3.SS8.SSS3.p1" class="ltx_para">
<p id="S3.SS8.SSS3.p1.1" class="ltx_p">The Video Instance Segmentation Transformer (VisTR) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib209" title="" class="ltx_ref">209</a>]</cite> model extends DETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> for video object instance segmentation (VIS) task. Local features are obtained using a backbone CNN on a collection of video frames. An encoder and a decoder Transformer is used similar to DETR to frame the instance segmentation problem as a sequence to sequence prediction task. The input frame-level features are concatenated to form clip representations and the Transformer outputs instance predictions in a order that is consistent across frames. This integrates the object detection and tracking with-in a single unified architecture. The predicted outputs are matched with the ground-truth using bipartitie matching. Similar to Mask R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib127" title="" class="ltx_ref">127</a>]</cite>, a separate head is used to predict the instance mask based on self-attention and 3D convolutions. The overall results are competitive among the single model approaches on YouTube VIS dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib221" title="" class="ltx_ref">221</a>]</cite>, but performs somewhat lower compared to more complex CNN-based models such as MaskProp <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib222" title="" class="ltx_ref">222</a>]</cite>.</p>
</div>
<figure id="S3.F14" class="ltx_figure">
<div class="ltx_flex_figure">

<div class="ltx_flex_cell 
                  ltx_flex_size_2">
<figure id="S3.F13.sf1" class="ltx_figure ltx_flex_size_2 ltx_align_center"><img src="/html/2101.01169/assets/Figs/SSA.png" id="S3.F13.sf1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="326" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F13.sf1.3.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S3.F13.sf1.4.2" class="ltx_text" style="font-size:90%;">Spatial Self-Attention</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell 
                  ltx_flex_size_2">
<figure id="S3.F13.sf2" class="ltx_figure ltx_flex_size_2 ltx_align_center"><img src="/html/2101.01169/assets/Figs/TSA.png" id="S3.F13.sf2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="373" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F13.sf2.3.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S3.F13.sf2.4.2" class="ltx_text" style="font-size:90%;">Temporal Self-Attention</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F14.3.1.1" class="ltx_text" style="font-size:90%;">Figure 14</span>: </span><span id="S3.F14.4.2" class="ltx_text" style="font-size:90%;">Spatial/Temporal Attention for Skeleton Data Representations. Relationships between body-joints and inter-frame dependencies are modeled using two dedicated self-attention modules. Figure is from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib216" title="" class="ltx_ref">216</a>]</cite>.</span></figcaption>
</figure>
</section>
</section>
<section id="S3.SS9" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.9 </span><span id="S3.SS9.1.1" class="ltx_text ltx_font_italic">Transformers in Low-shot Learning</span>
</h3>

<div id="S3.SS9.p1" class="ltx_para">
<p id="S3.SS9.p1.1" class="ltx_p">In the few-shot learning settings, a support set is provided at the inference to adapt to a novel set of categories. Transformer models have been used to learn set-to-set mappings on this support set <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> or learn the spatial relationships between a given input query and support set samples <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>. In terms of absolute performance, the patch-wise spatial self-attention between query and support set images excels compared to an image level association learned in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>. However, the patch-wise attention computation is computationally expensive. We elaborate on these approaches below.</p>
</div>
<div id="S3.SS9.p2" class="ltx_para">
<p id="S3.SS9.p2.1" class="ltx_p">Doersch <span id="S3.SS9.p2.1.1" class="ltx_text ltx_font_italic">et al.<cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS9.p2.1.1.1.1" class="ltx_text ltx_font_upright">[</span><a href="#bib.bib25" title="" class="ltx_ref">25</a><span id="S3.SS9.p2.1.1.2.2" class="ltx_text ltx_font_upright">]</span></cite></span> explore the utility of self-supervision and Transformer model for few-shot fine-grained classification, where distribution mismatch exists between training and evaluation phases. They develop Cross-Transformer model to relate a given query image with the few-examples available in the support set. To this end, the Transformer finds spatially similar regions in the query and support set images, and the corresponding features are then used to obtain class decisions for the query. The queries in the Transformer architecture are derived from the grid features obtained using the query image. Similarly, grid features from the support images are used to construct keys and values which are in turn used to derive attended outputs. This approach, besides a contrastive self-supervision based training mechanism, leads to the best performance on the challenging Meta-dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib223" title="" class="ltx_ref">223</a>]</cite>.</p>
</div>
<div id="S3.SS9.p3" class="ltx_para">
<p id="S3.SS9.p3.1" class="ltx_p">Ye <span id="S3.SS9.p3.1.1" class="ltx_text ltx_font_italic">et al.<cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS9.p3.1.1.1.1" class="ltx_text ltx_font_upright">[</span><a href="#bib.bib26" title="" class="ltx_ref">26</a><span id="S3.SS9.p3.1.1.2.2" class="ltx_text ltx_font_upright">]</span></cite></span> propose to adapt the few-shot embeddings learned on the base classes to the few-shot target classes during inference using a Transformer module. This leads to task-specific embeddings that perform better on the discriminative tasks such as few-shot classification. While many other set-to-set functions are also evaluated, such as Graph convolutional networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib224" title="" class="ltx_ref">224</a>]</cite>, Bidirectional LSTMs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> and DeepSets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib225" title="" class="ltx_ref">225</a>]</cite>, the best performance is achieved with the Transformer-based mapping. This is attributed to the better contextualization, task interpolation and extrapolation capability of Transformers and their permutation invariance while maintaining a relatively lower parameter complexity. The Transformer architecture in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> follows the standard model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. The embeddings are adapted using a contrastive loss function for preserving discriminative properties (Fig. <a href="#S3.F15" title="Figure 15 ‣ 3.9 Transformers in Low-shot Learning ‣ 3 Self-Attention &amp; Transformers in Vision ‣ Transformers in Vision: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">15</span></a>).
The resulting model achieves strong performance on inductive, transductive, and generalized FSL tasks.</p>
</div>
<div id="S3.SS9.p4" class="ltx_para">
<p id="S3.SS9.p4.1" class="ltx_p"><span id="S3.SS9.p4.1.1" class="ltx_text" style="color:#000000;">Liu <span id="S3.SS9.p4.1.1.1" class="ltx_text ltx_font_italic">et al.<cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS9.p4.1.1.1.1.1" class="ltx_text ltx_font_upright">[</span><a href="#bib.bib226" title="" class="ltx_ref">226</a><span id="S3.SS9.p4.1.1.1.2.2" class="ltx_text ltx_font_upright">]</span></cite></span> learn a multi-head self-attention based module, to integrate the visual representation learned by the models trained on different domains present in the meta-dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib223" title="" class="ltx_ref">223</a>]</cite>. The Universal Representation Transformer (URT) layer dynamically re-weights the representations from different domain-specific backbones, and proves very effective in handling few shot tasks across a variety of data distributions.
</span></p>
</div>
<figure id="S3.F15" class="ltx_figure"><img src="/html/2101.01169/assets/Figs/Feat_transformer.png" id="S3.F15.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="262" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F15.3.1.1" class="ltx_text" style="font-size:90%;">Figure 15</span>: </span><span id="S3.F15.4.2" class="ltx_text" style="font-size:90%;">An overview of FEAT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>. Compared to the conventional instance embedding methods in FSL that keep the embedding function same for all tasks (a), FEAT uses a set-to-set function to adapt the embedding function to each FSL task (b). It evaluates several set-to-set functions and found the Transformer module to be the most suitable choice for FSL. Figure from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>.</span></figcaption>
</figure>
</section>
<section id="S3.SS10" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.10 </span><span id="S3.SS10.1.1" class="ltx_text ltx_font_italic">Transformers for Clustering</span>
</h3>

<div id="S3.SS10.p1" class="ltx_para">
<p id="S3.SS10.p1.4" class="ltx_p">Clustering aims to discover structure in the data by grouping similar data points together. It has numerous applications such as data visualization and interpretation, anomaly detection, and open-set categorization. Neural networks have been developed for set prediction problems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib225" title="" class="ltx_ref">225</a>, <a href="#bib.bib227" title="" class="ltx_ref">227</a>]</cite>, however, the setpoints are processed individually which can lose information about inter-point relationships. Recent works employ Transformers that operate on set inputs called the Set Transformers (ST) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib228" title="" class="ltx_ref">228</a>]</cite> for <em id="S3.SS10.p1.4.1" class="ltx_emph ltx_font_italic">amortized</em> clustering. Amortized clustering is a challenging problem that seeks to learn a parametric function that can map an input set of points to their corresponding cluster centers. Lee <span id="S3.SS10.p1.4.2" class="ltx_text ltx_font_italic">et al.<cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS10.p1.4.2.1.1" class="ltx_text ltx_font_upright">[</span><a href="#bib.bib228" title="" class="ltx_ref">228</a><span id="S3.SS10.p1.4.2.2.2" class="ltx_text ltx_font_upright">]</span></cite></span> propose to learn such a mapping function using a Transformer architecture comprising of multi-head self-attention blocks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>.
The Transformer model is permutation invariant by design and allows encoding both pair-wise and higher-order relationships between the input points.
However, a full Transformer would lead to a high computational cost of <math id="S3.SS10.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{O}(n^{2})" display="inline"><semantics id="S3.SS10.p1.1.m1.1a"><mrow id="S3.SS10.p1.1.m1.1.1" xref="S3.SS10.p1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS10.p1.1.m1.1.1.3" xref="S3.SS10.p1.1.m1.1.1.3.cmml">𝒪</mi><mo id="S3.SS10.p1.1.m1.1.1.2" xref="S3.SS10.p1.1.m1.1.1.2.cmml" lspace='0px' rspace='0px'></mo><mrow id="S3.SS10.p1.1.m1.1.1.1.1" xref="S3.SS10.p1.1.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS10.p1.1.m1.1.1.1.1.2" xref="S3.SS10.p1.1.m1.1.1.1.1.1.cmml">(</mo><msup id="S3.SS10.p1.1.m1.1.1.1.1.1" xref="S3.SS10.p1.1.m1.1.1.1.1.1.cmml"><mi id="S3.SS10.p1.1.m1.1.1.1.1.1.2" xref="S3.SS10.p1.1.m1.1.1.1.1.1.2.cmml">n</mi><mn id="S3.SS10.p1.1.m1.1.1.1.1.1.3" xref="S3.SS10.p1.1.m1.1.1.1.1.1.3.cmml">2</mn></msup><mo stretchy="false" id="S3.SS10.p1.1.m1.1.1.1.1.3" xref="S3.SS10.p1.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS10.p1.1.m1.1b"><apply id="S3.SS10.p1.1.m1.1.1.cmml" xref="S3.SS10.p1.1.m1.1.1"><times id="S3.SS10.p1.1.m1.1.1.2.cmml" xref="S3.SS10.p1.1.m1.1.1.2"></times><ci id="S3.SS10.p1.1.m1.1.1.3.cmml" xref="S3.SS10.p1.1.m1.1.1.3">𝒪</ci><apply id="S3.SS10.p1.1.m1.1.1.1.1.1.cmml" xref="S3.SS10.p1.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS10.p1.1.m1.1.1.1.1.1.1.cmml" xref="S3.SS10.p1.1.m1.1.1.1.1">superscript</csymbol><ci id="S3.SS10.p1.1.m1.1.1.1.1.1.2.cmml" xref="S3.SS10.p1.1.m1.1.1.1.1.1.2">𝑛</ci><cn type="integer" id="S3.SS10.p1.1.m1.1.1.1.1.1.3.cmml" xref="S3.SS10.p1.1.m1.1.1.1.1.1.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS10.p1.1.m1.1c">\mathcal{O}(n^{2})</annotation><annotation encoding="application/x-llamapun" id="S3.SS10.p1.1.m1.1d">caligraphic_O ( italic_n start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT )</annotation></semantics></math> in each self-attention layer, where <math id="S3.SS10.p1.2.m2.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.SS10.p1.2.m2.1a"><mi id="S3.SS10.p1.2.m2.1.1" xref="S3.SS10.p1.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS10.p1.2.m2.1b"><ci id="S3.SS10.p1.2.m2.1.1.cmml" xref="S3.SS10.p1.2.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS10.p1.2.m2.1c">n</annotation><annotation encoding="application/x-llamapun" id="S3.SS10.p1.2.m2.1d">italic_n</annotation></semantics></math> is the number of points in the set. ST reduces this cost to <math id="S3.SS10.p1.3.m3.1" class="ltx_Math" alttext="\mathcal{O}(mn)" display="inline"><semantics id="S3.SS10.p1.3.m3.1a"><mrow id="S3.SS10.p1.3.m3.1.1" xref="S3.SS10.p1.3.m3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS10.p1.3.m3.1.1.3" xref="S3.SS10.p1.3.m3.1.1.3.cmml">𝒪</mi><mo id="S3.SS10.p1.3.m3.1.1.2" xref="S3.SS10.p1.3.m3.1.1.2.cmml" lspace='0px' rspace='0px'></mo><mrow id="S3.SS10.p1.3.m3.1.1.1.1" xref="S3.SS10.p1.3.m3.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS10.p1.3.m3.1.1.1.1.2" xref="S3.SS10.p1.3.m3.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS10.p1.3.m3.1.1.1.1.1" xref="S3.SS10.p1.3.m3.1.1.1.1.1.cmml"><mi id="S3.SS10.p1.3.m3.1.1.1.1.1.2" xref="S3.SS10.p1.3.m3.1.1.1.1.1.2.cmml">m</mi><mo id="S3.SS10.p1.3.m3.1.1.1.1.1.1" xref="S3.SS10.p1.3.m3.1.1.1.1.1.1.cmml" lspace='0px' rspace='0px'></mo><mi id="S3.SS10.p1.3.m3.1.1.1.1.1.3" xref="S3.SS10.p1.3.m3.1.1.1.1.1.3.cmml">n</mi></mrow><mo stretchy="false" id="S3.SS10.p1.3.m3.1.1.1.1.3" xref="S3.SS10.p1.3.m3.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS10.p1.3.m3.1b"><apply id="S3.SS10.p1.3.m3.1.1.cmml" xref="S3.SS10.p1.3.m3.1.1"><times id="S3.SS10.p1.3.m3.1.1.2.cmml" xref="S3.SS10.p1.3.m3.1.1.2"></times><ci id="S3.SS10.p1.3.m3.1.1.3.cmml" xref="S3.SS10.p1.3.m3.1.1.3">𝒪</ci><apply id="S3.SS10.p1.3.m3.1.1.1.1.1.cmml" xref="S3.SS10.p1.3.m3.1.1.1.1"><times id="S3.SS10.p1.3.m3.1.1.1.1.1.1.cmml" xref="S3.SS10.p1.3.m3.1.1.1.1.1.1"></times><ci id="S3.SS10.p1.3.m3.1.1.1.1.1.2.cmml" xref="S3.SS10.p1.3.m3.1.1.1.1.1.2">𝑚</ci><ci id="S3.SS10.p1.3.m3.1.1.1.1.1.3.cmml" xref="S3.SS10.p1.3.m3.1.1.1.1.1.3">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS10.p1.3.m3.1c">\mathcal{O}(mn)</annotation><annotation encoding="application/x-llamapun" id="S3.SS10.p1.3.m3.1d">caligraphic_O ( italic_m italic_n )</annotation></semantics></math> by using an Induced Self-Attention Block that uses a low-rank projection (<math id="S3.SS10.p1.4.m4.1" class="ltx_Math" alttext="H\in\mathbb{R}^{m}" display="inline"><semantics id="S3.SS10.p1.4.m4.1a"><mrow id="S3.SS10.p1.4.m4.1.1" xref="S3.SS10.p1.4.m4.1.1.cmml"><mi id="S3.SS10.p1.4.m4.1.1.2" xref="S3.SS10.p1.4.m4.1.1.2.cmml">H</mi><mo id="S3.SS10.p1.4.m4.1.1.1" xref="S3.SS10.p1.4.m4.1.1.1.cmml">∈</mo><msup id="S3.SS10.p1.4.m4.1.1.3" xref="S3.SS10.p1.4.m4.1.1.3.cmml"><mi id="S3.SS10.p1.4.m4.1.1.3.2" xref="S3.SS10.p1.4.m4.1.1.3.2.cmml">ℝ</mi><mi id="S3.SS10.p1.4.m4.1.1.3.3" xref="S3.SS10.p1.4.m4.1.1.3.3.cmml">m</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS10.p1.4.m4.1b"><apply id="S3.SS10.p1.4.m4.1.1.cmml" xref="S3.SS10.p1.4.m4.1.1"><in id="S3.SS10.p1.4.m4.1.1.1.cmml" xref="S3.SS10.p1.4.m4.1.1.1"></in><ci id="S3.SS10.p1.4.m4.1.1.2.cmml" xref="S3.SS10.p1.4.m4.1.1.2">𝐻</ci><apply id="S3.SS10.p1.4.m4.1.1.3.cmml" xref="S3.SS10.p1.4.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS10.p1.4.m4.1.1.3.1.cmml" xref="S3.SS10.p1.4.m4.1.1.3">superscript</csymbol><ci id="S3.SS10.p1.4.m4.1.1.3.2.cmml" xref="S3.SS10.p1.4.m4.1.1.3.2">ℝ</ci><ci id="S3.SS10.p1.4.m4.1.1.3.3.cmml" xref="S3.SS10.p1.4.m4.1.1.3.3">𝑚</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS10.p1.4.m4.1c">H\in\mathbb{R}^{m}</annotation><annotation encoding="application/x-llamapun" id="S3.SS10.p1.4.m4.1d">italic_H ∈ blackboard_R start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT</annotation></semantics></math>) to allow operating on large sets. The model was trained to learn optimal parameters that maximize the likelihood of a mixture of Gaussians (MoGs). Thus MoG parameters are estimated by the ST given a set of data points. Beyond amortized clustering, <span id="S3.SS10.p1.4.3" class="ltx_text" style="color:#000000;">ST is a generic framework which can handle other set-input problems such as counting unique elements in an input set, multi-instance learning, set anomaly detection, and 3D point-cloud classification.</span> More recently, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib229" title="" class="ltx_ref">229</a>]</cite> improves <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib228" title="" class="ltx_ref">228</a>]</cite> by taking a sequential approach to cluster generation, thereby allowing assignment to a variable number of clusters.</p>
</div>
</section>
<section id="S3.SS11" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.11 </span><span id="S3.SS11.1.1" class="ltx_text ltx_font_italic">Transformers for 3D Analysis</span>
</h3>

<div id="S3.SS11.p1" class="ltx_para">
<p id="S3.SS11.p1.1" class="ltx_p">Given the irregular (variable number of points) and permutation invariant nature of 3D point cloud representations, Transformers provide a promising mechanism to encode rich relationships between 3D data points. To this end, recent works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib230" title="" class="ltx_ref">230</a>, <a href="#bib.bib231" title="" class="ltx_ref">231</a>]</cite> are motivated by the capability of Transformers to learn set-functions. Specifically, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib230" title="" class="ltx_ref">230</a>]</cite> introduced a Point Transformer which uses vector attention to learn weights for each channel, while <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib231" title="" class="ltx_ref">231</a>]</cite> suggest an alternate design where local 3D structure is explicitly encoded. The non-local nature of Transformers is exploited in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite> towards an accurate human pose and mesh reconstruction algorithm. We discuss these approaches below.</p>
</div>
<div id="S3.SS11.p2" class="ltx_para">
<p id="S3.SS11.p2.1" class="ltx_p">Self-attention being a set-operator is ideally suited for processing point clouds, a 3D data representation that demands invariance to number of points and their permutations. Zhao <span id="S3.SS11.p2.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib230" title="" class="ltx_ref">230</a>]</cite> propose a point Transformer layer that applies self-attention in the local neighborhood of 3D points.
The proposed layer builds on vectorized self-attention network (SAN) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib82" title="" class="ltx_ref">82</a>]</cite> where attention weights are represented with vectors.Furthermore, a positional encoding
is added both to the attention vector and transformed features (value vectors) to represent location information. The point Transformer layer is sandwiched between two linear layers to create a point Transformer block that is stacked multiple times in the developed network architecture. Their design also included transition down/up blocks to reduce/increase the number of points in the input (in a typical encoding-decoding pipeline style). The resulting architecture shows promising results on the 3D classification and segmentation tasks.
</p>
</div>
<figure id="S3.F16" class="ltx_figure"><img src="/html/2101.01169/assets/Figs/METRO.png" id="S3.F16.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="568" height="210" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F16.3.1.1" class="ltx_text" style="font-size:90%;">Figure 16</span>: </span><span id="S3.F16.4.2" class="ltx_text" style="font-size:90%;">Mesh Transformer architecture. The joint and vertex queries are appended with positional embeddings and passed through multiple self-attention layers to jointly regress 3D coordinates of joints and mesh vertices. Figure is from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>. </span></figcaption>
</figure>
<div id="S3.SS11.p3" class="ltx_para">
<p id="S3.SS11.p3.1" class="ltx_p">The Point Cloud Transformer (PCT) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib231" title="" class="ltx_ref">231</a>]</cite> is a parallel work to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib230" title="" class="ltx_ref">230</a>]</cite> and motivated by the permutation invariance property of Transformers. However, compared to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib230" title="" class="ltx_ref">230</a>]</cite>, it is more directly based on the conventional Transformer architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> and does not involve vector attention. The key modifications include a 3D coordinate-based position encoding, an offset attention module, and a neighbor embedding that encodes local 3D structure in point-clouds. Specifically, the offset attention layer calculates the difference between the self-attended features and the input features using element-wise subtraction. The local neighbor embedding simply finds self-attention relationships among a group of points instead of individual 3D points. Explicitly incorporating local neighbourhood information makes this a more efficient architecture compared to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib230" title="" class="ltx_ref">230</a>]</cite>. The method shows promising performance on 3D shape classification, normal estimation and segmentation tasks on ModelNet40 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib232" title="" class="ltx_ref">232</a>]</cite> and ShapeNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib233" title="" class="ltx_ref">233</a>]</cite> datasets.</p>
</div>
<div id="S3.SS11.p4" class="ltx_para">
<p id="S3.SS11.p4.3" class="ltx_p">The Mesh Transformer (METRO) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite> model targets 3D human pose and mesh reconstruction from a single 2D image. A key challenge here is to faithfully learn the non-local interactions between body-joints and mesh vertices (<em id="S3.SS11.p4.3.1" class="ltx_emph ltx_font_italic">e.g.</em>, hand and foot). The expressivity of Transformer network is used to jointly model <em id="S3.SS11.p4.3.2" class="ltx_emph ltx_font_italic">vertex to vertex</em> relationships in a mesh as well as the <em id="S3.SS11.p4.3.3" class="ltx_emph ltx_font_italic">vertex to body-joint</em> relationships. The self-attention mechanism can attend to any combination of vertices in the mesh, thereby encoding non-local relationships.
The multi-layer Transformer architecture sequentially performs dimensionality reduction to map the 2D image to 3D mesh. Position encoding is performed using the 3D coordinates (<math id="S3.SS11.p4.1.m1.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S3.SS11.p4.1.m1.1a"><mi id="S3.SS11.p4.1.m1.1.1" xref="S3.SS11.p4.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS11.p4.1.m1.1b"><ci id="S3.SS11.p4.1.m1.1.1.cmml" xref="S3.SS11.p4.1.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS11.p4.1.m1.1c">x</annotation><annotation encoding="application/x-llamapun" id="S3.SS11.p4.1.m1.1d">italic_x</annotation></semantics></math>,<math id="S3.SS11.p4.2.m2.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S3.SS11.p4.2.m2.1a"><mi id="S3.SS11.p4.2.m2.1.1" xref="S3.SS11.p4.2.m2.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S3.SS11.p4.2.m2.1b"><ci id="S3.SS11.p4.2.m2.1.1.cmml" xref="S3.SS11.p4.2.m2.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS11.p4.2.m2.1c">y</annotation><annotation encoding="application/x-llamapun" id="S3.SS11.p4.2.m2.1d">italic_y</annotation></semantics></math>,<math id="S3.SS11.p4.3.m3.1" class="ltx_Math" alttext="z" display="inline"><semantics id="S3.SS11.p4.3.m3.1a"><mi id="S3.SS11.p4.3.m3.1.1" xref="S3.SS11.p4.3.m3.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S3.SS11.p4.3.m3.1b"><ci id="S3.SS11.p4.3.m3.1.1.cmml" xref="S3.SS11.p4.3.m3.1.1">𝑧</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS11.p4.3.m3.1c">z</annotation><annotation encoding="application/x-llamapun" id="S3.SS11.p4.3.m3.1d">italic_z</annotation></semantics></math>) of each vertex and each body-joint. Similar to masked language modeling in NLP, METRO uses masked vertex modeling (MVM) which randomly masks some percentage of input queries (see Fig. <a href="#S3.F16" title="Figure 16 ‣ 3.11 Transformers for 3D Analysis ‣ 3 Self-Attention &amp; Transformers in Vision ‣ Transformers in Vision: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">16</span></a>). The Transformer is tasked with regressing all the joints and vertices which helps encode inter-dependencies between them.
METRO obtains state-of-the-art results on human mesh reconstruction on Human3.6M <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib234" title="" class="ltx_ref">234</a>]</cite> and 3DPW <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib235" title="" class="ltx_ref">235</a>]</cite> datasets. Since the approach does not depends on a parametric mesh model, it generalizes well to other reconstruction tasks such as 3D hand reconstruction <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib236" title="" class="ltx_ref">236</a>]</cite>. Overall, this is the first effort to employ Transformers for 3D human reconstruction tasks and leads to fairly good results.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps" style="color:#000000;">Open Challenges<span id="S4.1.1.1" class="ltx_text" style="color:black;"> &amp; Future Directions</span></span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Despite excellent performance from Transformer models and their interesting salient features (Table <a href="#S4.T1" title="TABLE I ‣ 4.1 High Computational Cost ‣ 4 Open Challenges &amp; Future Directions ‣ Transformers in Vision: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>), there exist several challenges associated with their applicability to practical settings (Table <a href="#S4.T2" title="TABLE II ‣ 4.1 High Computational Cost ‣ 4 Open Challenges &amp; Future Directions ‣ Transformers in Vision: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>). The most important bottlenecks include requirement for large-amounts of training data and associated high computational costs. There have also been some challenges to visualize and interpret Transformer models. In this section, we provide an overview of these challenges, mention some of the recent efforts to address those limitations and highlight the open research questions.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span><span id="S4.SS1.1.1" class="ltx_text ltx_font_italic">High Computational Cost</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.2" class="ltx_p"><span id="S4.SS1.p1.2.1" class="ltx_text" style="color:#000000;">As discussed in Sec. <a href="#S1" title="1 Introduction ‣ Transformers in Vision: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, a strength of Transformer models is their flexibility to scale to high parametric complexity. While this is a remarkable property that allows training enormous sized models, this results in high training and inference cost (a detailed comparison between CNN and ViTs is shown in Table <a href="#S4.T3" title="TABLE III ‣ 4.1 High Computational Cost ‣ 4 Open Challenges &amp; Future Directions ‣ Transformers in Vision: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>).</span> As an example, the BERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> basic model (with 109 million parameters) took around 1.89 peta-flop days<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>A peta-flop day is a measure of computation and equals to performing <math id="footnote2.m1.1" class="ltx_Math" alttext="10^{15}" display="inline"><semantics id="footnote2.m1.1b"><msup id="footnote2.m1.1.1" xref="footnote2.m1.1.1.cmml"><mn id="footnote2.m1.1.1.2" xref="footnote2.m1.1.1.2.cmml">10</mn><mn id="footnote2.m1.1.1.3" xref="footnote2.m1.1.1.3.cmml">15</mn></msup><annotation-xml encoding="MathML-Content" id="footnote2.m1.1c"><apply id="footnote2.m1.1.1.cmml" xref="footnote2.m1.1.1"><csymbol cd="ambiguous" id="footnote2.m1.1.1.1.cmml" xref="footnote2.m1.1.1">superscript</csymbol><cn type="integer" id="footnote2.m1.1.1.2.cmml" xref="footnote2.m1.1.1.2">10</cn><cn type="integer" id="footnote2.m1.1.1.3.cmml" xref="footnote2.m1.1.1.3">15</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="footnote2.m1.1d">10^{15}</annotation><annotation encoding="application/x-llamapun" id="footnote2.m1.1e">10 start_POSTSUPERSCRIPT 15 end_POSTSUPERSCRIPT</annotation></semantics></math> neural net operations per second for one complete day.</span></span></span> for training, while the latest GPT3 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> model (175 billion parameters) took around 3640 peta-flop days for training (a staggering <math id="S4.SS1.p1.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S4.SS1.p1.1.m1.1a"><mo id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><csymbol cd="latexml" id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.1.m1.1d">∼</annotation></semantics></math>1925<math id="S4.SS1.p1.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS1.p1.2.m2.1a"><mo id="S4.SS1.p1.2.m2.1.1" xref="S4.SS1.p1.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.2.m2.1b"><times id="S4.SS1.p1.2.m2.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.2.m2.1d">×</annotation></semantics></math> increase). This comes with a huge price tag, <em id="S4.SS1.p1.2.2" class="ltx_emph ltx_font_italic">e.g.</em>, according to one estimate <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib237" title="" class="ltx_ref">237</a>]</cite>, GPT3 training might have cost OpenAI 4.6 million USD. Additionally, these large-scale models require aggressive compression (<em id="S4.SS1.p1.2.3" class="ltx_emph ltx_font_italic">e.g.</em>, distillation) to make them feasible for real-world settings.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p"><span id="S4.SS1.p2.1.1" class="ltx_text" style="color:#000000;">An empirical study on the scalability of Vision Transformers for number of parameters (ranging from five million to two billion), size of the training datasets (ranging from 30 million to three billion training images), and compute budget (1-10000 TPU core-days) is presented in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib238" title="" class="ltx_ref">238</a>]</cite>. From this study, We can draw the following conclusions (a) scaling up on compute, model and size of training samples improves performance (b) only large models (with more parameters) can benefit from more training data, and the performance of smaller models platueas quickly and can not leverage from additional data. This indicates that large scale models have the capacity to further enhance their representation learning capabilities. However, with the current designs, scaling upon Transformer models is expensive and compute prohibitive, thus necessitating the need for efficient designs.</span></p>
</div>
<figure id="S4.T1" class="ltx_table">
<div id="S4.T1.1" class="ltx_inline-block ltx_transformed_outer" style="width:1192.5pt;height:297.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-198.7pt,49.7pt) scale(0.75,0.75) ;">
<table id="S4.T1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.1.1.2.1" class="ltx_tr" style="background-color:#E6E6E6;">
<th id="S4.T1.1.1.2.1.1" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="width:64.0pt;padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T1.1.1.2.1.1.1" class="ltx_text ltx_font_bold ltx_align_center ltx_align_top" style="font-size:90%;background-color:#E6E6E6;">Task</span></th>
<th id="S4.T1.1.1.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T1.1.1.2.1.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;background-color:#E6E6E6;">Method</span></th>
<th id="S4.T1.1.1.2.1.3" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt" style="width:170.7pt;padding-left:5.0pt;padding-right:5.0pt;">
<p id="S4.T1.1.1.2.1.3.1" class="ltx_p ltx_align_top" style="background-color:#E6E6E6;"><span id="S4.T1.1.1.2.1.3.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Design Highlights</span><span id="S4.T1.1.1.2.1.3.1.2" class="ltx_text" style="font-size:90%;"> (focus on differences with the standard form)</span></p>
</th>
<th id="S4.T1.1.1.2.1.4" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt" style="width:85.4pt;padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T1.1.1.2.1.4.1" class="ltx_text ltx_font_bold ltx_align_center ltx_align_top" style="font-size:90%;background-color:#E6E6E6;">Input Data Type</span></th>
<th id="S4.T1.1.1.2.1.5" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt" style="width:56.9pt;padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T1.1.1.2.1.5.1" class="ltx_text ltx_font_bold ltx_align_center ltx_align_top" style="font-size:90%;background-color:#E6E6E6;">Label Type</span></th>
<th id="S4.T1.1.1.2.1.6" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt" style="width:85.4pt;padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T1.1.1.2.1.6.1" class="ltx_text ltx_font_bold ltx_align_center ltx_align_top" style="font-size:90%;background-color:#E6E6E6;">Loss</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.1.1.3.1" class="ltx_tr">
<th id="S4.T1.1.1.3.1.1" class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_t" style="width:64.0pt;padding-left:5.0pt;padding-right:5.0pt;">
<p id="S4.T1.1.1.3.1.1.1" class="ltx_p ltx_align_top"><span id="S4.T1.1.1.3.1.1.1.1" class="ltx_text" style="font-size:90%;">Image Classification</span></p>
</th>
<th id="S4.T1.1.1.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S4.T1.1.1.3.1.2.1" class="ltx_text" style="font-size:90%;">ViT </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.1.1.3.1.2.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib11" title="" class="ltx_ref">11</a><span id="S4.T1.1.1.3.1.2.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T1.1.1.3.1.3" class="ltx_td ltx_align_justify ltx_border_t" style="width:170.7pt;padding-left:5.0pt;padding-right:5.0pt;">
<p id="S4.T1.1.1.3.1.3.1" class="ltx_p ltx_align_top"><span id="S4.T1.1.1.3.1.3.1.1" class="ltx_text" style="font-size:90%;">Directly adopted NLP Transformer Encoder for images, Mechanism to linearly embed image patches with positional embedding suitable for the Encoder.</span></p>
</td>
<td id="S4.T1.1.1.3.1.4" class="ltx_td ltx_align_justify ltx_border_t" style="width:85.4pt;padding-left:5.0pt;padding-right:5.0pt;">
<p id="S4.T1.1.1.3.1.4.1" class="ltx_p ltx_align_top"><span id="S4.T1.1.1.3.1.4.1.1" class="ltx_text" style="font-size:90%;">2D Image</span></p>
</td>
<td id="S4.T1.1.1.3.1.5" class="ltx_td ltx_align_justify ltx_border_t" style="width:56.9pt;padding-left:5.0pt;padding-right:5.0pt;">
<p id="S4.T1.1.1.3.1.5.1" class="ltx_p ltx_align_top"><span id="S4.T1.1.1.3.1.5.1.1" class="ltx_text" style="font-size:90%;">Class labels</span></p>
</td>
<td id="S4.T1.1.1.3.1.6" class="ltx_td ltx_align_justify ltx_border_t" style="width:85.4pt;padding-left:5.0pt;padding-right:5.0pt;">
<p id="S4.T1.1.1.3.1.6.1" class="ltx_p ltx_align_top"><span id="S4.T1.1.1.3.1.6.1.1" class="ltx_text" style="font-size:90%;">Cross-entropy</span></p>
</td>
</tr>
<tr id="S4.T1.1.1.4.2" class="ltx_tr">
<th id="S4.T1.1.1.4.2.1" class="ltx_td ltx_th ltx_th_row" style="width:64.0pt;padding-left:5.0pt;padding-right:5.0pt;"></th>
<th id="S4.T1.1.1.4.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S4.T1.1.1.4.2.2.1" class="ltx_text" style="font-size:90%;">DeiT </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.1.1.4.2.2.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib12" title="" class="ltx_ref">12</a><span id="S4.T1.1.1.4.2.2.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T1.1.1.4.2.3" class="ltx_td ltx_align_justify ltx_border_t" style="width:170.7pt;padding-left:5.0pt;padding-right:5.0pt;">
<p id="S4.T1.1.1.4.2.3.1" class="ltx_p ltx_align_top"><span id="S4.T1.1.1.4.2.3.1.1" class="ltx_text" style="font-size:90%;">Transformer as s student while CNN as a teacher, Distillation tokens to produce estimated labels from teacher, Attention between class and distillation tokens.</span></p>
</td>
<td id="S4.T1.1.1.4.2.4" class="ltx_td ltx_align_justify ltx_border_t" style="width:85.4pt;padding-left:5.0pt;padding-right:5.0pt;">
<p id="S4.T1.1.1.4.2.4.1" class="ltx_p ltx_align_top"><span id="S4.T1.1.1.4.2.4.1.1" class="ltx_text" style="font-size:90%;">2D Image</span></p>
</td>
<td id="S4.T1.1.1.4.2.5" class="ltx_td ltx_align_justify ltx_border_t" style="width:56.9pt;padding-left:5.0pt;padding-right:5.0pt;">
<p id="S4.T1.1.1.4.2.5.1" class="ltx_p ltx_align_top"><span id="S4.T1.1.1.4.2.5.1.1" class="ltx_text" style="font-size:90%;">Class labels</span></p>
</td>
<td id="S4.T1.1.1.4.2.6" class="ltx_td ltx_align_justify ltx_border_t" style="width:85.4pt;padding-left:5.0pt;padding-right:5.0pt;">
<p id="S4.T1.1.1.4.2.6.1" class="ltx_p ltx_align_top"><span id="S4.T1.1.1.4.2.6.1.1" class="ltx_text" style="font-size:90%;">Cross-entropy, Distillation loss based on KL-divergence</span></p>
</td>
</tr>
<tr id="S4.T1.1.1.5.3" class="ltx_tr">
<th id="S4.T1.1.1.5.3.1" class="ltx_td ltx_th ltx_th_row" style="width:64.0pt;padding-left:5.0pt;padding-right:5.0pt;"></th>
<th id="S4.T1.1.1.5.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S4.T1.1.1.5.3.2.1" class="ltx_text" style="font-size:90%;">CLIP </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.1.1.5.3.2.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib195" title="" class="ltx_ref">195</a><span id="S4.T1.1.1.5.3.2.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T1.1.1.5.3.3" class="ltx_td ltx_align_justify ltx_border_t" style="width:170.7pt;padding-left:5.0pt;padding-right:5.0pt;">
<p id="S4.T1.1.1.5.3.3.1" class="ltx_p ltx_align_top"><span id="S4.T1.1.1.5.3.3.1.1" class="ltx_text" style="font-size:90%;">Jointly train image and text encoders on image-text pairs, to maximize similarity of valid pairs and minimize otherwise</span></p>
</td>
<td id="S4.T1.1.1.5.3.4" class="ltx_td ltx_align_justify ltx_border_t" style="width:85.4pt;padding-left:5.0pt;padding-right:5.0pt;">
<p id="S4.T1.1.1.5.3.4.1" class="ltx_p ltx_align_top"><span id="S4.T1.1.1.5.3.4.1.1" class="ltx_text" style="font-size:90%;">2D Images &amp; texts</span></p>
</td>
<td id="S4.T1.1.1.5.3.5" class="ltx_td ltx_align_justify ltx_border_t" style="width:56.9pt;padding-left:5.0pt;padding-right:5.0pt;">
<p id="S4.T1.1.1.5.3.5.1" class="ltx_p ltx_align_top"><span id="S4.T1.1.1.5.3.5.1.1" class="ltx_text" style="font-size:90%;">Image-text pairs</span></p>
</td>
<td id="S4.T1.1.1.5.3.6" class="ltx_td ltx_align_justify ltx_border_t" style="width:85.4pt;padding-left:5.0pt;padding-right:5.0pt;">
<p id="S4.T1.1.1.5.3.6.1" class="ltx_p ltx_align_top"><span id="S4.T1.1.1.5.3.6.1.1" class="ltx_text" style="font-size:90%;">Symmetric cross-entropy</span></p>
</td>
</tr>
<tr id="S4.T1.1.1.6.4" class="ltx_tr">
<th id="S4.T1.1.1.6.4.1" class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_t" style="width:64.0pt;padding-left:5.0pt;padding-right:5.0pt;">
<p id="S4.T1.1.1.6.4.1.1" class="ltx_p ltx_align_top"><span id="S4.T1.1.1.6.4.1.1.1" class="ltx_text" style="font-size:90%;">Object Detection</span></p>
</th>
<th id="S4.T1.1.1.6.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S4.T1.1.1.6.4.2.1" class="ltx_text" style="font-size:90%;">DETR </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.1.1.6.4.2.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib13" title="" class="ltx_ref">13</a><span id="S4.T1.1.1.6.4.2.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T1.1.1.6.4.3" class="ltx_td ltx_align_justify ltx_border_t" style="width:170.7pt;padding-left:5.0pt;padding-right:5.0pt;">
<p id="S4.T1.1.1.6.4.3.1" class="ltx_p ltx_align_top"><span id="S4.T1.1.1.6.4.3.1.1" class="ltx_text" style="font-size:90%;">Linear projection layer to reduce CNN feature dimension, Spatial positional embedding added to each multi-head self-attention layer of both encoder and decoder. Object queries (output positional encoding) added to each multi-head self-attention layer of decoder.</span></p>
</td>
<td id="S4.T1.1.1.6.4.4" class="ltx_td ltx_align_justify ltx_border_t" style="width:85.4pt;padding-left:5.0pt;padding-right:5.0pt;">
<p id="S4.T1.1.1.6.4.4.1" class="ltx_p ltx_align_top"><span id="S4.T1.1.1.6.4.4.1.1" class="ltx_text" style="font-size:90%;">2D Image</span></p>
</td>
<td id="S4.T1.1.1.6.4.5" class="ltx_td ltx_align_justify ltx_border_t" style="width:56.9pt;padding-left:5.0pt;padding-right:5.0pt;">
<p id="S4.T1.1.1.6.4.5.1" class="ltx_p ltx_align_top"><span id="S4.T1.1.1.6.4.5.1.1" class="ltx_text" style="font-size:90%;">Class labels</span></p>
</td>
<td id="S4.T1.1.1.6.4.6" class="ltx_td ltx_align_justify ltx_border_t" style="width:85.4pt;padding-left:5.0pt;padding-right:5.0pt;">
<p id="S4.T1.1.1.6.4.6.1" class="ltx_p ltx_align_top"><span id="S4.T1.1.1.6.4.6.1.1" class="ltx_text" style="font-size:90%;">Hungarian loss based on bipartite matching between predicted and ground truths</span></p>
</td>
</tr>
<tr id="S4.T1.1.1.7.5" class="ltx_tr">
<th id="S4.T1.1.1.7.5.1" class="ltx_td ltx_th ltx_th_row" style="width:64.0pt;padding-left:5.0pt;padding-right:5.0pt;"></th>
<th id="S4.T1.1.1.7.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S4.T1.1.1.7.5.2.1" class="ltx_text" style="font-size:90%;">D-DETR </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.1.1.7.5.2.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib14" title="" class="ltx_ref">14</a><span id="S4.T1.1.1.7.5.2.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T1.1.1.7.5.3" class="ltx_td ltx_align_justify ltx_border_t" style="width:170.7pt;padding-left:5.0pt;padding-right:5.0pt;">
<p id="S4.T1.1.1.7.5.3.1" class="ltx_p ltx_align_top"><span id="S4.T1.1.1.7.5.3.1.1" class="ltx_text" style="font-size:90%;">Deformable Transformer consists of deformable attention layers to introduce sparse priors in Transformers, Multi-scale attention module.</span></p>
</td>
<td id="S4.T1.1.1.7.5.4" class="ltx_td ltx_align_justify ltx_border_t" style="width:85.4pt;padding-left:5.0pt;padding-right:5.0pt;">
<p id="S4.T1.1.1.7.5.4.1" class="ltx_p ltx_align_top"><span id="S4.T1.1.1.7.5.4.1.1" class="ltx_text" style="font-size:90%;">2D Image</span></p>
</td>
<td id="S4.T1.1.1.7.5.5" class="ltx_td ltx_align_justify ltx_border_t" style="width:56.9pt;padding-left:5.0pt;padding-right:5.0pt;">
<p id="S4.T1.1.1.7.5.5.1" class="ltx_p ltx_align_top"><span id="S4.T1.1.1.7.5.5.1.1" class="ltx_text" style="font-size:90%;">Class labels</span></p>
</td>
<td id="S4.T1.1.1.7.5.6" class="ltx_td ltx_align_justify ltx_border_t" style="width:85.4pt;padding-left:5.0pt;padding-right:5.0pt;">
<p id="S4.T1.1.1.7.5.6.1" class="ltx_p ltx_align_top"><span id="S4.T1.1.1.7.5.6.1.1" class="ltx_text" style="font-size:90%;">Hungarian loss</span></p>
</td>
</tr>
<tr id="S4.T1.1.1.8.6" class="ltx_tr">
<th id="S4.T1.1.1.8.6.1" class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_t" style="width:64.0pt;padding-left:5.0pt;padding-right:5.0pt;">
<p id="S4.T1.1.1.8.6.1.1" class="ltx_p ltx_align_top"><span id="S4.T1.1.1.8.6.1.1.1" class="ltx_text" style="font-size:90%;">Low Shot Learning</span></p>
</th>
<th id="S4.T1.1.1.8.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S4.T1.1.1.8.6.2.1" class="ltx_text" style="font-size:90%;">CT </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.1.1.8.6.2.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib25" title="" class="ltx_ref">25</a><span id="S4.T1.1.1.8.6.2.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T1.1.1.8.6.3" class="ltx_td ltx_align_justify ltx_border_t" style="width:170.7pt;padding-left:5.0pt;padding-right:5.0pt;">
<p id="S4.T1.1.1.8.6.3.1" class="ltx_p ltx_align_top"><span id="S4.T1.1.1.8.6.3.1.1" class="ltx_text" style="font-size:90%;">Self-supervised pretraining, Query-aligned class prototypes that provide spatial correspondence between the support-set images and query image.</span></p>
</td>
<td id="S4.T1.1.1.8.6.4" class="ltx_td ltx_align_justify ltx_border_t" style="width:85.4pt;padding-left:5.0pt;padding-right:5.0pt;">
<p id="S4.T1.1.1.8.6.4.1" class="ltx_p ltx_align_top"><span id="S4.T1.1.1.8.6.4.1.1" class="ltx_text" style="font-size:90%;">2D Image</span></p>
</td>
<td id="S4.T1.1.1.8.6.5" class="ltx_td ltx_align_justify ltx_border_t" style="width:56.9pt;padding-left:5.0pt;padding-right:5.0pt;">
<p id="S4.T1.1.1.8.6.5.1" class="ltx_p ltx_align_top"><span id="S4.T1.1.1.8.6.5.1.1" class="ltx_text" style="font-size:90%;">Pretraining without labels and few-shot learning with Class labels</span></p>
</td>
<td id="S4.T1.1.1.8.6.6" class="ltx_td ltx_align_justify ltx_border_t" style="width:85.4pt;padding-left:5.0pt;padding-right:5.0pt;">
<p id="S4.T1.1.1.8.6.6.1" class="ltx_p ltx_align_top"><span id="S4.T1.1.1.8.6.6.1.1" class="ltx_text" style="font-size:90%;">Normalized Cross-entropy</span></p>
</td>
</tr>
<tr id="S4.T1.1.1.9.7" class="ltx_tr">
<th id="S4.T1.1.1.9.7.1" class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_t" style="width:64.0pt;padding-left:5.0pt;padding-right:5.0pt;">
<p id="S4.T1.1.1.9.7.1.1" class="ltx_p ltx_align_top"><span id="S4.T1.1.1.9.7.1.1.1" class="ltx_text" style="font-size:90%;">Image Colorization</span></p>
</th>
<th id="S4.T1.1.1.9.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S4.T1.1.1.9.7.2.1" class="ltx_text" style="font-size:90%;">ColTran </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.1.1.9.7.2.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib24" title="" class="ltx_ref">24</a><span id="S4.T1.1.1.9.7.2.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T1.1.1.9.7.3" class="ltx_td ltx_align_justify ltx_border_t" style="width:170.7pt;padding-left:5.0pt;padding-right:5.0pt;">
<p id="S4.T1.1.1.9.7.3.1" class="ltx_p ltx_align_top"><span id="S4.T1.1.1.9.7.3.1.1" class="ltx_text" style="font-size:90%;">Conditional Row/column multi-head attention layers, Progressive multi-scale colorization scheme.</span></p>
</td>
<td id="S4.T1.1.1.9.7.4" class="ltx_td ltx_align_justify ltx_border_t" style="width:85.4pt;padding-left:5.0pt;padding-right:5.0pt;">
<p id="S4.T1.1.1.9.7.4.1" class="ltx_p ltx_align_top"><span id="S4.T1.1.1.9.7.4.1.1" class="ltx_text" style="font-size:90%;">2D Image</span></p>
</td>
<td id="S4.T1.1.1.9.7.5" class="ltx_td ltx_align_justify ltx_border_t" style="width:56.9pt;padding-left:5.0pt;padding-right:5.0pt;">
<p id="S4.T1.1.1.9.7.5.1" class="ltx_p ltx_align_top"><span id="S4.T1.1.1.9.7.5.1.1" class="ltx_text" style="font-size:90%;">2D Image</span></p>
</td>
<td id="S4.T1.1.1.9.7.6" class="ltx_td ltx_align_justify ltx_border_t" style="width:85.4pt;padding-left:5.0pt;padding-right:5.0pt;">
<p id="S4.T1.1.1.9.7.6.1" class="ltx_p ltx_align_top"><span id="S4.T1.1.1.9.7.6.1.1" class="ltx_text" style="font-size:90%;">Negative log-likelihood of the images</span></p>
</td>
</tr>
<tr id="S4.T1.1.1.10.8" class="ltx_tr">
<th id="S4.T1.1.1.10.8.1" class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_t" style="width:64.0pt;padding-left:5.0pt;padding-right:5.0pt;">
<p id="S4.T1.1.1.10.8.1.1" class="ltx_p ltx_align_top"><span id="S4.T1.1.1.10.8.1.1.1" class="ltx_text" style="font-size:90%;">Action Recognition</span></p>
</th>
<th id="S4.T1.1.1.10.8.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S4.T1.1.1.10.8.2.1" class="ltx_text" style="font-size:90%;">ST-TR </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.1.1.10.8.2.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib216" title="" class="ltx_ref">216</a><span id="S4.T1.1.1.10.8.2.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T1.1.1.10.8.3" class="ltx_td ltx_align_justify ltx_border_t" style="width:170.7pt;padding-left:5.0pt;padding-right:5.0pt;">
<p id="S4.T1.1.1.10.8.3.1" class="ltx_p ltx_align_top"><span id="S4.T1.1.1.10.8.3.1.1" class="ltx_text" style="font-size:90%;">Spatial and Temporal self-attention to operates on graph data such as joints in skeletons.</span></p>
</td>
<td id="S4.T1.1.1.10.8.4" class="ltx_td ltx_align_justify ltx_border_t" style="width:85.4pt;padding-left:5.0pt;padding-right:5.0pt;">
<p id="S4.T1.1.1.10.8.4.1" class="ltx_p ltx_align_top"><span id="S4.T1.1.1.10.8.4.1.1" class="ltx_text" style="font-size:90%;">Skeleton</span></p>
</td>
<td id="S4.T1.1.1.10.8.5" class="ltx_td ltx_align_justify ltx_border_t" style="width:56.9pt;padding-left:5.0pt;padding-right:5.0pt;">
<p id="S4.T1.1.1.10.8.5.1" class="ltx_p ltx_align_top"><span id="S4.T1.1.1.10.8.5.1.1" class="ltx_text" style="font-size:90%;">Action Classes</span></p>
</td>
<td id="S4.T1.1.1.10.8.6" class="ltx_td ltx_align_justify ltx_border_t" style="width:85.4pt;padding-left:5.0pt;padding-right:5.0pt;">
<p id="S4.T1.1.1.10.8.6.1" class="ltx_p ltx_align_top"><span id="S4.T1.1.1.10.8.6.1.1" class="ltx_text" style="font-size:90%;">Cross-entropy</span></p>
</td>
</tr>
<tr id="S4.T1.1.1.11.9" class="ltx_tr">
<th id="S4.T1.1.1.11.9.1" class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_t" style="width:64.0pt;padding-left:5.0pt;padding-right:5.0pt;">
<p id="S4.T1.1.1.11.9.1.1" class="ltx_p ltx_align_top"><span id="S4.T1.1.1.11.9.1.1.1" class="ltx_text" style="font-size:90%;">Super-resolution</span></p>
</th>
<th id="S4.T1.1.1.11.9.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S4.T1.1.1.11.9.2.1" class="ltx_text" style="font-size:90%;">TTSR </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.1.1.11.9.2.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib16" title="" class="ltx_ref">16</a><span id="S4.T1.1.1.11.9.2.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T1.1.1.11.9.3" class="ltx_td ltx_align_justify ltx_border_t" style="width:170.7pt;padding-left:5.0pt;padding-right:5.0pt;">
<p id="S4.T1.1.1.11.9.3.1" class="ltx_p ltx_align_top"><span id="S4.T1.1.1.11.9.3.1.1" class="ltx_text" style="font-size:90%;">Texture enhancing Transformer module, Relevance embeddings to compute the relevance between the low-resolution and reference image.</span></p>
</td>
<td id="S4.T1.1.1.11.9.4" class="ltx_td ltx_align_justify ltx_border_t" style="width:85.4pt;padding-left:5.0pt;padding-right:5.0pt;">
<p id="S4.T1.1.1.11.9.4.1" class="ltx_p ltx_align_top"><span id="S4.T1.1.1.11.9.4.1.1" class="ltx_text" style="font-size:90%;">2D Image</span></p>
</td>
<td id="S4.T1.1.1.11.9.5" class="ltx_td ltx_align_justify ltx_border_t" style="width:56.9pt;padding-left:5.0pt;padding-right:5.0pt;">
<p id="S4.T1.1.1.11.9.5.1" class="ltx_p ltx_align_top"><span id="S4.T1.1.1.11.9.5.1.1" class="ltx_text" style="font-size:90%;">2D Image</span></p>
</td>
<td id="S4.T1.1.1.11.9.6" class="ltx_td ltx_align_justify ltx_border_t" style="width:85.4pt;padding-left:5.0pt;padding-right:5.0pt;">
<p id="S4.T1.1.1.11.9.6.1" class="ltx_p ltx_align_top"><span id="S4.T1.1.1.11.9.6.1.1" class="ltx_text" style="font-size:90%;">Reconstruction loss, Perceptual loss defined on pretrained VGG19 features.</span></p>
</td>
</tr>
<tr id="S4.T1.1.1.12.10" class="ltx_tr">
<th id="S4.T1.1.1.12.10.1" class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_t" style="width:64.0pt;padding-left:5.0pt;padding-right:5.0pt;">
<p id="S4.T1.1.1.12.10.1.1" class="ltx_p ltx_align_top"><span id="S4.T1.1.1.12.10.1.1.1" class="ltx_text" style="font-size:90%;">Multi-Model Learning</span></p>
</th>
<th id="S4.T1.1.1.12.10.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S4.T1.1.1.12.10.2.1" class="ltx_text" style="font-size:90%;">Oscar </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.1.1.12.10.2.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib44" title="" class="ltx_ref">44</a><span id="S4.T1.1.1.12.10.2.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T1.1.1.12.10.3" class="ltx_td ltx_align_justify ltx_border_t" style="width:170.7pt;padding-left:5.0pt;padding-right:5.0pt;">
<p id="S4.T1.1.1.12.10.3.1" class="ltx_p ltx_align_top"><span id="S4.T1.1.1.12.10.3.1.1" class="ltx_text" style="font-size:90%;">Transformer layer to jointly process triplet representation of image-text [words, tags, features], Masked tokens to represent text data.</span></p>
</td>
<td id="S4.T1.1.1.12.10.4" class="ltx_td ltx_align_justify ltx_border_t" style="width:85.4pt;padding-left:5.0pt;padding-right:5.0pt;">
<p id="S4.T1.1.1.12.10.4.1" class="ltx_p ltx_align_top"><span id="S4.T1.1.1.12.10.4.1.1" class="ltx_text" style="font-size:90%;">2D Image</span></p>
</td>
<td id="S4.T1.1.1.12.10.5" class="ltx_td ltx_align_justify ltx_border_t" style="width:56.9pt;padding-left:5.0pt;padding-right:5.0pt;">
<p id="S4.T1.1.1.12.10.5.1" class="ltx_p ltx_align_top"><span id="S4.T1.1.1.12.10.5.1.1" class="ltx_text" style="font-size:90%;">Captions, Class labels, Object tags</span></p>
</td>
<td id="S4.T1.1.1.12.10.6" class="ltx_td ltx_align_justify ltx_border_t" style="width:85.4pt;padding-left:5.0pt;padding-right:5.0pt;">
<p id="S4.T1.1.1.12.10.6.1" class="ltx_p ltx_align_top"><span id="S4.T1.1.1.12.10.6.1.1" class="ltx_text" style="font-size:90%;">Negative log-likelihood of masked tokens, Contrastive binary cross-entropy</span></p>
</td>
</tr>
<tr id="S4.T1.1.1.13.11" class="ltx_tr">
<th id="S4.T1.1.1.13.11.1" class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_t" style="width:64.0pt;padding-left:5.0pt;padding-right:5.0pt;">
<p id="S4.T1.1.1.13.11.1.1" class="ltx_p ltx_align_top"><span id="S4.T1.1.1.13.11.1.1.1" class="ltx_text" style="font-size:90%;">3D Classification/Segmentation</span></p>
</th>
<th id="S4.T1.1.1.13.11.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S4.T1.1.1.13.11.2.1" class="ltx_text" style="font-size:90%;">PT </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.1.1.13.11.2.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib230" title="" class="ltx_ref">230</a><span id="S4.T1.1.1.13.11.2.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T1.1.1.13.11.3" class="ltx_td ltx_align_justify ltx_border_t" style="width:170.7pt;padding-left:5.0pt;padding-right:5.0pt;">
<p id="S4.T1.1.1.13.11.3.1" class="ltx_p ltx_align_top"><span id="S4.T1.1.1.13.11.3.1.1" class="ltx_text" style="font-size:90%;">Point Transformer block, Transition down block to reduce cardinality of the point set, Transition up for dense prediction tasks.</span></p>
</td>
<td id="S4.T1.1.1.13.11.4" class="ltx_td ltx_align_justify ltx_border_t" style="width:85.4pt;padding-left:5.0pt;padding-right:5.0pt;">
<p id="S4.T1.1.1.13.11.4.1" class="ltx_p ltx_align_top"><span id="S4.T1.1.1.13.11.4.1.1" class="ltx_text" style="font-size:90%;">CAD models, 3D object part segmentation</span></p>
</td>
<td id="S4.T1.1.1.13.11.5" class="ltx_td ltx_align_justify ltx_border_t" style="width:56.9pt;padding-left:5.0pt;padding-right:5.0pt;">
<p id="S4.T1.1.1.13.11.5.1" class="ltx_p ltx_align_top"><span id="S4.T1.1.1.13.11.5.1.1" class="ltx_text" style="font-size:90%;">Object and shape categories</span></p>
</td>
<td id="S4.T1.1.1.13.11.6" class="ltx_td ltx_align_justify ltx_border_t" style="width:85.4pt;padding-left:5.0pt;padding-right:5.0pt;">
<p id="S4.T1.1.1.13.11.6.1" class="ltx_p ltx_align_top"><span id="S4.T1.1.1.13.11.6.1.1" class="ltx_text" style="font-size:90%;">Cross-entropy</span></p>
</td>
</tr>
<tr id="S4.T1.1.1.1" class="ltx_tr">
<th id="S4.T1.1.1.1.2" class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_t" style="width:64.0pt;padding-left:5.0pt;padding-right:5.0pt;">
<p id="S4.T1.1.1.1.2.1" class="ltx_p ltx_align_top"><span id="S4.T1.1.1.1.2.1.1" class="ltx_text" style="font-size:90%;">3D Mesh Reconstruction</span></p>
</th>
<th id="S4.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S4.T1.1.1.1.3.1" class="ltx_text" style="font-size:90%;">METRO </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.1.1.1.3.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib45" title="" class="ltx_ref">45</a><span id="S4.T1.1.1.1.3.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T1.1.1.1.4" class="ltx_td ltx_align_justify ltx_border_t" style="width:170.7pt;padding-left:5.0pt;padding-right:5.0pt;">
<p id="S4.T1.1.1.1.4.1" class="ltx_p ltx_align_top"><span id="S4.T1.1.1.1.4.1.1" class="ltx_text" style="font-size:90%;">Progressive dimensionality reduction across Transformer layers, Positional Encoding with 3D joint and 3D vertex coordinates, Masked vertex/joint modeling.</span></p>
</td>
<td id="S4.T1.1.1.1.5" class="ltx_td ltx_align_justify ltx_border_t" style="width:85.4pt;padding-left:5.0pt;padding-right:5.0pt;">
<p id="S4.T1.1.1.1.5.1" class="ltx_p ltx_align_top"><span id="S4.T1.1.1.1.5.1.1" class="ltx_text" style="font-size:90%;">2D Image</span></p>
</td>
<td id="S4.T1.1.1.1.6" class="ltx_td ltx_align_justify ltx_border_t" style="width:56.9pt;padding-left:5.0pt;padding-right:5.0pt;">
<p id="S4.T1.1.1.1.6.1" class="ltx_p ltx_align_top"><span id="S4.T1.1.1.1.6.1.1" class="ltx_text" style="font-size:90%;">3D Mesh + Human Pose</span></p>
</td>
<td id="S4.T1.1.1.1.1" class="ltx_td ltx_align_justify ltx_border_t" style="width:85.4pt;padding-left:5.0pt;padding-right:5.0pt;">
<p id="S4.T1.1.1.1.1.1.1" class="ltx_p ltx_align_top"><math id="S4.T1.1.1.1.1.1.1.m1.1" class="ltx_centering" alttext="L_{1}" display="inline"><semantics id="S4.T1.1.1.1.1.1.1.m1.1a"><msub id="S4.T1.1.1.1.1.1.1.m1.1.1" xref="S4.T1.1.1.1.1.1.1.m1.1.1.cmml"><mi mathsize="90%" id="S4.T1.1.1.1.1.1.1.m1.1.1.2" xref="S4.T1.1.1.1.1.1.1.m1.1.1.2.cmml">L</mi><mn mathsize="90%" id="S4.T1.1.1.1.1.1.1.m1.1.1.3" xref="S4.T1.1.1.1.1.1.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.1.1.1.m1.1b"><apply id="S4.T1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S4.T1.1.1.1.1.1.1.m1.1.1">subscript</csymbol><ci id="S4.T1.1.1.1.1.1.1.m1.1.1.2.cmml" xref="S4.T1.1.1.1.1.1.1.m1.1.1.2">𝐿</ci><cn type="integer" id="S4.T1.1.1.1.1.1.1.m1.1.1.3.cmml" xref="S4.T1.1.1.1.1.1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.1.1.1.m1.1c">L_{1}</annotation><annotation encoding="application/x-llamapun" id="S4.T1.1.1.1.1.1.1.m1.1d">italic_L start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math><span id="S4.T1.1.1.1.1.1.1.1" class="ltx_text" style="font-size:90%;"> loss on mesh vertices and joints in 3D and 2D projection.</span></p>
</td>
</tr>
<tr id="S4.T1.1.1.14.12" class="ltx_tr">
<th id="S4.T1.1.1.14.12.1" class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_t" style="width:64.0pt;padding-left:5.0pt;padding-right:5.0pt;">
<p id="S4.T1.1.1.14.12.1.1" class="ltx_p ltx_align_top"><span id="S4.T1.1.1.14.12.1.1.1" class="ltx_text" style="font-size:90%;">Vision and Language Navigation</span></p>
</th>
<th id="S4.T1.1.1.14.12.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S4.T1.1.1.14.12.2.1" class="ltx_text" style="font-size:90%;">Chen </span><span id="S4.T1.1.1.14.12.2.2" class="ltx_text ltx_font_italic" style="font-size:90%;">et al.<cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.1.1.14.12.2.2.1.1" class="ltx_text ltx_font_upright">[</span><a href="#bib.bib194" title="" class="ltx_ref">194</a><span id="S4.T1.1.1.14.12.2.2.2.2" class="ltx_text ltx_font_upright">]</span></cite></span>
</th>
<td id="S4.T1.1.1.14.12.3" class="ltx_td ltx_align_justify ltx_border_t" style="width:170.7pt;padding-left:5.0pt;padding-right:5.0pt;">
<p id="S4.T1.1.1.14.12.3.1" class="ltx_p ltx_align_top"><span id="S4.T1.1.1.14.12.3.1.1" class="ltx_text" style="font-size:90%;">Uni-modal encoders on language and map inputs followed by a cross-modal transformer, Trajectory position encodings in the map encoder.</span></p>
</td>
<td id="S4.T1.1.1.14.12.4" class="ltx_td ltx_align_justify ltx_border_t" style="width:85.4pt;padding-left:5.0pt;padding-right:5.0pt;">
<p id="S4.T1.1.1.14.12.4.1" class="ltx_p ltx_align_top"><span id="S4.T1.1.1.14.12.4.1.1" class="ltx_text" style="font-size:90%;">Instruction text + RGBD panorama + Topological Environment Map</span></p>
</td>
<td id="S4.T1.1.1.14.12.5" class="ltx_td ltx_align_justify ltx_border_t" style="width:56.9pt;padding-left:5.0pt;padding-right:5.0pt;">
<p id="S4.T1.1.1.14.12.5.1" class="ltx_p ltx_align_top"><span id="S4.T1.1.1.14.12.5.1.1" class="ltx_text" style="font-size:90%;">Navigation Plan</span></p>
</td>
<td id="S4.T1.1.1.14.12.6" class="ltx_td ltx_align_justify ltx_border_t" style="width:85.4pt;padding-left:5.0pt;padding-right:5.0pt;">
<p id="S4.T1.1.1.14.12.6.1" class="ltx_p ltx_align_top"><span id="S4.T1.1.1.14.12.6.1.1" class="ltx_text" style="font-size:90%;">Cross-entropy over nodes and </span><span id="S4.T1.1.1.14.12.6.1.2" class="ltx_text ltx_font_typewriter ltx_align_center" style="font-size:90%;">[stop]</span><span id="S4.T1.1.1.14.12.6.1.3" class="ltx_text" style="font-size:90%;"> action</span></p>
</td>
</tr>
<tr id="S4.T1.1.1.15.13" class="ltx_tr">
<th id="S4.T1.1.1.15.13.1" class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_t" style="width:64.0pt;padding-left:5.0pt;padding-right:5.0pt;">
<p id="S4.T1.1.1.15.13.1.1" class="ltx_p ltx_align_top"><span id="S4.T1.1.1.15.13.1.1.1" class="ltx_text" style="font-size:90%;">Referring Image Segmentation</span></p>
</th>
<th id="S4.T1.1.1.15.13.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S4.T1.1.1.15.13.2.1" class="ltx_text" style="font-size:90%;">CMSA </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.1.1.15.13.2.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib15" title="" class="ltx_ref">15</a><span id="S4.T1.1.1.15.13.2.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T1.1.1.15.13.3" class="ltx_td ltx_align_justify ltx_border_t" style="width:170.7pt;padding-left:5.0pt;padding-right:5.0pt;">
<p id="S4.T1.1.1.15.13.3.1" class="ltx_p ltx_align_top"><span id="S4.T1.1.1.15.13.3.1.1" class="ltx_text" style="font-size:90%;">Multimodal feature, Cross-modal self-attention on multiple levels and their fusion using learned gates.</span></p>
</td>
<td id="S4.T1.1.1.15.13.4" class="ltx_td ltx_align_justify ltx_border_t" style="width:85.4pt;padding-left:5.0pt;padding-right:5.0pt;">
<p id="S4.T1.1.1.15.13.4.1" class="ltx_p ltx_align_top"><span id="S4.T1.1.1.15.13.4.1.1" class="ltx_text" style="font-size:90%;">2D Image + Language expression</span></p>
</td>
<td id="S4.T1.1.1.15.13.5" class="ltx_td ltx_align_justify ltx_border_t" style="width:56.9pt;padding-left:5.0pt;padding-right:5.0pt;">
<p id="S4.T1.1.1.15.13.5.1" class="ltx_p ltx_align_top"><span id="S4.T1.1.1.15.13.5.1.1" class="ltx_text" style="font-size:90%;">Segmentation mask</span></p>
</td>
<td id="S4.T1.1.1.15.13.6" class="ltx_td ltx_align_justify ltx_border_t" style="width:85.4pt;padding-left:5.0pt;padding-right:5.0pt;">
<p id="S4.T1.1.1.15.13.6.1" class="ltx_p ltx_align_top"><span id="S4.T1.1.1.15.13.6.1.1" class="ltx_text" style="font-size:90%;">Binary cross-entropy loss</span></p>
</td>
</tr>
<tr id="S4.T1.1.1.16.14" class="ltx_tr">
<th id="S4.T1.1.1.16.14.1" class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_bb ltx_border_t" style="width:64.0pt;padding-left:5.0pt;padding-right:5.0pt;">
<p id="S4.T1.1.1.16.14.1.1" class="ltx_p ltx_align_top"><span id="S4.T1.1.1.16.14.1.1.1" class="ltx_text" style="font-size:90%;">Video Classification</span></p>
</th>
<th id="S4.T1.1.1.16.14.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S4.T1.1.1.16.14.2.1" class="ltx_text" style="font-size:90%;">Lee </span><span id="S4.T1.1.1.16.14.2.2" class="ltx_text ltx_font_italic" style="font-size:90%;">et al.<cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.1.1.16.14.2.2.1.1" class="ltx_text ltx_font_upright">[</span><a href="#bib.bib182" title="" class="ltx_ref">182</a><span id="S4.T1.1.1.16.14.2.2.2.2" class="ltx_text ltx_font_upright">]</span></cite></span>
</th>
<td id="S4.T1.1.1.16.14.3" class="ltx_td ltx_align_justify ltx_border_bb ltx_border_t" style="width:170.7pt;padding-left:5.0pt;padding-right:5.0pt;">
<p id="S4.T1.1.1.16.14.3.1" class="ltx_p ltx_align_top"><span id="S4.T1.1.1.16.14.3.1.1" class="ltx_text" style="font-size:90%;">Operates on real-valued audio-visual signals instead of tokens, Contrastive learning for pre-training, End-to-end multimodal transformer learning.</span></p>
</td>
<td id="S4.T1.1.1.16.14.4" class="ltx_td ltx_align_justify ltx_border_bb ltx_border_t" style="width:85.4pt;padding-left:5.0pt;padding-right:5.0pt;">
<p id="S4.T1.1.1.16.14.4.1" class="ltx_p ltx_align_top"><span id="S4.T1.1.1.16.14.4.1.1" class="ltx_text" style="font-size:90%;">Audio-Visual</span></p>
</td>
<td id="S4.T1.1.1.16.14.5" class="ltx_td ltx_align_justify ltx_border_bb ltx_border_t" style="width:56.9pt;padding-left:5.0pt;padding-right:5.0pt;">
<p id="S4.T1.1.1.16.14.5.1" class="ltx_p ltx_align_top"><span id="S4.T1.1.1.16.14.5.1.1" class="ltx_text" style="font-size:90%;">Activity labels</span></p>
</td>
<td id="S4.T1.1.1.16.14.6" class="ltx_td ltx_align_justify ltx_border_bb ltx_border_t" style="width:85.4pt;padding-left:5.0pt;padding-right:5.0pt;">
<p id="S4.T1.1.1.16.14.6.1" class="ltx_p ltx_align_top"><span id="S4.T1.1.1.16.14.6.1.1" class="ltx_text" style="font-size:90%;">Contrastive InfoNCE loss and Binary cross-entropy</span></p>
</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">TABLE I: </span>A summary of key design choices adopted in different variants of transformers for a representative set of computer vision applications. The main changes relate to specific loss function choices, architectural modifications, different position embeddings and variations in input data modalities. </figcaption>
</figure>
<figure id="S4.T2" class="ltx_table">
<div id="S4.T2.5" class="ltx_inline-block ltx_transformed_outer" style="width:935.6pt;height:377.8pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-155.9pt,62.8pt) scale(0.75,0.75) ;">
<table id="S4.T2.5.5" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.5.5.6.1" class="ltx_tr" style="background-color:#E6E6E6;">
<th id="S4.T2.5.5.6.1.1" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt" style="width:42.7pt;padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T2.5.5.6.1.1.1" class="ltx_text ltx_font_bold ltx_align_center ltx_align_top" style="font-size:90%;background-color:#E6E6E6;">Task</span></th>
<th id="S4.T2.5.5.6.1.2" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt" style="width:56.9pt;padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T2.5.5.6.1.2.1" class="ltx_text ltx_font_bold ltx_align_top" style="font-size:90%;background-color:#E6E6E6;">Method</span></th>
<th id="S4.T2.5.5.6.1.3" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt" style="width:42.7pt;padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T2.5.5.6.1.3.1" class="ltx_text ltx_font_bold ltx_align_top" style="font-size:90%;background-color:#E6E6E6;">Metric</span></th>
<th id="S4.T2.5.5.6.1.4" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt" style="width:42.7pt;padding-left:4.0pt;padding-right:4.0pt;">
<p id="S4.T2.5.5.6.1.4.1" class="ltx_p ltx_align_top" style="background-color:#E6E6E6;"><span id="S4.T2.5.5.6.1.4.1.1" class="ltx_text" style="font-size:90%;"></span><span id="S4.T2.5.5.6.1.4.1.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Dataset</span></p>
</th>
<th id="S4.T2.5.5.6.1.5" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt" style="width:42.7pt;padding-left:4.0pt;padding-right:4.0pt;">
<p id="S4.T2.5.5.6.1.5.1" class="ltx_p ltx_align_top" style="background-color:#E6E6E6;"><span id="S4.T2.5.5.6.1.5.1.1" class="ltx_text" style="font-size:90%;"></span><span id="S4.T2.5.5.6.1.5.1.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Performance</span></p>
</th>
<th id="S4.T2.5.5.6.1.6" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt" style="width:142.3pt;padding-left:4.0pt;padding-right:4.0pt;">
<p id="S4.T2.5.5.6.1.6.1" class="ltx_p ltx_align_top" style="background-color:#E6E6E6;"><span id="S4.T2.5.5.6.1.6.1.1" class="ltx_text" style="font-size:90%;">             </span><span id="S4.T2.5.5.6.1.6.1.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Highlights</span></p>
</th>
<th id="S4.T2.5.5.6.1.7" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt" style="width:142.3pt;padding-left:4.0pt;padding-right:4.0pt;">
<p id="S4.T2.5.5.6.1.7.1" class="ltx_p ltx_align_top" style="background-color:#E6E6E6;"><span id="S4.T2.5.5.6.1.7.1.1" class="ltx_text" style="font-size:90%;">             </span><span id="S4.T2.5.5.6.1.7.1.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Limitations</span></p>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.5.5.7.1" class="ltx_tr">
<td id="S4.T2.5.5.7.1.1" class="ltx_td ltx_align_justify ltx_border_t" style="width:42.7pt;padding-left:4.0pt;padding-right:4.0pt;">
<p id="S4.T2.5.5.7.1.1.1" class="ltx_p ltx_align_top"><span id="S4.T2.5.5.7.1.1.1.1" class="ltx_text" style="font-size:90%;">Image Classification</span></p>
</td>
<td id="S4.T2.5.5.7.1.2" class="ltx_td ltx_align_justify ltx_border_t" style="width:56.9pt;padding-left:4.0pt;padding-right:4.0pt;">
<span id="S4.T2.5.5.7.1.2.1" class="ltx_inline-block ltx_parbox ltx_align_top" style="width:54.1pt;">
<span id="S4.T2.5.5.7.1.2.1.1" class="ltx_p"><span id="S4.T2.5.5.7.1.2.1.1.1" class="ltx_text" style="font-size:90%;">ViT </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.5.5.7.1.2.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib11" title="" class="ltx_ref">11</a><span id="S4.T2.5.5.7.1.2.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.T2.5.5.7.1.2.1.1.4" class="ltx_text" style="font-size:90%;"></span></span>
<span id="S4.T2.5.5.7.1.2.1.2" class="ltx_p"><span id="S4.T2.5.5.7.1.2.1.2.1" class="ltx_text" style="font-size:90%;">ICLR’21</span></span>
</span>
</td>
<td id="S4.T2.5.5.7.1.3" class="ltx_td ltx_align_justify ltx_border_t" style="width:42.7pt;padding-left:4.0pt;padding-right:4.0pt;">
<p id="S4.T2.5.5.7.1.3.1" class="ltx_p ltx_align_top"><span id="S4.T2.5.5.7.1.3.1.1" class="ltx_text" style="font-size:90%;">Top-1 Acc.</span></p>
</td>
<td id="S4.T2.5.5.7.1.4" class="ltx_td ltx_align_justify ltx_border_t" style="width:42.7pt;padding-left:4.0pt;padding-right:4.0pt;">
<p id="S4.T2.5.5.7.1.4.1" class="ltx_p ltx_align_top"><span id="S4.T2.5.5.7.1.4.1.1" class="ltx_text" style="font-size:90%;">ImageNet</span></p>
</td>
<td id="S4.T2.5.5.7.1.5" class="ltx_td ltx_align_justify ltx_border_t" style="width:42.7pt;padding-left:4.0pt;padding-right:4.0pt;">
<p id="S4.T2.5.5.7.1.5.1" class="ltx_p ltx_align_top"><span id="S4.T2.5.5.7.1.5.1.1" class="ltx_text" style="font-size:90%;">88.55</span></p>
</td>
<td id="S4.T2.5.5.7.1.6" class="ltx_td ltx_align_justify ltx_border_t" style="width:142.3pt;padding-left:4.0pt;padding-right:4.0pt;">
<p id="S4.T2.5.5.7.1.6.1" class="ltx_p ltx_align_top"><span id="S4.T2.5.5.7.1.6.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">a)</span><span id="S4.T2.5.5.7.1.6.1.2" class="ltx_text" style="font-size:90%;"> First application of Transformer (global self-attention) directly on image patches, </span><span id="S4.T2.5.5.7.1.6.1.3" class="ltx_text ltx_font_bold" style="font-size:90%;">b)</span><span id="S4.T2.5.5.7.1.6.1.4" class="ltx_text" style="font-size:90%;"> Convolution-free network architecture, </span><span id="S4.T2.5.5.7.1.6.1.5" class="ltx_text ltx_font_bold" style="font-size:90%;">c)</span><span id="S4.T2.5.5.7.1.6.1.6" class="ltx_text" style="font-size:90%;"> Outperforms CNN models such as ResNet.</span></p>
</td>
<td id="S4.T2.5.5.7.1.7" class="ltx_td ltx_align_justify ltx_border_t" style="width:142.3pt;padding-left:4.0pt;padding-right:4.0pt;">
<p id="S4.T2.5.5.7.1.7.1" class="ltx_p ltx_align_top"><span id="S4.T2.5.5.7.1.7.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">a)</span><span id="S4.T2.5.5.7.1.7.1.2" class="ltx_text" style="font-size:90%;"> Requires training on large-scale data </span><em id="S4.T2.5.5.7.1.7.1.3" class="ltx_emph ltx_font_italic" style="font-size:90%;">e.g.</em><span id="S4.T2.5.5.7.1.7.1.4" class="ltx_text" style="font-size:90%;">, 300-Million images, </span><span id="S4.T2.5.5.7.1.7.1.5" class="ltx_text ltx_font_bold" style="font-size:90%;">b)</span><span id="S4.T2.5.5.7.1.7.1.6" class="ltx_text" style="font-size:90%;"> Requires careful transfer learning to the new task, </span><span id="S4.T2.5.5.7.1.7.1.7" class="ltx_text ltx_font_bold" style="font-size:90%;">c)</span><span id="S4.T2.5.5.7.1.7.1.8" class="ltx_text" style="font-size:90%;"> Requires large model with 632-Million parameters to achieve SOTA results.</span></p>
</td>
</tr>
<tr id="S4.T2.5.5.8.2" class="ltx_tr">
<td id="S4.T2.5.5.8.2.1" class="ltx_td" style="width:42.7pt;padding-left:4.0pt;padding-right:4.0pt;"></td>
<td id="S4.T2.5.5.8.2.2" class="ltx_td ltx_align_justify ltx_border_t" style="width:56.9pt;padding-left:4.0pt;padding-right:4.0pt;">
<span id="S4.T2.5.5.8.2.2.1" class="ltx_inline-block ltx_parbox ltx_align_top" style="width:54.1pt;">
<span id="S4.T2.5.5.8.2.2.1.1" class="ltx_p"><span id="S4.T2.5.5.8.2.2.1.1.1" class="ltx_text" style="font-size:90%;">DeiT </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.5.5.8.2.2.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib12" title="" class="ltx_ref">12</a><span id="S4.T2.5.5.8.2.2.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.T2.5.5.8.2.2.1.1.4" class="ltx_text" style="font-size:90%;"></span></span>
<span id="S4.T2.5.5.8.2.2.1.2" class="ltx_p"><span id="S4.T2.5.5.8.2.2.1.2.1" class="ltx_text" style="font-size:90%;">arXiv’20</span></span>
</span>
</td>
<td id="S4.T2.5.5.8.2.3" class="ltx_td ltx_align_justify ltx_border_t" style="width:42.7pt;padding-left:4.0pt;padding-right:4.0pt;">
<p id="S4.T2.5.5.8.2.3.1" class="ltx_p ltx_align_top"><span id="S4.T2.5.5.8.2.3.1.1" class="ltx_text" style="font-size:90%;">Top-1 Acc.</span></p>
</td>
<td id="S4.T2.5.5.8.2.4" class="ltx_td ltx_align_justify ltx_border_t" style="width:42.7pt;padding-left:4.0pt;padding-right:4.0pt;">
<p id="S4.T2.5.5.8.2.4.1" class="ltx_p ltx_align_top"><span id="S4.T2.5.5.8.2.4.1.1" class="ltx_text" style="font-size:90%;">ImageNet</span></p>
</td>
<td id="S4.T2.5.5.8.2.5" class="ltx_td ltx_align_justify ltx_border_t" style="width:42.7pt;padding-left:4.0pt;padding-right:4.0pt;">
<p id="S4.T2.5.5.8.2.5.1" class="ltx_p ltx_align_top"><span id="S4.T2.5.5.8.2.5.1.1" class="ltx_text" style="font-size:90%;">83.10</span></p>
</td>
<td id="S4.T2.5.5.8.2.6" class="ltx_td ltx_align_justify ltx_border_t" style="width:142.3pt;padding-left:4.0pt;padding-right:4.0pt;">
<p id="S4.T2.5.5.8.2.6.1" class="ltx_p ltx_align_top"><span id="S4.T2.5.5.8.2.6.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">a)</span><span id="S4.T2.5.5.8.2.6.1.2" class="ltx_text" style="font-size:90%;"> Successfully trains Transformer on ImageNet only, </span><span id="S4.T2.5.5.8.2.6.1.3" class="ltx_text ltx_font_bold" style="font-size:90%;">b)</span><span id="S4.T2.5.5.8.2.6.1.4" class="ltx_text" style="font-size:90%;"> Introduces attention-based distillation method. </span><span id="S4.T2.5.5.8.2.6.1.5" class="ltx_text ltx_font_bold" style="font-size:90%;">c)</span><span id="S4.T2.5.5.8.2.6.1.6" class="ltx_text" style="font-size:90%;"> Produces competitive performance with small (86-Million parameters) Transformers.</span></p>
</td>
<td id="S4.T2.5.5.8.2.7" class="ltx_td ltx_align_justify ltx_border_t" style="width:142.3pt;padding-left:4.0pt;padding-right:4.0pt;">
<p id="S4.T2.5.5.8.2.7.1" class="ltx_p ltx_align_top"><span id="S4.T2.5.5.8.2.7.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">a)</span><span id="S4.T2.5.5.8.2.7.1.2" class="ltx_text" style="font-size:90%;"> Requires access to pretrained CNN based teacher model thus performance depends on the quality of the teacher model.</span></p>
</td>
</tr>
<tr id="S4.T2.5.5.9.3" class="ltx_tr">
<td id="S4.T2.5.5.9.3.1" class="ltx_td ltx_border_t" style="width:42.7pt;padding-left:4.0pt;padding-right:4.0pt;"></td>
<td id="S4.T2.5.5.9.3.2" class="ltx_td ltx_align_justify ltx_border_t" style="width:56.9pt;padding-left:4.0pt;padding-right:4.0pt;">
<span id="S4.T2.5.5.9.3.2.1" class="ltx_inline-block ltx_parbox ltx_align_top" style="width:54.1pt;">
<span id="S4.T2.5.5.9.3.2.1.1" class="ltx_p"><span id="S4.T2.5.5.9.3.2.1.1.1" class="ltx_text" style="font-size:90%;">Swin-T </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.5.5.9.3.2.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib36" title="" class="ltx_ref">36</a><span id="S4.T2.5.5.9.3.2.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.T2.5.5.9.3.2.1.1.4" class="ltx_text" style="font-size:90%;"></span></span>
<span id="S4.T2.5.5.9.3.2.1.2" class="ltx_p"><span id="S4.T2.5.5.9.3.2.1.2.1" class="ltx_text" style="font-size:90%;">arXiv’21</span></span>
</span>
</td>
<td id="S4.T2.5.5.9.3.3" class="ltx_td ltx_align_justify ltx_border_t" style="width:42.7pt;padding-left:4.0pt;padding-right:4.0pt;">
<p id="S4.T2.5.5.9.3.3.1" class="ltx_p ltx_align_top"><span id="S4.T2.5.5.9.3.3.1.1" class="ltx_text" style="font-size:90%;">Top-1 Acc.</span></p>
</td>
<td id="S4.T2.5.5.9.3.4" class="ltx_td ltx_align_justify ltx_border_t" style="width:42.7pt;padding-left:4.0pt;padding-right:4.0pt;">
<p id="S4.T2.5.5.9.3.4.1" class="ltx_p ltx_align_top"><span id="S4.T2.5.5.9.3.4.1.1" class="ltx_text" style="font-size:90%;">ImageNet</span></p>
</td>
<td id="S4.T2.5.5.9.3.5" class="ltx_td ltx_align_justify ltx_border_t" style="width:42.7pt;padding-left:4.0pt;padding-right:4.0pt;">
<p id="S4.T2.5.5.9.3.5.1" class="ltx_p ltx_align_top"><span id="S4.T2.5.5.9.3.5.1.1" class="ltx_text" style="font-size:90%;">84.5</span></p>
</td>
<td id="S4.T2.5.5.9.3.6" class="ltx_td ltx_align_justify ltx_border_t" style="width:142.3pt;padding-left:4.0pt;padding-right:4.0pt;">
<p id="S4.T2.5.5.9.3.6.1" class="ltx_p ltx_align_top"><span id="S4.T2.5.5.9.3.6.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">a)</span><span id="S4.T2.5.5.9.3.6.1.2" class="ltx_text" style="font-size:90%;"> Provides a general purpose backbone for different vision tasks e.g., classification, detection and segmentation </span><span id="S4.T2.5.5.9.3.6.1.3" class="ltx_text ltx_font_bold" style="font-size:90%;">b)</span><span id="S4.T2.5.5.9.3.6.1.4" class="ltx_text" style="font-size:90%;"> A hierarchical design using shifted-windows operation.</span></p>
</td>
<td id="S4.T2.5.5.9.3.7" class="ltx_td ltx_align_justify ltx_border_t" style="width:142.3pt;padding-left:4.0pt;padding-right:4.0pt;">
<p id="S4.T2.5.5.9.3.7.1" class="ltx_p ltx_align_top"><span id="S4.T2.5.5.9.3.7.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">a)</span><span id="S4.T2.5.5.9.3.7.1.2" class="ltx_text" style="font-size:90%;"> Hard to train from scratch on smaller datasets </span><span id="S4.T2.5.5.9.3.7.1.3" class="ltx_text ltx_font_bold" style="font-size:90%;">b)</span><span id="S4.T2.5.5.9.3.7.1.4" class="ltx_text" style="font-size:90%;"> Quadratic compute complexity inherent to the self-attention operation.</span></p>
</td>
</tr>
<tr id="S4.T2.5.5.10.4" class="ltx_tr">
<td id="S4.T2.5.5.10.4.1" class="ltx_td ltx_align_justify ltx_border_t" style="width:42.7pt;padding-left:4.0pt;padding-right:4.0pt;">
<p id="S4.T2.5.5.10.4.1.1" class="ltx_p ltx_align_top"><span id="S4.T2.5.5.10.4.1.1.1" class="ltx_text" style="font-size:90%;">Low-Shot Learning</span></p>
</td>
<td id="S4.T2.5.5.10.4.2" class="ltx_td ltx_align_justify ltx_border_t" style="width:56.9pt;padding-left:4.0pt;padding-right:4.0pt;">
<span id="S4.T2.5.5.10.4.2.1" class="ltx_inline-block ltx_parbox ltx_align_top" style="width:54.1pt;">
<span id="S4.T2.5.5.10.4.2.1.1" class="ltx_p"><span id="S4.T2.5.5.10.4.2.1.1.1" class="ltx_text" style="font-size:90%;">CT </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.5.5.10.4.2.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib25" title="" class="ltx_ref">25</a><span id="S4.T2.5.5.10.4.2.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.T2.5.5.10.4.2.1.1.4" class="ltx_text" style="font-size:90%;"></span></span>
<span id="S4.T2.5.5.10.4.2.1.2" class="ltx_p"><span id="S4.T2.5.5.10.4.2.1.2.1" class="ltx_text" style="font-size:90%;">NeurIPS’20</span></span>
</span>
</td>
<td id="S4.T2.5.5.10.4.3" class="ltx_td ltx_align_justify ltx_border_t" style="width:42.7pt;padding-left:4.0pt;padding-right:4.0pt;">
<p id="S4.T2.5.5.10.4.3.1" class="ltx_p ltx_align_top"><span id="S4.T2.5.5.10.4.3.1.1" class="ltx_text" style="font-size:90%;">Top-1 Acc.</span></p>
</td>
<td id="S4.T2.5.5.10.4.4" class="ltx_td ltx_align_justify ltx_border_t" style="width:42.7pt;padding-left:4.0pt;padding-right:4.0pt;">
<span id="S4.T2.5.5.10.4.4.1" class="ltx_inline-block ltx_parbox ltx_align_top" style="width:54.1pt;">
<span id="S4.T2.5.5.10.4.4.1.1" class="ltx_p"><span id="S4.T2.5.5.10.4.4.1.1.1" class="ltx_text" style="font-size:90%;">ImageNet</span></span>
<span id="S4.T2.5.5.10.4.4.1.2" class="ltx_p"><span id="S4.T2.5.5.10.4.4.1.2.1" class="ltx_text" style="font-size:90%;">COCO</span></span>
</span>
</td>
<td id="S4.T2.5.5.10.4.5" class="ltx_td ltx_align_justify ltx_border_t" style="width:42.7pt;padding-left:4.0pt;padding-right:4.0pt;">
<span id="S4.T2.5.5.10.4.5.1" class="ltx_inline-block ltx_parbox ltx_align_top" style="width:54.1pt;">
<span id="S4.T2.5.5.10.4.5.1.1" class="ltx_p"><span id="S4.T2.5.5.10.4.5.1.1.1" class="ltx_text" style="font-size:90%;">62.25</span></span>
<span id="S4.T2.5.5.10.4.5.1.2" class="ltx_p"><span id="S4.T2.5.5.10.4.5.1.2.1" class="ltx_text" style="font-size:90%;">60.35</span></span>
</span>
</td>
<td id="S4.T2.5.5.10.4.6" class="ltx_td ltx_align_justify ltx_border_t" style="width:142.3pt;padding-left:4.0pt;padding-right:4.0pt;">
<p id="S4.T2.5.5.10.4.6.1" class="ltx_p ltx_align_top"><span id="S4.T2.5.5.10.4.6.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">a)</span><span id="S4.T2.5.5.10.4.6.1.2" class="ltx_text" style="font-size:90%;"> Self-supervised pre-training mechanism that does not need manual labels, </span><span id="S4.T2.5.5.10.4.6.1.3" class="ltx_text ltx_font_bold" style="font-size:90%;">b)</span><span id="S4.T2.5.5.10.4.6.1.4" class="ltx_text" style="font-size:90%;"> Dynamic inference using Transformer achieving stat-of-the-art results.</span></p>
</td>
<td id="S4.T2.5.5.10.4.7" class="ltx_td ltx_align_justify ltx_border_t" style="width:142.3pt;padding-left:4.0pt;padding-right:4.0pt;">
<p id="S4.T2.5.5.10.4.7.1" class="ltx_p ltx_align_top"><span id="S4.T2.5.5.10.4.7.1.1" class="ltx_text" style="font-size:90%;">Proposed algorithm is limited in its capacity to perform on datasets that lack spatial details such as texture.</span></p>
</td>
</tr>
<tr id="S4.T2.5.5.11.5" class="ltx_tr">
<td id="S4.T2.5.5.11.5.1" class="ltx_td ltx_align_justify ltx_border_t" style="width:42.7pt;padding-left:4.0pt;padding-right:4.0pt;">
<p id="S4.T2.5.5.11.5.1.1" class="ltx_p ltx_align_top"><span id="S4.T2.5.5.11.5.1.1.1" class="ltx_text" style="font-size:90%;">Object Detection</span></p>
</td>
<td id="S4.T2.5.5.11.5.2" class="ltx_td ltx_align_justify ltx_border_t" style="width:56.9pt;padding-left:4.0pt;padding-right:4.0pt;">
<span id="S4.T2.5.5.11.5.2.1" class="ltx_inline-block ltx_parbox ltx_align_top" style="width:54.1pt;">
<span id="S4.T2.5.5.11.5.2.1.1" class="ltx_p"><span id="S4.T2.5.5.11.5.2.1.1.1" class="ltx_text" style="font-size:90%;">DETR </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.5.5.11.5.2.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib13" title="" class="ltx_ref">13</a><span id="S4.T2.5.5.11.5.2.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
<span id="S4.T2.5.5.11.5.2.1.2" class="ltx_p"><span id="S4.T2.5.5.11.5.2.1.2.1" class="ltx_text" style="font-size:90%;">ECCV’20</span></span>
</span>
</td>
<td id="S4.T2.5.5.11.5.3" class="ltx_td ltx_align_justify ltx_border_t" style="width:42.7pt;padding-left:4.0pt;padding-right:4.0pt;">
<p id="S4.T2.5.5.11.5.3.1" class="ltx_p ltx_align_top"><span id="S4.T2.5.5.11.5.3.1.1" class="ltx_text" style="font-size:90%;">AP</span></p>
</td>
<td id="S4.T2.5.5.11.5.4" class="ltx_td ltx_align_justify ltx_border_t" style="width:42.7pt;padding-left:4.0pt;padding-right:4.0pt;">
<p id="S4.T2.5.5.11.5.4.1" class="ltx_p ltx_align_top"><span id="S4.T2.5.5.11.5.4.1.1" class="ltx_text" style="font-size:90%;">COCO</span></p>
</td>
<td id="S4.T2.5.5.11.5.5" class="ltx_td ltx_align_justify ltx_border_t" style="width:42.7pt;padding-left:4.0pt;padding-right:4.0pt;">
<p id="S4.T2.5.5.11.5.5.1" class="ltx_p ltx_align_top"><span id="S4.T2.5.5.11.5.5.1.1" class="ltx_text" style="font-size:90%;">44.9</span></p>
</td>
<td id="S4.T2.5.5.11.5.6" class="ltx_td ltx_align_justify ltx_border_t" style="width:142.3pt;padding-left:4.0pt;padding-right:4.0pt;">
<p id="S4.T2.5.5.11.5.6.1" class="ltx_p ltx_align_top"><span id="S4.T2.5.5.11.5.6.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">a)</span><span id="S4.T2.5.5.11.5.6.1.2" class="ltx_text" style="font-size:90%;"> Use of Transformer allows end-to-end training pipeline for object detection,</span><span id="S4.T2.5.5.11.5.6.1.3" class="ltx_text ltx_font_bold" style="font-size:90%;"> b)</span><span id="S4.T2.5.5.11.5.6.1.4" class="ltx_text" style="font-size:90%;"> Removes the need for hand-crafted post-processing steps.</span></p>
</td>
<td id="S4.T2.5.5.11.5.7" class="ltx_td ltx_align_justify ltx_border_t" style="width:142.3pt;padding-left:4.0pt;padding-right:4.0pt;">
<p id="S4.T2.5.5.11.5.7.1" class="ltx_p ltx_align_top"><span id="S4.T2.5.5.11.5.7.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">a)</span><span id="S4.T2.5.5.11.5.7.1.2" class="ltx_text" style="font-size:90%;"> Performs poorly on small objects, </span><span id="S4.T2.5.5.11.5.7.1.3" class="ltx_text ltx_font_bold" style="font-size:90%;">b)</span><span id="S4.T2.5.5.11.5.7.1.4" class="ltx_text" style="font-size:90%;"> Requires long training time to converge.</span></p>
</td>
</tr>
<tr id="S4.T2.5.5.12.6" class="ltx_tr">
<td id="S4.T2.5.5.12.6.1" class="ltx_td" style="width:42.7pt;padding-left:4.0pt;padding-right:4.0pt;"></td>
<td id="S4.T2.5.5.12.6.2" class="ltx_td ltx_align_justify ltx_border_t" style="width:56.9pt;padding-left:4.0pt;padding-right:4.0pt;">
<span id="S4.T2.5.5.12.6.2.1" class="ltx_inline-block ltx_parbox ltx_align_top" style="width:54.1pt;">
<span id="S4.T2.5.5.12.6.2.1.1" class="ltx_p"><span id="S4.T2.5.5.12.6.2.1.1.1" class="ltx_text" style="font-size:90%;">D-DETR </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.5.5.12.6.2.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib14" title="" class="ltx_ref">14</a><span id="S4.T2.5.5.12.6.2.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.T2.5.5.12.6.2.1.1.4" class="ltx_text" style="font-size:90%;"></span></span>
<span id="S4.T2.5.5.12.6.2.1.2" class="ltx_p"><span id="S4.T2.5.5.12.6.2.1.2.1" class="ltx_text" style="font-size:90%;">ICLR’21</span></span>
</span>
</td>
<td id="S4.T2.5.5.12.6.3" class="ltx_td ltx_align_justify ltx_border_t" style="width:42.7pt;padding-left:4.0pt;padding-right:4.0pt;">
<p id="S4.T2.5.5.12.6.3.1" class="ltx_p ltx_align_top"><span id="S4.T2.5.5.12.6.3.1.1" class="ltx_text" style="font-size:90%;">AP</span></p>
</td>
<td id="S4.T2.5.5.12.6.4" class="ltx_td ltx_align_justify ltx_border_t" style="width:42.7pt;padding-left:4.0pt;padding-right:4.0pt;">
<p id="S4.T2.5.5.12.6.4.1" class="ltx_p ltx_align_top"><span id="S4.T2.5.5.12.6.4.1.1" class="ltx_text" style="font-size:90%;">COCO</span></p>
</td>
<td id="S4.T2.5.5.12.6.5" class="ltx_td ltx_align_justify ltx_border_t" style="width:42.7pt;padding-left:4.0pt;padding-right:4.0pt;">
<p id="S4.T2.5.5.12.6.5.1" class="ltx_p ltx_align_top"><span id="S4.T2.5.5.12.6.5.1.1" class="ltx_text" style="font-size:90%;">43.8</span></p>
</td>
<td id="S4.T2.5.5.12.6.6" class="ltx_td ltx_align_justify ltx_border_t" style="width:142.3pt;padding-left:4.0pt;padding-right:4.0pt;">
<p id="S4.T2.5.5.12.6.6.1" class="ltx_p ltx_align_top"><span id="S4.T2.5.5.12.6.6.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">a)</span><span id="S4.T2.5.5.12.6.6.1.2" class="ltx_text" style="font-size:90%;"> Achieves better performance on small objects than DETR </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.5.5.12.6.6.1.3.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib13" title="" class="ltx_ref">13</a><span id="S4.T2.5.5.12.6.6.1.4.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.T2.5.5.12.6.6.1.5" class="ltx_text" style="font-size:90%;">,</span><span id="S4.T2.5.5.12.6.6.1.6" class="ltx_text ltx_font_bold" style="font-size:90%;"> b)</span><span id="S4.T2.5.5.12.6.6.1.7" class="ltx_text" style="font-size:90%;"> Faster convergence than DETR </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.5.5.12.6.6.1.8.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib13" title="" class="ltx_ref">13</a><span id="S4.T2.5.5.12.6.6.1.9.2" class="ltx_text" style="font-size:90%;">]</span></cite></p>
</td>
<td id="S4.T2.5.5.12.6.7" class="ltx_td ltx_align_justify ltx_border_t" style="width:142.3pt;padding-left:4.0pt;padding-right:4.0pt;">
<p id="S4.T2.5.5.12.6.7.1" class="ltx_p ltx_align_top"><span id="S4.T2.5.5.12.6.7.1.1" class="ltx_text" style="font-size:90%;">Obtain SOTA results with 52.3 AP but with two stage detector design and test time augmentations.</span></p>
</td>
</tr>
<tr id="S4.T2.1.1.1" class="ltx_tr">
<td id="S4.T2.1.1.1.2" class="ltx_td ltx_align_justify ltx_border_t" style="width:42.7pt;padding-left:4.0pt;padding-right:4.0pt;">
<p id="S4.T2.1.1.1.2.1" class="ltx_p ltx_align_top"><span id="S4.T2.1.1.1.2.1.1" class="ltx_text" style="font-size:90%;">Image Colorization</span></p>
</td>
<td id="S4.T2.1.1.1.3" class="ltx_td ltx_align_justify ltx_border_t" style="width:56.9pt;padding-left:4.0pt;padding-right:4.0pt;">
<span id="S4.T2.1.1.1.3.1" class="ltx_inline-block ltx_parbox ltx_align_top" style="width:54.1pt;">
<span id="S4.T2.1.1.1.3.1.1" class="ltx_p"><span id="S4.T2.1.1.1.3.1.1.1" class="ltx_text" style="font-size:90%;">ColTran </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.1.1.1.3.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib24" title="" class="ltx_ref">24</a><span id="S4.T2.1.1.1.3.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
<span id="S4.T2.1.1.1.3.1.2" class="ltx_p"><span id="S4.T2.1.1.1.3.1.2.1" class="ltx_text" style="font-size:90%;">ICLR’21</span></span>
</span>
</td>
<td id="S4.T2.1.1.1.4" class="ltx_td ltx_align_justify ltx_border_t" style="width:42.7pt;padding-left:4.0pt;padding-right:4.0pt;">
<p id="S4.T2.1.1.1.4.1" class="ltx_p ltx_align_top"><span id="S4.T2.1.1.1.4.1.1" class="ltx_text" style="font-size:90%;">FID</span></p>
</td>
<td id="S4.T2.1.1.1.5" class="ltx_td ltx_align_justify ltx_border_t" style="width:42.7pt;padding-left:4.0pt;padding-right:4.0pt;">
<p id="S4.T2.1.1.1.5.1" class="ltx_p ltx_align_top"><span id="S4.T2.1.1.1.5.1.1" class="ltx_text" style="font-size:90%;">ImageNet</span></p>
</td>
<td id="S4.T2.1.1.1.6" class="ltx_td ltx_align_justify ltx_border_t" style="width:42.7pt;padding-left:4.0pt;padding-right:4.0pt;">
<p id="S4.T2.1.1.1.6.1" class="ltx_p ltx_align_top"><span id="S4.T2.1.1.1.6.1.1" class="ltx_text" style="font-size:90%;">19.71</span></p>
</td>
<td id="S4.T2.1.1.1.7" class="ltx_td ltx_align_justify ltx_border_t" style="width:142.3pt;padding-left:4.0pt;padding-right:4.0pt;">
<p id="S4.T2.1.1.1.7.1" class="ltx_p ltx_align_top"><span id="S4.T2.1.1.1.7.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">a)</span><span id="S4.T2.1.1.1.7.1.2" class="ltx_text" style="font-size:90%;"> First successful application of Transformer to image colorization, </span><span id="S4.T2.1.1.1.7.1.3" class="ltx_text ltx_font_bold" style="font-size:90%;">b)</span><span id="S4.T2.1.1.1.7.1.4" class="ltx_text" style="font-size:90%;"> Achieves SOTA FID score.</span></p>
</td>
<td id="S4.T2.1.1.1.1" class="ltx_td ltx_align_justify ltx_border_t" style="width:142.3pt;padding-left:4.0pt;padding-right:4.0pt;">
<p id="S4.T2.1.1.1.1.1.1" class="ltx_p ltx_align_top"><span id="S4.T2.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">a)</span><span id="S4.T2.1.1.1.1.1.1.2" class="ltx_text" style="font-size:90%;"> Lacks end-to-end training, </span><span id="S4.T2.1.1.1.1.1.1.3" class="ltx_text ltx_font_bold" style="font-size:90%;">b)</span><span id="S4.T2.1.1.1.1.1.1.4" class="ltx_text" style="font-size:90%;"> limited to images of size 256</span><math id="S4.T2.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T2.1.1.1.1.1.1.m1.1a"><mo mathsize="90%" id="S4.T2.1.1.1.1.1.1.m1.1.1" xref="S4.T2.1.1.1.1.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.1.1.1.m1.1b"><times id="S4.T2.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.1.1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T2.1.1.1.1.1.1.m1.1d">×</annotation></semantics></math><span id="S4.T2.1.1.1.1.1.1.5" class="ltx_text" style="font-size:90%;">256.</span></p>
</td>
</tr>
<tr id="S4.T2.5.5.13.7" class="ltx_tr">
<td id="S4.T2.5.5.13.7.1" class="ltx_td ltx_align_justify ltx_border_t" style="width:42.7pt;padding-left:4.0pt;padding-right:4.0pt;">
<p id="S4.T2.5.5.13.7.1.1" class="ltx_p ltx_align_top"><span id="S4.T2.5.5.13.7.1.1.1" class="ltx_text" style="font-size:90%;">Action Recognition</span></p>
</td>
<td id="S4.T2.5.5.13.7.2" class="ltx_td ltx_align_justify ltx_border_t" style="width:56.9pt;padding-left:4.0pt;padding-right:4.0pt;">
<span id="S4.T2.5.5.13.7.2.1" class="ltx_inline-block ltx_parbox ltx_align_top" style="width:54.1pt;">
<span id="S4.T2.5.5.13.7.2.1.1" class="ltx_p"><span id="S4.T2.5.5.13.7.2.1.1.1" class="ltx_text" style="font-size:90%;">ST-TR </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.5.5.13.7.2.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib216" title="" class="ltx_ref">216</a><span id="S4.T2.5.5.13.7.2.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
<span id="S4.T2.5.5.13.7.2.1.2" class="ltx_p"><span id="S4.T2.5.5.13.7.2.1.2.1" class="ltx_text" style="font-size:90%;">arXiv’20</span></span>
</span>
</td>
<td id="S4.T2.5.5.13.7.3" class="ltx_td ltx_align_justify ltx_border_t" style="width:42.7pt;padding-left:4.0pt;padding-right:4.0pt;">
<p id="S4.T2.5.5.13.7.3.1" class="ltx_p ltx_align_top"><span id="S4.T2.5.5.13.7.3.1.1" class="ltx_text" style="font-size:90%;">Top-1 Acc.</span></p>
</td>
<td id="S4.T2.5.5.13.7.4" class="ltx_td ltx_align_justify ltx_border_t" style="width:42.7pt;padding-left:4.0pt;padding-right:4.0pt;">
<p id="S4.T2.5.5.13.7.4.1" class="ltx_p ltx_align_top"><span id="S4.T2.5.5.13.7.4.1.1" class="ltx_text" style="font-size:90%;">NTU 60/120</span></p>
</td>
<td id="S4.T2.5.5.13.7.5" class="ltx_td ltx_align_justify ltx_border_t" style="width:42.7pt;padding-left:4.0pt;padding-right:4.0pt;">
<p id="S4.T2.5.5.13.7.5.1" class="ltx_p ltx_align_top"><span id="S4.T2.5.5.13.7.5.1.1" class="ltx_text" style="font-size:90%;">94.0/84.7</span></p>
</td>
<td id="S4.T2.5.5.13.7.6" class="ltx_td ltx_align_justify ltx_border_t" style="width:142.3pt;padding-left:4.0pt;padding-right:4.0pt;">
<p id="S4.T2.5.5.13.7.6.1" class="ltx_p ltx_align_top"><span id="S4.T2.5.5.13.7.6.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">a)</span><span id="S4.T2.5.5.13.7.6.1.2" class="ltx_text" style="font-size:90%;"> Successfully applies Transformer to model relations between body joints both in spatial and temporal domain, </span><span id="S4.T2.5.5.13.7.6.1.3" class="ltx_text ltx_font_bold" style="font-size:90%;">b)</span><span id="S4.T2.5.5.13.7.6.1.4" class="ltx_text" style="font-size:90%;"> Achieves SOTA results.</span></p>
</td>
<td id="S4.T2.5.5.13.7.7" class="ltx_td ltx_align_justify ltx_border_t" style="width:142.3pt;padding-left:4.0pt;padding-right:4.0pt;">
<p id="S4.T2.5.5.13.7.7.1" class="ltx_p ltx_align_top"><span id="S4.T2.5.5.13.7.7.1.1" class="ltx_text" style="font-size:90%;">Proposed Transformers do not process joints directly rather operate on features extracted by a CNN, thus the overall model is based on hand-crafted design.</span></p>
</td>
</tr>
<tr id="S4.T2.5.5.14.8" class="ltx_tr">
<td id="S4.T2.5.5.14.8.1" class="ltx_td ltx_align_justify ltx_border_t" style="width:42.7pt;padding-left:4.0pt;padding-right:4.0pt;">
<p id="S4.T2.5.5.14.8.1.1" class="ltx_p ltx_align_top"><span id="S4.T2.5.5.14.8.1.1.1" class="ltx_text" style="font-size:90%;">Super-Resolution</span></p>
</td>
<td id="S4.T2.5.5.14.8.2" class="ltx_td ltx_align_justify ltx_border_t" style="width:56.9pt;padding-left:4.0pt;padding-right:4.0pt;">
<span id="S4.T2.5.5.14.8.2.1" class="ltx_inline-block ltx_parbox ltx_align_top" style="width:54.1pt;">
<span id="S4.T2.5.5.14.8.2.1.1" class="ltx_p"><span id="S4.T2.5.5.14.8.2.1.1.1" class="ltx_text" style="font-size:90%;">TTSR </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.5.5.14.8.2.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib16" title="" class="ltx_ref">16</a><span id="S4.T2.5.5.14.8.2.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
<span id="S4.T2.5.5.14.8.2.1.2" class="ltx_p"><span id="S4.T2.5.5.14.8.2.1.2.1" class="ltx_text" style="font-size:90%;">CVPR’20</span></span>
</span>
</td>
<td id="S4.T2.5.5.14.8.3" class="ltx_td ltx_align_justify ltx_border_t" style="width:42.7pt;padding-left:4.0pt;padding-right:4.0pt;">
<span id="S4.T2.5.5.14.8.3.1" class="ltx_inline-block ltx_parbox ltx_align_top" style="width:54.1pt;">
<span id="S4.T2.5.5.14.8.3.1.1" class="ltx_p"><span id="S4.T2.5.5.14.8.3.1.1.1" class="ltx_text" style="font-size:90%;">PSNR/</span></span>
<span id="S4.T2.5.5.14.8.3.1.2" class="ltx_p"><span id="S4.T2.5.5.14.8.3.1.2.1" class="ltx_text" style="font-size:90%;">SSIM</span></span>
</span>
</td>
<td id="S4.T2.5.5.14.8.4" class="ltx_td ltx_align_justify ltx_border_t" style="width:42.7pt;padding-left:4.0pt;padding-right:4.0pt;">
<span id="S4.T2.5.5.14.8.4.1" class="ltx_inline-block ltx_parbox ltx_align_top" style="width:54.1pt;">
<span id="S4.T2.5.5.14.8.4.1.1" class="ltx_p"><span id="S4.T2.5.5.14.8.4.1.1.1" class="ltx_text" style="font-size:90%;">CUFED5</span></span>
<span id="S4.T2.5.5.14.8.4.1.2" class="ltx_p"><span id="S4.T2.5.5.14.8.4.1.2.1" class="ltx_text" style="font-size:90%;">Sun80</span></span>
<span id="S4.T2.5.5.14.8.4.1.3" class="ltx_p"><span id="S4.T2.5.5.14.8.4.1.3.1" class="ltx_text" style="font-size:90%;">Urban100</span></span>
<span id="S4.T2.5.5.14.8.4.1.4" class="ltx_p"><span id="S4.T2.5.5.14.8.4.1.4.1" class="ltx_text" style="font-size:90%;">Manga109</span></span>
</span>
</td>
<td id="S4.T2.5.5.14.8.5" class="ltx_td ltx_align_justify ltx_border_t" style="width:42.7pt;padding-left:4.0pt;padding-right:4.0pt;">
<span id="S4.T2.5.5.14.8.5.1" class="ltx_inline-block ltx_parbox ltx_align_top" style="width:54.1pt;">
<span id="S4.T2.5.5.14.8.5.1.1" class="ltx_p"><span id="S4.T2.5.5.14.8.5.1.1.1" class="ltx_text" style="font-size:90%;">27.1 / 0.8</span></span>
<span id="S4.T2.5.5.14.8.5.1.2" class="ltx_p"><span id="S4.T2.5.5.14.8.5.1.2.1" class="ltx_text" style="font-size:90%;">30.0 / 0.81</span></span>
<span id="S4.T2.5.5.14.8.5.1.3" class="ltx_p"><span id="S4.T2.5.5.14.8.5.1.3.1" class="ltx_text" style="font-size:90%;">25.9 / 0.78</span></span>
<span id="S4.T2.5.5.14.8.5.1.4" class="ltx_p"><span id="S4.T2.5.5.14.8.5.1.4.1" class="ltx_text" style="font-size:90%;">30.1 / 0.91</span></span>
</span>
</td>
<td id="S4.T2.5.5.14.8.6" class="ltx_td ltx_align_justify ltx_border_t" style="width:142.3pt;padding-left:4.0pt;padding-right:4.0pt;">
<p id="S4.T2.5.5.14.8.6.1" class="ltx_p ltx_align_top"><span id="S4.T2.5.5.14.8.6.1.1" class="ltx_text" style="font-size:90%;"></span><span id="S4.T2.5.5.14.8.6.1.2" class="ltx_text ltx_font_bold" style="font-size:90%;">a)</span><span id="S4.T2.5.5.14.8.6.1.3" class="ltx_text" style="font-size:90%;"> Achieves state-of-the-art super-resolution by using attention, </span><span id="S4.T2.5.5.14.8.6.1.4" class="ltx_text ltx_font_bold" style="font-size:90%;">b)</span><span id="S4.T2.5.5.14.8.6.1.5" class="ltx_text" style="font-size:90%;"> Novel Transformer inspired architectures that can process multi-scale features.</span></p>
</td>
<td id="S4.T2.5.5.14.8.7" class="ltx_td ltx_align_justify ltx_border_t" style="width:142.3pt;padding-left:4.0pt;padding-right:4.0pt;">
<p id="S4.T2.5.5.14.8.7.1" class="ltx_p ltx_align_top"><span id="S4.T2.5.5.14.8.7.1.1" class="ltx_text" style="font-size:90%;"></span><span id="S4.T2.5.5.14.8.7.1.2" class="ltx_text ltx_font_bold" style="font-size:90%;">a)</span><span id="S4.T2.5.5.14.8.7.1.3" class="ltx_text" style="font-size:90%;"> Proposed Transformer does not process images directly but features extracted by a convolution based network, </span><span id="S4.T2.5.5.14.8.7.1.4" class="ltx_text ltx_font_bold" style="font-size:90%;">b)</span><span id="S4.T2.5.5.14.8.7.1.5" class="ltx_text" style="font-size:90%;"> Model with large number of trainable parameters, and </span><span id="S4.T2.5.5.14.8.7.1.6" class="ltx_text ltx_font_bold" style="font-size:90%;">c)</span><span id="S4.T2.5.5.14.8.7.1.7" class="ltx_text" style="font-size:90%;"> Compute intensive.</span></p>
</td>
</tr>
<tr id="S4.T2.2.2.2" class="ltx_tr">
<td id="S4.T2.2.2.2.2" class="ltx_td ltx_align_justify ltx_border_t" style="width:42.7pt;padding-left:4.0pt;padding-right:4.0pt;">
<p id="S4.T2.2.2.2.2.1" class="ltx_p ltx_align_top"><span id="S4.T2.2.2.2.2.1.1" class="ltx_text" style="font-size:90%;">Multi-Model Learning</span></p>
</td>
<td id="S4.T2.2.2.2.3" class="ltx_td ltx_align_justify ltx_border_t" style="width:56.9pt;padding-left:4.0pt;padding-right:4.0pt;">
<span id="S4.T2.2.2.2.3.1" class="ltx_inline-block ltx_parbox ltx_align_top" style="width:54.1pt;">
<span id="S4.T2.2.2.2.3.1.1" class="ltx_p"><span id="S4.T2.2.2.2.3.1.1.1" class="ltx_text" style="font-size:90%;">ViLBERT </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.2.2.2.3.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib181" title="" class="ltx_ref">181</a><span id="S4.T2.2.2.2.3.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.T2.2.2.2.3.1.1.4" class="ltx_text" style="font-size:90%;"></span></span>
<span id="S4.T2.2.2.2.3.1.2" class="ltx_p"><span id="S4.T2.2.2.2.3.1.2.1" class="ltx_text" style="font-size:90%;">NeurIPS’19</span></span>
</span>
</td>
<td id="S4.T2.2.2.2.1" class="ltx_td ltx_align_justify ltx_border_t" style="width:42.7pt;padding-left:4.0pt;padding-right:4.0pt;">
<span id="S4.T2.2.2.2.1.1.1.1" class="ltx_inline-block ltx_parbox ltx_align_top" style="width:54.1pt;">
<span id="S4.T2.2.2.2.1.1.1.1.2" class="ltx_p"><span id="S4.T2.2.2.2.1.1.1.1.2.1" class="ltx_text" style="font-size:90%;">Acc./</span></span>
<span id="S4.T2.2.2.2.1.1.1.1.1" class="ltx_p"><span id="S4.T2.2.2.2.1.1.1.1.1.1" class="ltx_text" style="font-size:90%;">mAP (</span><math id="S4.T2.2.2.2.1.1.1.1.1.m1.1" class="ltx_Math" alttext="R@1" display="inline"><semantics id="S4.T2.2.2.2.1.1.1.1.1.m1.1a"><mrow id="S4.T2.2.2.2.1.1.1.1.1.m1.1.1" xref="S4.T2.2.2.2.1.1.1.1.1.m1.1.1.cmml"><mi mathsize="90%" id="S4.T2.2.2.2.1.1.1.1.1.m1.1.1.2" xref="S4.T2.2.2.2.1.1.1.1.1.m1.1.1.2.cmml">R</mi><mo id="S4.T2.2.2.2.1.1.1.1.1.m1.1.1.1" xref="S4.T2.2.2.2.1.1.1.1.1.m1.1.1.1.cmml" lspace='0px' rspace='0px'></mo><mi mathsize="90%" mathvariant="normal" id="S4.T2.2.2.2.1.1.1.1.1.m1.1.1.3" xref="S4.T2.2.2.2.1.1.1.1.1.m1.1.1.3.cmml">@</mi><mo id="S4.T2.2.2.2.1.1.1.1.1.m1.1.1.1a" xref="S4.T2.2.2.2.1.1.1.1.1.m1.1.1.1.cmml" lspace='0px' rspace='0px'></mo><mn mathsize="90%" id="S4.T2.2.2.2.1.1.1.1.1.m1.1.1.4" xref="S4.T2.2.2.2.1.1.1.1.1.m1.1.1.4.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.2.1.1.1.1.1.m1.1b"><apply id="S4.T2.2.2.2.1.1.1.1.1.m1.1.1.cmml" xref="S4.T2.2.2.2.1.1.1.1.1.m1.1.1"><times id="S4.T2.2.2.2.1.1.1.1.1.m1.1.1.1.cmml" xref="S4.T2.2.2.2.1.1.1.1.1.m1.1.1.1"></times><ci id="S4.T2.2.2.2.1.1.1.1.1.m1.1.1.2.cmml" xref="S4.T2.2.2.2.1.1.1.1.1.m1.1.1.2">𝑅</ci><ci id="S4.T2.2.2.2.1.1.1.1.1.m1.1.1.3.cmml" xref="S4.T2.2.2.2.1.1.1.1.1.m1.1.1.3">@</ci><cn type="integer" id="S4.T2.2.2.2.1.1.1.1.1.m1.1.1.4.cmml" xref="S4.T2.2.2.2.1.1.1.1.1.m1.1.1.4">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.2.1.1.1.1.1.m1.1c">R@1</annotation><annotation encoding="application/x-llamapun" id="S4.T2.2.2.2.1.1.1.1.1.m1.1d">italic_R @ 1</annotation></semantics></math><span id="S4.T2.2.2.2.1.1.1.1.1.2" class="ltx_text" style="font-size:90%;">)</span></span>
</span>
</td>
<td id="S4.T2.2.2.2.4" class="ltx_td ltx_align_justify ltx_border_t" style="width:42.7pt;padding-left:4.0pt;padding-right:4.0pt;">
<span id="S4.T2.2.2.2.4.1" class="ltx_inline-block ltx_parbox ltx_align_top" style="width:54.1pt;">
<span id="S4.T2.2.2.2.4.1.1" class="ltx_p"><span id="S4.T2.2.2.2.4.1.1.1" class="ltx_text" style="font-size:90%;">VQA </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.2.2.2.4.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib183" title="" class="ltx_ref">183</a><span id="S4.T2.2.2.2.4.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.T2.2.2.2.4.1.1.4" class="ltx_text" style="font-size:90%;">/</span></span>
<span id="S4.T2.2.2.2.4.1.2" class="ltx_p"><span id="S4.T2.2.2.2.4.1.2.1" class="ltx_text" style="font-size:90%;">Retrieval</span></span>
<span id="S4.T2.2.2.2.4.1.3" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.2.2.2.4.1.3.1.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib239" title="" class="ltx_ref">239</a><span id="S4.T2.2.2.2.4.1.3.2.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td id="S4.T2.2.2.2.5" class="ltx_td ltx_align_justify ltx_border_t" style="width:42.7pt;padding-left:4.0pt;padding-right:4.0pt;">
<p id="S4.T2.2.2.2.5.1" class="ltx_p ltx_align_top"><span id="S4.T2.2.2.2.5.1.1" class="ltx_text" style="font-size:90%;">70.6/ 58.2</span></p>
</td>
<td id="S4.T2.2.2.2.6" class="ltx_td ltx_align_justify ltx_border_t" style="width:142.3pt;padding-left:4.0pt;padding-right:4.0pt;">
<p id="S4.T2.2.2.2.6.1" class="ltx_p ltx_align_top"><span id="S4.T2.2.2.2.6.1.1" class="ltx_text" style="font-size:90%;"></span><span id="S4.T2.2.2.2.6.1.2" class="ltx_text ltx_font_bold" style="font-size:90%;">a)</span><span id="S4.T2.2.2.2.6.1.3" class="ltx_text" style="font-size:90%;"> Proposed Transformer architecture can combine text and visual information to understand inter-task dependencies, </span><span id="S4.T2.2.2.2.6.1.4" class="ltx_text ltx_font_bold" style="font-size:90%;">b)</span><span id="S4.T2.2.2.2.6.1.5" class="ltx_text" style="font-size:90%;"> Achieves pre-training on unlabelled dataset.</span></p>
</td>
<td id="S4.T2.2.2.2.7" class="ltx_td ltx_align_justify ltx_border_t" style="width:142.3pt;padding-left:4.0pt;padding-right:4.0pt;">
<p id="S4.T2.2.2.2.7.1" class="ltx_p ltx_align_top"><span id="S4.T2.2.2.2.7.1.1" class="ltx_text" style="font-size:90%;"></span><span id="S4.T2.2.2.2.7.1.2" class="ltx_text ltx_font_bold" style="font-size:90%;">a)</span><span id="S4.T2.2.2.2.7.1.3" class="ltx_text" style="font-size:90%;"> Requires large amount of data for pre-training,</span><span id="S4.T2.2.2.2.7.1.4" class="ltx_text ltx_font_bold" style="font-size:90%;"> b)</span><span id="S4.T2.2.2.2.7.1.5" class="ltx_text" style="font-size:90%;"> Requires fine tuning to the new task.</span></p>
</td>
</tr>
<tr id="S4.T2.3.3.3" class="ltx_tr">
<td id="S4.T2.3.3.3.2" class="ltx_td" style="width:42.7pt;padding-left:4.0pt;padding-right:4.0pt;"></td>
<td id="S4.T2.3.3.3.3" class="ltx_td ltx_align_justify ltx_border_t" style="width:56.9pt;padding-left:4.0pt;padding-right:4.0pt;">
<span id="S4.T2.3.3.3.3.1" class="ltx_inline-block ltx_parbox ltx_align_top" style="width:54.1pt;">
<span id="S4.T2.3.3.3.3.1.1" class="ltx_p"><span id="S4.T2.3.3.3.3.1.1.1" class="ltx_text" style="font-size:90%;">Oscar </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.3.3.3.3.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib44" title="" class="ltx_ref">44</a><span id="S4.T2.3.3.3.3.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.T2.3.3.3.3.1.1.4" class="ltx_text" style="font-size:90%;"></span></span>
<span id="S4.T2.3.3.3.3.1.2" class="ltx_p"><span id="S4.T2.3.3.3.3.1.2.1" class="ltx_text" style="font-size:90%;">ECCV’20</span></span>
</span>
</td>
<td id="S4.T2.3.3.3.1" class="ltx_td ltx_align_justify ltx_border_t" style="width:42.7pt;padding-left:4.0pt;padding-right:4.0pt;">
<span id="S4.T2.3.3.3.1.1.1.1" class="ltx_inline-block ltx_parbox ltx_align_top" style="width:54.1pt;">
<span id="S4.T2.3.3.3.1.1.1.1.2" class="ltx_p"><span id="S4.T2.3.3.3.1.1.1.1.2.1" class="ltx_text" style="font-size:90%;">Acc./</span></span>
<span id="S4.T2.3.3.3.1.1.1.1.1" class="ltx_p"><span id="S4.T2.3.3.3.1.1.1.1.1.1" class="ltx_text" style="font-size:90%;">mAP (</span><math id="S4.T2.3.3.3.1.1.1.1.1.m1.1" class="ltx_Math" alttext="R@1" display="inline"><semantics id="S4.T2.3.3.3.1.1.1.1.1.m1.1a"><mrow id="S4.T2.3.3.3.1.1.1.1.1.m1.1.1" xref="S4.T2.3.3.3.1.1.1.1.1.m1.1.1.cmml"><mi mathsize="90%" id="S4.T2.3.3.3.1.1.1.1.1.m1.1.1.2" xref="S4.T2.3.3.3.1.1.1.1.1.m1.1.1.2.cmml">R</mi><mo id="S4.T2.3.3.3.1.1.1.1.1.m1.1.1.1" xref="S4.T2.3.3.3.1.1.1.1.1.m1.1.1.1.cmml" lspace='0px' rspace='0px'></mo><mi mathsize="90%" mathvariant="normal" id="S4.T2.3.3.3.1.1.1.1.1.m1.1.1.3" xref="S4.T2.3.3.3.1.1.1.1.1.m1.1.1.3.cmml">@</mi><mo id="S4.T2.3.3.3.1.1.1.1.1.m1.1.1.1a" xref="S4.T2.3.3.3.1.1.1.1.1.m1.1.1.1.cmml" lspace='0px' rspace='0px'></mo><mn mathsize="90%" id="S4.T2.3.3.3.1.1.1.1.1.m1.1.1.4" xref="S4.T2.3.3.3.1.1.1.1.1.m1.1.1.4.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.3.3.3.1.1.1.1.1.m1.1b"><apply id="S4.T2.3.3.3.1.1.1.1.1.m1.1.1.cmml" xref="S4.T2.3.3.3.1.1.1.1.1.m1.1.1"><times id="S4.T2.3.3.3.1.1.1.1.1.m1.1.1.1.cmml" xref="S4.T2.3.3.3.1.1.1.1.1.m1.1.1.1"></times><ci id="S4.T2.3.3.3.1.1.1.1.1.m1.1.1.2.cmml" xref="S4.T2.3.3.3.1.1.1.1.1.m1.1.1.2">𝑅</ci><ci id="S4.T2.3.3.3.1.1.1.1.1.m1.1.1.3.cmml" xref="S4.T2.3.3.3.1.1.1.1.1.m1.1.1.3">@</ci><cn type="integer" id="S4.T2.3.3.3.1.1.1.1.1.m1.1.1.4.cmml" xref="S4.T2.3.3.3.1.1.1.1.1.m1.1.1.4">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.3.3.1.1.1.1.1.m1.1c">R@1</annotation><annotation encoding="application/x-llamapun" id="S4.T2.3.3.3.1.1.1.1.1.m1.1d">italic_R @ 1</annotation></semantics></math><span id="S4.T2.3.3.3.1.1.1.1.1.2" class="ltx_text" style="font-size:90%;">)</span></span>
</span>
</td>
<td id="S4.T2.3.3.3.4" class="ltx_td ltx_align_justify ltx_border_t" style="width:42.7pt;padding-left:4.0pt;padding-right:4.0pt;">
<span id="S4.T2.3.3.3.4.1" class="ltx_inline-block ltx_parbox ltx_align_top" style="width:54.1pt;">
<span id="S4.T2.3.3.3.4.1.1" class="ltx_p"><span id="S4.T2.3.3.3.4.1.1.1" class="ltx_text" style="font-size:90%;">VQA </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.3.3.3.4.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib240" title="" class="ltx_ref">240</a><span id="S4.T2.3.3.3.4.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.T2.3.3.3.4.1.1.4" class="ltx_text" style="font-size:90%;">/</span></span>
<span id="S4.T2.3.3.3.4.1.2" class="ltx_p"><span id="S4.T2.3.3.3.4.1.2.1" class="ltx_text" style="font-size:90%;">COCO</span></span>
</span>
</td>
<td id="S4.T2.3.3.3.5" class="ltx_td ltx_align_justify ltx_border_t" style="width:42.7pt;padding-left:4.0pt;padding-right:4.0pt;">
<span id="S4.T2.3.3.3.5.1" class="ltx_inline-block ltx_parbox ltx_align_top" style="width:54.1pt;">
<span id="S4.T2.3.3.3.5.1.1" class="ltx_p"><span id="S4.T2.3.3.3.5.1.1.1" class="ltx_text" style="font-size:90%;">80.37/57.5</span></span>
</span>
</td>
<td id="S4.T2.3.3.3.6" class="ltx_td ltx_align_justify ltx_border_t" style="width:142.3pt;padding-left:4.0pt;padding-right:4.0pt;">
<p id="S4.T2.3.3.3.6.1" class="ltx_p ltx_align_top"><span id="S4.T2.3.3.3.6.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">a)</span><span id="S4.T2.3.3.3.6.1.2" class="ltx_text" style="font-size:90%;"> Exploit novel supervisory signal via object tags to achieve text and image alignment, </span><span id="S4.T2.3.3.3.6.1.3" class="ltx_text ltx_font_bold" style="font-size:90%;">b)</span><span id="S4.T2.3.3.3.6.1.4" class="ltx_text" style="font-size:90%;"> Achieves state-of-the-art results.</span></p>
</td>
<td id="S4.T2.3.3.3.7" class="ltx_td ltx_align_justify ltx_border_t" style="width:142.3pt;padding-left:4.0pt;padding-right:4.0pt;">
<p id="S4.T2.3.3.3.7.1" class="ltx_p ltx_align_top"><span id="S4.T2.3.3.3.7.1.1" class="ltx_text" style="font-size:90%;">Requires extra supervision through pre-trained object detectors thus performance is dependent on the quality of object detectors.</span></p>
</td>
</tr>
<tr id="S4.T2.4.4.4" class="ltx_tr">
<td id="S4.T2.4.4.4.2" class="ltx_td" style="width:42.7pt;padding-left:4.0pt;padding-right:4.0pt;"></td>
<td id="S4.T2.4.4.4.3" class="ltx_td ltx_align_justify ltx_border_t" style="width:56.9pt;padding-left:4.0pt;padding-right:4.0pt;">
<span id="S4.T2.4.4.4.3.1" class="ltx_inline-block ltx_parbox ltx_align_top" style="width:54.1pt;">
<span id="S4.T2.4.4.4.3.1.1" class="ltx_p"><span id="S4.T2.4.4.4.3.1.1.1" class="ltx_text" style="font-size:90%;">UNITER </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.4.4.4.3.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib43" title="" class="ltx_ref">43</a><span id="S4.T2.4.4.4.3.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
<span id="S4.T2.4.4.4.3.1.2" class="ltx_p"><span id="S4.T2.4.4.4.3.1.2.1" class="ltx_text" style="font-size:90%;">ECCV’20</span></span>
</span>
</td>
<td id="S4.T2.4.4.4.1" class="ltx_td ltx_align_justify ltx_border_t" style="width:42.7pt;padding-left:4.0pt;padding-right:4.0pt;">
<span id="S4.T2.4.4.4.1.1.1.1" class="ltx_inline-block ltx_parbox ltx_align_top" style="width:54.1pt;">
<span id="S4.T2.4.4.4.1.1.1.1.2" class="ltx_p"><span id="S4.T2.4.4.4.1.1.1.1.2.1" class="ltx_text" style="font-size:90%;">Acc./</span></span>
<span id="S4.T2.4.4.4.1.1.1.1.1" class="ltx_p"><span id="S4.T2.4.4.4.1.1.1.1.1.1" class="ltx_text" style="font-size:90%;">Avg. (</span><math id="S4.T2.4.4.4.1.1.1.1.1.m1.1" class="ltx_Math" alttext="R@1/5/10" display="inline"><semantics id="S4.T2.4.4.4.1.1.1.1.1.m1.1a"><mrow id="S4.T2.4.4.4.1.1.1.1.1.m1.1.1" xref="S4.T2.4.4.4.1.1.1.1.1.m1.1.1.cmml"><mrow id="S4.T2.4.4.4.1.1.1.1.1.m1.1.1.2" xref="S4.T2.4.4.4.1.1.1.1.1.m1.1.1.2.cmml"><mi mathsize="90%" id="S4.T2.4.4.4.1.1.1.1.1.m1.1.1.2.2" xref="S4.T2.4.4.4.1.1.1.1.1.m1.1.1.2.2.cmml">R</mi><mo id="S4.T2.4.4.4.1.1.1.1.1.m1.1.1.2.1" xref="S4.T2.4.4.4.1.1.1.1.1.m1.1.1.2.1.cmml" lspace='0px' rspace='0px'></mo><mi mathsize="90%" mathvariant="normal" id="S4.T2.4.4.4.1.1.1.1.1.m1.1.1.2.3" xref="S4.T2.4.4.4.1.1.1.1.1.m1.1.1.2.3.cmml">@</mi><mo id="S4.T2.4.4.4.1.1.1.1.1.m1.1.1.2.1a" xref="S4.T2.4.4.4.1.1.1.1.1.m1.1.1.2.1.cmml" lspace='0px' rspace='0px'></mo><mn mathsize="90%" id="S4.T2.4.4.4.1.1.1.1.1.m1.1.1.2.4" xref="S4.T2.4.4.4.1.1.1.1.1.m1.1.1.2.4.cmml">1</mn></mrow><mo maxsize="90%" minsize="90%" stretchy="true" symmetric="true" id="S4.T2.4.4.4.1.1.1.1.1.m1.1.1.1" xref="S4.T2.4.4.4.1.1.1.1.1.m1.1.1.1.cmml">/</mo><mn mathsize="90%" id="S4.T2.4.4.4.1.1.1.1.1.m1.1.1.3" xref="S4.T2.4.4.4.1.1.1.1.1.m1.1.1.3.cmml">5</mn><mo maxsize="90%" minsize="90%" stretchy="true" symmetric="true" id="S4.T2.4.4.4.1.1.1.1.1.m1.1.1.1a" xref="S4.T2.4.4.4.1.1.1.1.1.m1.1.1.1.cmml">/</mo><mn mathsize="90%" id="S4.T2.4.4.4.1.1.1.1.1.m1.1.1.4" xref="S4.T2.4.4.4.1.1.1.1.1.m1.1.1.4.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.4.4.4.1.1.1.1.1.m1.1b"><apply id="S4.T2.4.4.4.1.1.1.1.1.m1.1.1.cmml" xref="S4.T2.4.4.4.1.1.1.1.1.m1.1.1"><divide id="S4.T2.4.4.4.1.1.1.1.1.m1.1.1.1.cmml" xref="S4.T2.4.4.4.1.1.1.1.1.m1.1.1.1"></divide><apply id="S4.T2.4.4.4.1.1.1.1.1.m1.1.1.2.cmml" xref="S4.T2.4.4.4.1.1.1.1.1.m1.1.1.2"><times id="S4.T2.4.4.4.1.1.1.1.1.m1.1.1.2.1.cmml" xref="S4.T2.4.4.4.1.1.1.1.1.m1.1.1.2.1"></times><ci id="S4.T2.4.4.4.1.1.1.1.1.m1.1.1.2.2.cmml" xref="S4.T2.4.4.4.1.1.1.1.1.m1.1.1.2.2">𝑅</ci><ci id="S4.T2.4.4.4.1.1.1.1.1.m1.1.1.2.3.cmml" xref="S4.T2.4.4.4.1.1.1.1.1.m1.1.1.2.3">@</ci><cn type="integer" id="S4.T2.4.4.4.1.1.1.1.1.m1.1.1.2.4.cmml" xref="S4.T2.4.4.4.1.1.1.1.1.m1.1.1.2.4">1</cn></apply><cn type="integer" id="S4.T2.4.4.4.1.1.1.1.1.m1.1.1.3.cmml" xref="S4.T2.4.4.4.1.1.1.1.1.m1.1.1.3">5</cn><cn type="integer" id="S4.T2.4.4.4.1.1.1.1.1.m1.1.1.4.cmml" xref="S4.T2.4.4.4.1.1.1.1.1.m1.1.1.4">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.4.4.4.1.1.1.1.1.m1.1c">R@1/5/10</annotation><annotation encoding="application/x-llamapun" id="S4.T2.4.4.4.1.1.1.1.1.m1.1d">italic_R @ 1 / 5 / 10</annotation></semantics></math><span id="S4.T2.4.4.4.1.1.1.1.1.2" class="ltx_text" style="font-size:90%;">)</span></span>
</span>
</td>
<td id="S4.T2.4.4.4.4" class="ltx_td ltx_align_justify ltx_border_t" style="width:42.7pt;padding-left:4.0pt;padding-right:4.0pt;">
<span id="S4.T2.4.4.4.4.1" class="ltx_inline-block ltx_parbox ltx_align_top" style="width:54.1pt;">
<span id="S4.T2.4.4.4.4.1.1" class="ltx_p"><span id="S4.T2.4.4.4.4.1.1.1" class="ltx_text" style="font-size:90%;">VQA </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.4.4.4.4.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib183" title="" class="ltx_ref">183</a><span id="S4.T2.4.4.4.4.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.T2.4.4.4.4.1.1.4" class="ltx_text" style="font-size:90%;">/</span></span>
<span id="S4.T2.4.4.4.4.1.2" class="ltx_p"><span id="S4.T2.4.4.4.4.1.2.1" class="ltx_text" style="font-size:90%;">Flickr30K </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.4.4.4.4.1.2.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib241" title="" class="ltx_ref">241</a><span id="S4.T2.4.4.4.4.1.2.3.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td id="S4.T2.4.4.4.5" class="ltx_td ltx_align_justify ltx_border_t" style="width:42.7pt;padding-left:4.0pt;padding-right:4.0pt;">
<p id="S4.T2.4.4.4.5.1" class="ltx_p ltx_align_top"><span id="S4.T2.4.4.4.5.1.1" class="ltx_text" style="font-size:90%;">72.47/83.72</span></p>
</td>
<td id="S4.T2.4.4.4.6" class="ltx_td ltx_align_justify ltx_border_t" style="width:142.3pt;padding-left:4.0pt;padding-right:4.0pt;">
<p id="S4.T2.4.4.4.6.1" class="ltx_p ltx_align_top"><span id="S4.T2.4.4.4.6.1.1" class="ltx_text" style="font-size:90%;">Learns fine-grained relation alignment between text and images</span></p>
</td>
<td id="S4.T2.4.4.4.7" class="ltx_td ltx_align_justify ltx_border_t" style="width:142.3pt;padding-left:4.0pt;padding-right:4.0pt;">
<p id="S4.T2.4.4.4.7.1" class="ltx_p ltx_align_top"><span id="S4.T2.4.4.4.7.1.1" class="ltx_text" style="font-size:90%;">Requires large multi-task datasets for Transformer training which lead to high computational cost.</span></p>
</td>
</tr>
<tr id="S4.T2.5.5.5" class="ltx_tr">
<td id="S4.T2.5.5.5.2" class="ltx_td ltx_align_justify ltx_border_t" style="width:42.7pt;padding-left:4.0pt;padding-right:4.0pt;">
<p id="S4.T2.5.5.5.2.1" class="ltx_p ltx_align_top"><span id="S4.T2.5.5.5.2.1.1" class="ltx_text" style="font-size:90%;">3D Analysis</span></p>
</td>
<td id="S4.T2.5.5.5.3" class="ltx_td ltx_align_justify ltx_border_t" style="width:56.9pt;padding-left:4.0pt;padding-right:4.0pt;">
<span id="S4.T2.5.5.5.3.1" class="ltx_inline-block ltx_parbox ltx_align_top" style="width:54.1pt;">
<span id="S4.T2.5.5.5.3.1.1" class="ltx_p"><span id="S4.T2.5.5.5.3.1.1.1" class="ltx_text" style="font-size:90%;">Point Transformer </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.5.5.5.3.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib230" title="" class="ltx_ref">230</a><span id="S4.T2.5.5.5.3.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
<span id="S4.T2.5.5.5.3.1.2" class="ltx_p"><span id="S4.T2.5.5.5.3.1.2.1" class="ltx_text" style="font-size:90%;">arXiv’20</span></span>
</span>
</td>
<td id="S4.T2.5.5.5.4" class="ltx_td ltx_align_justify ltx_border_t" style="width:42.7pt;padding-left:4.0pt;padding-right:4.0pt;">
<span id="S4.T2.5.5.5.4.1" class="ltx_inline-block ltx_parbox ltx_align_top" style="width:54.1pt;">
<span id="S4.T2.5.5.5.4.1.1" class="ltx_p"><span id="S4.T2.5.5.5.4.1.1.1" class="ltx_text" style="font-size:90%;">Top-1 Acc.</span></span>
<span id="S4.T2.5.5.5.4.1.2" class="ltx_p"><span id="S4.T2.5.5.5.4.1.2.1" class="ltx_text" style="font-size:90%;">IoU</span></span>
</span>
</td>
<td id="S4.T2.5.5.5.5" class="ltx_td ltx_align_justify ltx_border_t" style="width:42.7pt;padding-left:4.0pt;padding-right:4.0pt;">
<span id="S4.T2.5.5.5.5.1" class="ltx_inline-block ltx_parbox ltx_align_top" style="width:54.1pt;">
<span id="S4.T2.5.5.5.5.1.1" class="ltx_p"><span id="S4.T2.5.5.5.5.1.1.1" class="ltx_text" style="font-size:90%;">ModelNet40 </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.5.5.5.5.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib232" title="" class="ltx_ref">232</a><span id="S4.T2.5.5.5.5.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td id="S4.T2.5.5.5.6" class="ltx_td ltx_align_justify ltx_border_t" style="width:42.7pt;padding-left:4.0pt;padding-right:4.0pt;">
<span id="S4.T2.5.5.5.6.1" class="ltx_inline-block ltx_parbox ltx_align_top" style="width:54.1pt;">
<span id="S4.T2.5.5.5.6.1.1" class="ltx_p"><span id="S4.T2.5.5.5.6.1.1.1" class="ltx_text" style="font-size:90%;">92.8</span></span>
<span id="S4.T2.5.5.5.6.1.2" class="ltx_p"><span id="S4.T2.5.5.5.6.1.2.1" class="ltx_text" style="font-size:90%;">85.9</span></span>
</span>
</td>
<td id="S4.T2.5.5.5.7" class="ltx_td ltx_align_justify ltx_border_t" style="width:142.3pt;padding-left:4.0pt;padding-right:4.0pt;">
<p id="S4.T2.5.5.5.7.1" class="ltx_p ltx_align_top"><span id="S4.T2.5.5.5.7.1.1" class="ltx_text" style="font-size:90%;"></span><span id="S4.T2.5.5.5.7.1.2" class="ltx_text ltx_font_bold" style="font-size:90%;">a)</span><span id="S4.T2.5.5.5.7.1.3" class="ltx_text" style="font-size:90%;"> Transformer based attention capable to process unordered and unstructured point sets, </span><span id="S4.T2.5.5.5.7.1.4" class="ltx_text ltx_font_bold" style="font-size:90%;">b)</span><span id="S4.T2.5.5.5.7.1.5" class="ltx_text" style="font-size:90%;"> Permutation invariant architecture.</span></p>
</td>
<td id="S4.T2.5.5.5.1" class="ltx_td ltx_align_justify ltx_border_t" style="width:142.3pt;padding-left:4.0pt;padding-right:4.0pt;">
<p id="S4.T2.5.5.5.1.1.1" class="ltx_p ltx_align_top"><span id="S4.T2.5.5.5.1.1.1.1" class="ltx_text" style="font-size:90%;"></span><span id="S4.T2.5.5.5.1.1.1.2" class="ltx_text ltx_font_bold" style="font-size:90%;">a)</span><span id="S4.T2.5.5.5.1.1.1.3" class="ltx_text" style="font-size:90%;"> Only moderate improvements over previous SOTA, </span><span id="S4.T2.5.5.5.1.1.1.4" class="ltx_text ltx_font_bold" style="font-size:90%;">b)</span><span id="S4.T2.5.5.5.1.1.1.5" class="ltx_text" style="font-size:90%;"> Large number of trainable parameters around 6</span><math id="S4.T2.5.5.5.1.1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T2.5.5.5.1.1.1.m1.1a"><mo mathsize="90%" id="S4.T2.5.5.5.1.1.1.m1.1.1" xref="S4.T2.5.5.5.1.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T2.5.5.5.1.1.1.m1.1b"><times id="S4.T2.5.5.5.1.1.1.m1.1.1.cmml" xref="S4.T2.5.5.5.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.5.5.5.1.1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T2.5.5.5.1.1.1.m1.1d">×</annotation></semantics></math><span id="S4.T2.5.5.5.1.1.1.6" class="ltx_text" style="font-size:90%;"> higher than PointNet++ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.5.5.5.1.1.1.7.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib242" title="" class="ltx_ref">242</a><span id="S4.T2.5.5.5.1.1.1.8.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.T2.5.5.5.1.1.1.9" class="ltx_text" style="font-size:90%;">.</span></p>
</td>
</tr>
<tr id="S4.T2.5.5.15.9" class="ltx_tr">
<td id="S4.T2.5.5.15.9.1" class="ltx_td ltx_border_bb" style="width:42.7pt;padding-left:4.0pt;padding-right:4.0pt;"></td>
<td id="S4.T2.5.5.15.9.2" class="ltx_td ltx_align_justify ltx_border_bb ltx_border_t" style="width:56.9pt;padding-left:4.0pt;padding-right:4.0pt;">
<span id="S4.T2.5.5.15.9.2.1" class="ltx_inline-block ltx_parbox ltx_align_top" style="width:54.1pt;">
<span id="S4.T2.5.5.15.9.2.1.1" class="ltx_p"><span id="S4.T2.5.5.15.9.2.1.1.1" class="ltx_text" style="font-size:90%;">METRO </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.5.5.15.9.2.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib45" title="" class="ltx_ref">45</a><span id="S4.T2.5.5.15.9.2.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.T2.5.5.15.9.2.1.1.4" class="ltx_text" style="font-size:90%;"></span></span>
<span id="S4.T2.5.5.15.9.2.1.2" class="ltx_p"><span id="S4.T2.5.5.15.9.2.1.2.1" class="ltx_text" style="font-size:90%;">arXiv’20</span></span>
</span>
</td>
<td id="S4.T2.5.5.15.9.3" class="ltx_td ltx_align_justify ltx_border_bb ltx_border_t" style="width:42.7pt;padding-left:4.0pt;padding-right:4.0pt;">
<span id="S4.T2.5.5.15.9.3.1" class="ltx_inline-block ltx_parbox ltx_align_top" style="width:54.1pt;">
<span id="S4.T2.5.5.15.9.3.1.1" class="ltx_p"><span id="S4.T2.5.5.15.9.3.1.1.1" class="ltx_text" style="font-size:90%;">MPJPE</span></span>
<span id="S4.T2.5.5.15.9.3.1.2" class="ltx_p"><span id="S4.T2.5.5.15.9.3.1.2.1" class="ltx_text" style="font-size:90%;">PA-MPJPE</span></span>
<span id="S4.T2.5.5.15.9.3.1.3" class="ltx_p"><span id="S4.T2.5.5.15.9.3.1.3.1" class="ltx_text" style="font-size:90%;">MPVE</span></span>
</span>
</td>
<td id="S4.T2.5.5.15.9.4" class="ltx_td ltx_align_justify ltx_border_bb ltx_border_t" style="width:42.7pt;padding-left:4.0pt;padding-right:4.0pt;">
<p id="S4.T2.5.5.15.9.4.1" class="ltx_p ltx_align_top"><span id="S4.T2.5.5.15.9.4.1.1" class="ltx_text" style="font-size:90%;">3DPW </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.5.5.15.9.4.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib235" title="" class="ltx_ref">235</a><span id="S4.T2.5.5.15.9.4.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite></p>
</td>
<td id="S4.T2.5.5.15.9.5" class="ltx_td ltx_align_justify ltx_border_bb ltx_border_t" style="width:42.7pt;padding-left:4.0pt;padding-right:4.0pt;">
<span id="S4.T2.5.5.15.9.5.1" class="ltx_inline-block ltx_parbox ltx_align_top" style="width:54.1pt;">
<span id="S4.T2.5.5.15.9.5.1.1" class="ltx_p"><span id="S4.T2.5.5.15.9.5.1.1.1" class="ltx_text" style="font-size:90%;">77.1</span></span>
<span id="S4.T2.5.5.15.9.5.1.2" class="ltx_p"><span id="S4.T2.5.5.15.9.5.1.2.1" class="ltx_text" style="font-size:90%;">47.9</span></span>
<span id="S4.T2.5.5.15.9.5.1.3" class="ltx_p"><span id="S4.T2.5.5.15.9.5.1.3.1" class="ltx_text" style="font-size:90%;">88.2</span></span>
</span>
</td>
<td id="S4.T2.5.5.15.9.6" class="ltx_td ltx_align_justify ltx_border_bb ltx_border_t" style="width:142.3pt;padding-left:4.0pt;padding-right:4.0pt;">
<p id="S4.T2.5.5.15.9.6.1" class="ltx_p ltx_align_top"><span id="S4.T2.5.5.15.9.6.1.1" class="ltx_text" style="font-size:90%;"></span><span id="S4.T2.5.5.15.9.6.1.2" class="ltx_text ltx_font_bold" style="font-size:90%;">a)</span><span id="S4.T2.5.5.15.9.6.1.3" class="ltx_text" style="font-size:90%;"> Does not depend on parametric mesh models so easily extendable to different objects, </span><span id="S4.T2.5.5.15.9.6.1.4" class="ltx_text ltx_font_bold" style="font-size:90%;">b)</span><span id="S4.T2.5.5.15.9.6.1.5" class="ltx_text" style="font-size:90%;"> Achieves SOTA results using Transformers.</span></p>
</td>
<td id="S4.T2.5.5.15.9.7" class="ltx_td ltx_align_justify ltx_border_bb ltx_border_t" style="width:142.3pt;padding-left:4.0pt;padding-right:4.0pt;">
<p id="S4.T2.5.5.15.9.7.1" class="ltx_p ltx_align_top"><span id="S4.T2.5.5.15.9.7.1.1" class="ltx_text" style="font-size:90%;">Dependent on hand-crafted network design.</span></p>
</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">TABLE II: </span>A summary of advantages and limitations of different Transformers based methods in different Tasks. (CT: Cross Transformers, AP: Average Precision, mAP: mean AP, IoU: Intersection over Union, FID: Fréchet inception distance, MPJPE: Mean Per Joint Position Error, MPVE: Mean Per Vertex Error).</figcaption>
</figure>
<figure id="S4.T3" class="ltx_table">
<div class="ltx_flex_figure ltx_flex_table">

<div class="ltx_flex_cell 
                  ltx_flex_size_1">
<div id="S4.T3.8" class="ltx_inline-block ltx_flex_size_1 ltx_align_center ltx_transformed_outer" style="width:276.2pt;height:474.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-24.4pt,41.9pt) scale(0.85,0.85) ;">
<table id="S4.T3.8.8" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T3.8.8.9.1" class="ltx_tr" style="background-color:#E6E6E6;">
<th id="S4.T3.8.8.9.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt"><span id="S4.T3.8.8.9.1.1.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">Method</span></th>
<td id="S4.T3.8.8.9.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T3.8.8.9.1.2.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">#Param (M)</span></td>
<td id="S4.T3.8.8.9.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T3.8.8.9.1.3.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">GFLOPs</span></td>
<td id="S4.T3.8.8.9.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T3.8.8.9.1.4.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">Top-1 Acc (%)</span></td>
</tr>
<tr id="S4.T3.1.1.1" class="ltx_tr">
<th id="S4.T3.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">
<span id="S4.T3.1.1.1.1.1" class="ltx_text" style="font-size:90%;">ResNet18 </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.1.1.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib67" title="" class="ltx_ref">67</a><span id="S4.T3.1.1.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><math id="S4.T3.1.1.1.1.m1.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S4.T3.1.1.1.1.m1.1a"><mo mathsize="90%" id="S4.T3.1.1.1.1.m1.1.1" xref="S4.T3.1.1.1.1.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.1.m1.1b"><ci id="S4.T3.1.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.1.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.1.m1.1c">\star</annotation><annotation encoding="application/x-llamapun" id="S4.T3.1.1.1.1.m1.1d">⋆</annotation></semantics></math>
</th>
<td id="S4.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.1.1.2.1" class="ltx_text" style="font-size:90%;">11.7</span></td>
<td id="S4.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.1.1.3.1" class="ltx_text" style="font-size:90%;">1.8</span></td>
<td id="S4.T3.1.1.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.1.1.4.1" class="ltx_text" style="font-size:90%;">69.8</span></td>
</tr>
<tr id="S4.T3.2.2.2" class="ltx_tr">
<th id="S4.T3.2.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T3.2.2.2.1.1" class="ltx_text" style="font-size:90%;">EfficientNet-B3 </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.2.2.2.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib87" title="" class="ltx_ref">87</a><span id="S4.T3.2.2.2.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><math id="S4.T3.2.2.2.1.m1.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S4.T3.2.2.2.1.m1.1a"><mo mathsize="90%" id="S4.T3.2.2.2.1.m1.1.1" xref="S4.T3.2.2.2.1.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S4.T3.2.2.2.1.m1.1b"><ci id="S4.T3.2.2.2.1.m1.1.1.cmml" xref="S4.T3.2.2.2.1.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.2.2.1.m1.1c">\star</annotation><annotation encoding="application/x-llamapun" id="S4.T3.2.2.2.1.m1.1d">⋆</annotation></semantics></math>
</th>
<td id="S4.T3.2.2.2.2" class="ltx_td ltx_align_center"><span id="S4.T3.2.2.2.2.1" class="ltx_text" style="font-size:90%;">12.0</span></td>
<td id="S4.T3.2.2.2.3" class="ltx_td ltx_align_center"><span id="S4.T3.2.2.2.3.1" class="ltx_text" style="font-size:90%;">1.8</span></td>
<td id="S4.T3.2.2.2.4" class="ltx_td ltx_align_center"><span id="S4.T3.2.2.2.4.1" class="ltx_text" style="font-size:90%;">81.6</span></td>
</tr>
<tr id="S4.T3.8.8.10.2" class="ltx_tr">
<th id="S4.T3.8.8.10.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T3.8.8.10.2.1.1" class="ltx_text" style="font-size:90%;">DeiT-T  </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.8.8.10.2.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib12" title="" class="ltx_ref">12</a><span id="S4.T3.8.8.10.2.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T3.8.8.10.2.2" class="ltx_td ltx_align_center"><span id="S4.T3.8.8.10.2.2.1" class="ltx_text" style="font-size:90%;">5.7</span></td>
<td id="S4.T3.8.8.10.2.3" class="ltx_td ltx_align_center"><span id="S4.T3.8.8.10.2.3.1" class="ltx_text" style="font-size:90%;">1.3</span></td>
<td id="S4.T3.8.8.10.2.4" class="ltx_td ltx_align_center"><span id="S4.T3.8.8.10.2.4.1" class="ltx_text" style="font-size:90%;">72.2</span></td>
</tr>
<tr id="S4.T3.3.3.3" class="ltx_tr">
<th id="S4.T3.3.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T3.3.3.3.1.1" class="ltx_text" style="font-size:90%;">T2T-ViT</span><math id="S4.T3.3.3.3.1.m1.1" class="ltx_Math" alttext="{}_{t}" display="inline"><semantics id="S4.T3.3.3.3.1.m1.1a"><msub id="S4.T3.3.3.3.1.m1.1.1" xref="S4.T3.3.3.3.1.m1.1.1.cmml"><mi id="S4.T3.3.3.3.1.m1.1.1a" xref="S4.T3.3.3.3.1.m1.1.1.cmml"></mi><mi mathsize="90%" id="S4.T3.3.3.3.1.m1.1.1.1" xref="S4.T3.3.3.3.1.m1.1.1.1.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S4.T3.3.3.3.1.m1.1b"><apply id="S4.T3.3.3.3.1.m1.1.1.cmml" xref="S4.T3.3.3.3.1.m1.1.1"><ci id="S4.T3.3.3.3.1.m1.1.1.1.cmml" xref="S4.T3.3.3.3.1.m1.1.1.1">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.3.3.3.1.m1.1c">{}_{t}</annotation><annotation encoding="application/x-llamapun" id="S4.T3.3.3.3.1.m1.1d">start_FLOATSUBSCRIPT italic_t end_FLOATSUBSCRIPT</annotation></semantics></math><span id="S4.T3.3.3.3.1.2" class="ltx_text" style="font-size:90%;">-7 </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.3.3.3.1.3.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib35" title="" class="ltx_ref">35</a><span id="S4.T3.3.3.3.1.4.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T3.3.3.3.2" class="ltx_td ltx_align_center"><span id="S4.T3.3.3.3.2.1" class="ltx_text" style="font-size:90%;">5.0</span></td>
<td id="S4.T3.3.3.3.3" class="ltx_td ltx_align_center"><span id="S4.T3.3.3.3.3.1" class="ltx_text" style="font-size:90%;">1.3</span></td>
<td id="S4.T3.3.3.3.4" class="ltx_td ltx_align_center"><span id="S4.T3.3.3.3.4.1" class="ltx_text" style="font-size:90%;">71.7</span></td>
</tr>
<tr id="S4.T3.8.8.11.3" class="ltx_tr">
<th id="S4.T3.8.8.11.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T3.8.8.11.3.1.1" class="ltx_text" style="font-size:90%;">LocalViT-T </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.8.8.11.3.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib107" title="" class="ltx_ref">107</a><span id="S4.T3.8.8.11.3.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T3.8.8.11.3.2" class="ltx_td ltx_align_center"><span id="S4.T3.8.8.11.3.2.1" class="ltx_text" style="font-size:90%;">5.9</span></td>
<td id="S4.T3.8.8.11.3.3" class="ltx_td ltx_align_center"><span id="S4.T3.8.8.11.3.3.1" class="ltx_text" style="font-size:90%;">1.3</span></td>
<td id="S4.T3.8.8.11.3.4" class="ltx_td ltx_align_center"><span id="S4.T3.8.8.11.3.4.1" class="ltx_text" style="font-size:90%;">74.8</span></td>
</tr>
<tr id="S4.T3.8.8.12.4" class="ltx_tr">
<th id="S4.T3.8.8.12.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T3.8.8.12.4.1.1" class="ltx_text" style="font-size:90%;">CrossViT-T </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.8.8.12.4.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib104" title="" class="ltx_ref">104</a><span id="S4.T3.8.8.12.4.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T3.8.8.12.4.2" class="ltx_td ltx_align_center"><span id="S4.T3.8.8.12.4.2.1" class="ltx_text" style="font-size:90%;">6.9</span></td>
<td id="S4.T3.8.8.12.4.3" class="ltx_td ltx_align_center"><span id="S4.T3.8.8.12.4.3.1" class="ltx_text" style="font-size:90%;">1.6</span></td>
<td id="S4.T3.8.8.12.4.4" class="ltx_td ltx_align_center"><span id="S4.T3.8.8.12.4.4.1" class="ltx_text" style="font-size:90%;">73.4</span></td>
</tr>
<tr id="S4.T3.8.8.13.5" class="ltx_tr">
<th id="S4.T3.8.8.13.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T3.8.8.13.5.1.1" class="ltx_text" style="font-size:90%;">PVTv1-T </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.8.8.13.5.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib93" title="" class="ltx_ref">93</a><span id="S4.T3.8.8.13.5.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T3.8.8.13.5.2" class="ltx_td ltx_align_center"><span id="S4.T3.8.8.13.5.2.1" class="ltx_text" style="font-size:90%;">13.2</span></td>
<td id="S4.T3.8.8.13.5.3" class="ltx_td ltx_align_center"><span id="S4.T3.8.8.13.5.3.1" class="ltx_text" style="font-size:90%;">1.9</span></td>
<td id="S4.T3.8.8.13.5.4" class="ltx_td ltx_align_center"><span id="S4.T3.8.8.13.5.4.1" class="ltx_text" style="font-size:90%;">75.1</span></td>
</tr>
<tr id="S4.T3.8.8.14.6" class="ltx_tr">
<th id="S4.T3.8.8.14.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T3.8.8.14.6.1.1" class="ltx_text" style="font-size:90%;">ResT-Lite </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.8.8.14.6.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib110" title="" class="ltx_ref">110</a><span id="S4.T3.8.8.14.6.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T3.8.8.14.6.2" class="ltx_td ltx_align_center"><span id="S4.T3.8.8.14.6.2.1" class="ltx_text" style="font-size:90%;">10.5</span></td>
<td id="S4.T3.8.8.14.6.3" class="ltx_td ltx_align_center"><span id="S4.T3.8.8.14.6.3.1" class="ltx_text" style="font-size:90%;">1.4</span></td>
<td id="S4.T3.8.8.14.6.4" class="ltx_td ltx_align_center"><span id="S4.T3.8.8.14.6.4.1" class="ltx_text" style="font-size:90%;">77.2</span></td>
</tr>
<tr id="S4.T3.8.8.15.7" class="ltx_tr">
<th id="S4.T3.8.8.15.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T3.8.8.15.7.1.1" class="ltx_text" style="font-size:90%;">CaiT-XXX-24 </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.8.8.15.7.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib243" title="" class="ltx_ref">243</a><span id="S4.T3.8.8.15.7.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T3.8.8.15.7.2" class="ltx_td ltx_align_center"><span id="S4.T3.8.8.15.7.2.1" class="ltx_text" style="font-size:90%;">12.0</span></td>
<td id="S4.T3.8.8.15.7.3" class="ltx_td ltx_align_center"><span id="S4.T3.8.8.15.7.3.1" class="ltx_text" style="font-size:90%;">2.5</span></td>
<td id="S4.T3.8.8.15.7.4" class="ltx_td ltx_align_center"><span id="S4.T3.8.8.15.7.4.1" class="ltx_text" style="font-size:90%;">77.6</span></td>
</tr>
<tr id="S4.T3.8.8.16.8" class="ltx_tr">
<th id="S4.T3.8.8.16.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T3.8.8.16.8.1.1" class="ltx_text" style="font-size:90%;">PVTv2-B1 </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.8.8.16.8.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib97" title="" class="ltx_ref">97</a><span id="S4.T3.8.8.16.8.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T3.8.8.16.8.2" class="ltx_td ltx_align_center"><span id="S4.T3.8.8.16.8.2.1" class="ltx_text" style="font-size:90%;">13.1</span></td>
<td id="S4.T3.8.8.16.8.3" class="ltx_td ltx_align_center"><span id="S4.T3.8.8.16.8.3.1" class="ltx_text" style="font-size:90%;">2.1</span></td>
<td id="S4.T3.8.8.16.8.4" class="ltx_td ltx_align_center"><span id="S4.T3.8.8.16.8.4.1" class="ltx_text" style="font-size:90%;">78.7</span></td>
</tr>
<tr id="S4.T3.8.8.17.9" class="ltx_tr">
<th id="S4.T3.8.8.17.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T3.8.8.17.9.1.1" class="ltx_text" style="font-size:90%;">Lv-ViT-T </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.8.8.17.9.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib89" title="" class="ltx_ref">89</a><span id="S4.T3.8.8.17.9.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T3.8.8.17.9.2" class="ltx_td ltx_align_center"><span id="S4.T3.8.8.17.9.2.1" class="ltx_text" style="font-size:90%;">8.5</span></td>
<td id="S4.T3.8.8.17.9.3" class="ltx_td ltx_align_center"><span id="S4.T3.8.8.17.9.3.1" class="ltx_text" style="font-size:90%;">–</span></td>
<td id="S4.T3.8.8.17.9.4" class="ltx_td ltx_align_center"><span id="S4.T3.8.8.17.9.4.1" class="ltx_text" style="font-size:90%;">79.1</span></td>
</tr>
<tr id="S4.T3.8.8.18.10" class="ltx_tr">
<th id="S4.T3.8.8.18.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T3.8.8.18.10.1.1" class="ltx_text" style="font-size:90%;">RegionViT-T </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.8.8.18.10.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib100" title="" class="ltx_ref">100</a><span id="S4.T3.8.8.18.10.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T3.8.8.18.10.2" class="ltx_td ltx_align_center"><span id="S4.T3.8.8.18.10.2.1" class="ltx_text" style="font-size:90%;">13.8</span></td>
<td id="S4.T3.8.8.18.10.3" class="ltx_td ltx_align_center"><span id="S4.T3.8.8.18.10.3.1" class="ltx_text" style="font-size:90%;">2.4</span></td>
<td id="S4.T3.8.8.18.10.4" class="ltx_td ltx_align_center"><span id="S4.T3.8.8.18.10.4.1" class="ltx_text" style="font-size:90%;">80.4</span></td>
</tr>
<tr id="S4.T3.4.4.4" class="ltx_tr">
<th id="S4.T3.4.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">
<span id="S4.T3.4.4.4.1.1" class="ltx_text" style="font-size:90%;">ResNet50 </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.4.4.4.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib67" title="" class="ltx_ref">67</a><span id="S4.T3.4.4.4.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><math id="S4.T3.4.4.4.1.m1.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S4.T3.4.4.4.1.m1.1a"><mo mathsize="90%" id="S4.T3.4.4.4.1.m1.1.1" xref="S4.T3.4.4.4.1.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S4.T3.4.4.4.1.m1.1b"><ci id="S4.T3.4.4.4.1.m1.1.1.cmml" xref="S4.T3.4.4.4.1.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.4.4.4.1.m1.1c">\star</annotation><annotation encoding="application/x-llamapun" id="S4.T3.4.4.4.1.m1.1d">⋆</annotation></semantics></math>
</th>
<td id="S4.T3.4.4.4.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.4.4.4.2.1" class="ltx_text" style="font-size:90%;">25.6</span></td>
<td id="S4.T3.4.4.4.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.4.4.4.3.1" class="ltx_text" style="font-size:90%;">4.1</span></td>
<td id="S4.T3.4.4.4.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.4.4.4.4.1" class="ltx_text" style="font-size:90%;">76.1</span></td>
</tr>
<tr id="S4.T3.5.5.5" class="ltx_tr">
<th id="S4.T3.5.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T3.5.5.5.1.1" class="ltx_text" style="font-size:90%;">ResNeXt50-32x4d </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.5.5.5.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib244" title="" class="ltx_ref">244</a><span id="S4.T3.5.5.5.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><math id="S4.T3.5.5.5.1.m1.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S4.T3.5.5.5.1.m1.1a"><mo mathsize="90%" id="S4.T3.5.5.5.1.m1.1.1" xref="S4.T3.5.5.5.1.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S4.T3.5.5.5.1.m1.1b"><ci id="S4.T3.5.5.5.1.m1.1.1.cmml" xref="S4.T3.5.5.5.1.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.5.5.5.1.m1.1c">\star</annotation><annotation encoding="application/x-llamapun" id="S4.T3.5.5.5.1.m1.1d">⋆</annotation></semantics></math>
</th>
<td id="S4.T3.5.5.5.2" class="ltx_td ltx_align_center"><span id="S4.T3.5.5.5.2.1" class="ltx_text" style="font-size:90%;">25.0</span></td>
<td id="S4.T3.5.5.5.3" class="ltx_td ltx_align_center"><span id="S4.T3.5.5.5.3.1" class="ltx_text" style="font-size:90%;">4.3</span></td>
<td id="S4.T3.5.5.5.4" class="ltx_td ltx_align_center"><span id="S4.T3.5.5.5.4.1" class="ltx_text" style="font-size:90%;">77.6</span></td>
</tr>
<tr id="S4.T3.6.6.6" class="ltx_tr">
<th id="S4.T3.6.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T3.6.6.6.1.1" class="ltx_text" style="font-size:90%;">RegNetY-4G </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.6.6.6.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib86" title="" class="ltx_ref">86</a><span id="S4.T3.6.6.6.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><math id="S4.T3.6.6.6.1.m1.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S4.T3.6.6.6.1.m1.1a"><mo mathsize="90%" id="S4.T3.6.6.6.1.m1.1.1" xref="S4.T3.6.6.6.1.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S4.T3.6.6.6.1.m1.1b"><ci id="S4.T3.6.6.6.1.m1.1.1.cmml" xref="S4.T3.6.6.6.1.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.6.6.6.1.m1.1c">\star</annotation><annotation encoding="application/x-llamapun" id="S4.T3.6.6.6.1.m1.1d">⋆</annotation></semantics></math>
</th>
<td id="S4.T3.6.6.6.2" class="ltx_td ltx_align_center"><span id="S4.T3.6.6.6.2.1" class="ltx_text" style="font-size:90%;">21.0</span></td>
<td id="S4.T3.6.6.6.3" class="ltx_td ltx_align_center"><span id="S4.T3.6.6.6.3.1" class="ltx_text" style="font-size:90%;">4.0</span></td>
<td id="S4.T3.6.6.6.4" class="ltx_td ltx_align_center"><span id="S4.T3.6.6.6.4.1" class="ltx_text" style="font-size:90%;">80.0</span></td>
</tr>
<tr id="S4.T3.7.7.7" class="ltx_tr">
<th id="S4.T3.7.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T3.7.7.7.1.1" class="ltx_text" style="font-size:90%;">EfficientNet-B4 </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.7.7.7.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib87" title="" class="ltx_ref">87</a><span id="S4.T3.7.7.7.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><math id="S4.T3.7.7.7.1.m1.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S4.T3.7.7.7.1.m1.1a"><mo mathsize="90%" id="S4.T3.7.7.7.1.m1.1.1" xref="S4.T3.7.7.7.1.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S4.T3.7.7.7.1.m1.1b"><ci id="S4.T3.7.7.7.1.m1.1.1.cmml" xref="S4.T3.7.7.7.1.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.7.7.7.1.m1.1c">\star</annotation><annotation encoding="application/x-llamapun" id="S4.T3.7.7.7.1.m1.1d">⋆</annotation></semantics></math>
</th>
<td id="S4.T3.7.7.7.2" class="ltx_td ltx_align_center"><span id="S4.T3.7.7.7.2.1" class="ltx_text" style="font-size:90%;">19.0</span></td>
<td id="S4.T3.7.7.7.3" class="ltx_td ltx_align_center"><span id="S4.T3.7.7.7.3.1" class="ltx_text" style="font-size:90%;">4.2</span></td>
<td id="S4.T3.7.7.7.4" class="ltx_td ltx_align_center"><span id="S4.T3.7.7.7.4.1" class="ltx_text" style="font-size:90%;">82.9</span></td>
</tr>
<tr id="S4.T3.8.8.19.11" class="ltx_tr">
<th id="S4.T3.8.8.19.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T3.8.8.19.11.1.1" class="ltx_text" style="font-size:90%;">DeiT-S </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.8.8.19.11.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib12" title="" class="ltx_ref">12</a><span id="S4.T3.8.8.19.11.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T3.8.8.19.11.2" class="ltx_td ltx_align_center"><span id="S4.T3.8.8.19.11.2.1" class="ltx_text" style="font-size:90%;">22.1</span></td>
<td id="S4.T3.8.8.19.11.3" class="ltx_td ltx_align_center"><span id="S4.T3.8.8.19.11.3.1" class="ltx_text" style="font-size:90%;">4.6</span></td>
<td id="S4.T3.8.8.19.11.4" class="ltx_td ltx_align_center"><span id="S4.T3.8.8.19.11.4.1" class="ltx_text" style="font-size:90%;">79.9</span></td>
</tr>
<tr id="S4.T3.8.8.20.12" class="ltx_tr">
<th id="S4.T3.8.8.20.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T3.8.8.20.12.1.1" class="ltx_text" style="font-size:90%;">PVTv1-S </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.8.8.20.12.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib93" title="" class="ltx_ref">93</a><span id="S4.T3.8.8.20.12.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T3.8.8.20.12.2" class="ltx_td ltx_align_center"><span id="S4.T3.8.8.20.12.2.1" class="ltx_text" style="font-size:90%;">24.5</span></td>
<td id="S4.T3.8.8.20.12.3" class="ltx_td ltx_align_center"><span id="S4.T3.8.8.20.12.3.1" class="ltx_text" style="font-size:90%;">3.8</span></td>
<td id="S4.T3.8.8.20.12.4" class="ltx_td ltx_align_center"><span id="S4.T3.8.8.20.12.4.1" class="ltx_text" style="font-size:90%;">79.8</span></td>
</tr>
<tr id="S4.T3.8.8.21.13" class="ltx_tr">
<th id="S4.T3.8.8.21.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T3.8.8.21.13.1.1" class="ltx_text" style="font-size:90%;">LocalViT-S </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.8.8.21.13.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib107" title="" class="ltx_ref">107</a><span id="S4.T3.8.8.21.13.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T3.8.8.21.13.2" class="ltx_td ltx_align_center"><span id="S4.T3.8.8.21.13.2.1" class="ltx_text" style="font-size:90%;">22.4</span></td>
<td id="S4.T3.8.8.21.13.3" class="ltx_td ltx_align_center"><span id="S4.T3.8.8.21.13.3.1" class="ltx_text" style="font-size:90%;">4.6</span></td>
<td id="S4.T3.8.8.21.13.4" class="ltx_td ltx_align_center"><span id="S4.T3.8.8.21.13.4.1" class="ltx_text" style="font-size:90%;">80.8</span></td>
</tr>
<tr id="S4.T3.8.8.22.14" class="ltx_tr">
<th id="S4.T3.8.8.22.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T3.8.8.22.14.1.1" class="ltx_text" style="font-size:90%;">CrossViT-S </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.8.8.22.14.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib104" title="" class="ltx_ref">104</a><span id="S4.T3.8.8.22.14.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T3.8.8.22.14.2" class="ltx_td ltx_align_center"><span id="S4.T3.8.8.22.14.2.1" class="ltx_text" style="font-size:90%;">26.7</span></td>
<td id="S4.T3.8.8.22.14.3" class="ltx_td ltx_align_center"><span id="S4.T3.8.8.22.14.3.1" class="ltx_text" style="font-size:90%;">5.6</span></td>
<td id="S4.T3.8.8.22.14.4" class="ltx_td ltx_align_center"><span id="S4.T3.8.8.22.14.4.1" class="ltx_text" style="font-size:90%;">81.0</span></td>
</tr>
<tr id="S4.T3.8.8.23.15" class="ltx_tr">
<th id="S4.T3.8.8.23.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T3.8.8.23.15.1.1" class="ltx_text" style="font-size:90%;">TNT-S </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.8.8.23.15.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib88" title="" class="ltx_ref">88</a><span id="S4.T3.8.8.23.15.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T3.8.8.23.15.2" class="ltx_td ltx_align_center"><span id="S4.T3.8.8.23.15.2.1" class="ltx_text" style="font-size:90%;">23.8</span></td>
<td id="S4.T3.8.8.23.15.3" class="ltx_td ltx_align_center"><span id="S4.T3.8.8.23.15.3.1" class="ltx_text" style="font-size:90%;">5.2</span></td>
<td id="S4.T3.8.8.23.15.4" class="ltx_td ltx_align_center"><span id="S4.T3.8.8.23.15.4.1" class="ltx_text" style="font-size:90%;">81.3</span></td>
</tr>
<tr id="S4.T3.8.8.24.16" class="ltx_tr">
<th id="S4.T3.8.8.24.16.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T3.8.8.24.16.1.1" class="ltx_text" style="font-size:90%;">Swin-T </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.8.8.24.16.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib36" title="" class="ltx_ref">36</a><span id="S4.T3.8.8.24.16.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T3.8.8.24.16.2" class="ltx_td ltx_align_center"><span id="S4.T3.8.8.24.16.2.1" class="ltx_text" style="font-size:90%;">29.0</span></td>
<td id="S4.T3.8.8.24.16.3" class="ltx_td ltx_align_center"><span id="S4.T3.8.8.24.16.3.1" class="ltx_text" style="font-size:90%;">4.5</span></td>
<td id="S4.T3.8.8.24.16.4" class="ltx_td ltx_align_center"><span id="S4.T3.8.8.24.16.4.1" class="ltx_text" style="font-size:90%;">81.3</span></td>
</tr>
<tr id="S4.T3.8.8.25.17" class="ltx_tr">
<th id="S4.T3.8.8.25.17.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T3.8.8.25.17.1.1" class="ltx_text" style="font-size:90%;">NesT-T </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.8.8.25.17.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib111" title="" class="ltx_ref">111</a><span id="S4.T3.8.8.25.17.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T3.8.8.25.17.2" class="ltx_td ltx_align_center"><span id="S4.T3.8.8.25.17.2.1" class="ltx_text" style="font-size:90%;">17.0</span></td>
<td id="S4.T3.8.8.25.17.3" class="ltx_td ltx_align_center"><span id="S4.T3.8.8.25.17.3.1" class="ltx_text" style="font-size:90%;">5.8</span></td>
<td id="S4.T3.8.8.25.17.4" class="ltx_td ltx_align_center"><span id="S4.T3.8.8.25.17.4.1" class="ltx_text" style="font-size:90%;">81.5</span></td>
</tr>
<tr id="S4.T3.8.8.8" class="ltx_tr">
<th id="S4.T3.8.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T3.8.8.8.1.1" class="ltx_text" style="font-size:90%;">T2T-ViT</span><math id="S4.T3.8.8.8.1.m1.1" class="ltx_Math" alttext="{}_{t}" display="inline"><semantics id="S4.T3.8.8.8.1.m1.1a"><msub id="S4.T3.8.8.8.1.m1.1.1" xref="S4.T3.8.8.8.1.m1.1.1.cmml"><mi id="S4.T3.8.8.8.1.m1.1.1a" xref="S4.T3.8.8.8.1.m1.1.1.cmml"></mi><mi mathsize="90%" id="S4.T3.8.8.8.1.m1.1.1.1" xref="S4.T3.8.8.8.1.m1.1.1.1.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S4.T3.8.8.8.1.m1.1b"><apply id="S4.T3.8.8.8.1.m1.1.1.cmml" xref="S4.T3.8.8.8.1.m1.1.1"><ci id="S4.T3.8.8.8.1.m1.1.1.1.cmml" xref="S4.T3.8.8.8.1.m1.1.1.1">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.8.8.8.1.m1.1c">{}_{t}</annotation><annotation encoding="application/x-llamapun" id="S4.T3.8.8.8.1.m1.1d">start_FLOATSUBSCRIPT italic_t end_FLOATSUBSCRIPT</annotation></semantics></math><span id="S4.T3.8.8.8.1.2" class="ltx_text" style="font-size:90%;">-14 </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.8.8.8.1.3.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib35" title="" class="ltx_ref">35</a><span id="S4.T3.8.8.8.1.4.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T3.8.8.8.2" class="ltx_td ltx_align_center"><span id="S4.T3.8.8.8.2.1" class="ltx_text" style="font-size:90%;">21.5</span></td>
<td id="S4.T3.8.8.8.3" class="ltx_td ltx_align_center"><span id="S4.T3.8.8.8.3.1" class="ltx_text" style="font-size:90%;">5.2</span></td>
<td id="S4.T3.8.8.8.4" class="ltx_td ltx_align_center"><span id="S4.T3.8.8.8.4.1" class="ltx_text" style="font-size:90%;">81.5</span></td>
</tr>
<tr id="S4.T3.8.8.26.18" class="ltx_tr">
<th id="S4.T3.8.8.26.18.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T3.8.8.26.18.1.1" class="ltx_text" style="font-size:90%;">CvT-13 </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.8.8.26.18.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib96" title="" class="ltx_ref">96</a><span id="S4.T3.8.8.26.18.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T3.8.8.26.18.2" class="ltx_td ltx_align_center"><span id="S4.T3.8.8.26.18.2.1" class="ltx_text" style="font-size:90%;">20.0</span></td>
<td id="S4.T3.8.8.26.18.3" class="ltx_td ltx_align_center"><span id="S4.T3.8.8.26.18.3.1" class="ltx_text" style="font-size:90%;">4.5</span></td>
<td id="S4.T3.8.8.26.18.4" class="ltx_td ltx_align_center"><span id="S4.T3.8.8.26.18.4.1" class="ltx_text" style="font-size:90%;">81.6</span></td>
</tr>
<tr id="S4.T3.8.8.27.19" class="ltx_tr">
<th id="S4.T3.8.8.27.19.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T3.8.8.27.19.1.1" class="ltx_text" style="font-size:90%;">ResT-B </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.8.8.27.19.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib110" title="" class="ltx_ref">110</a><span id="S4.T3.8.8.27.19.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T3.8.8.27.19.2" class="ltx_td ltx_align_center"><span id="S4.T3.8.8.27.19.2.1" class="ltx_text" style="font-size:90%;">30.3</span></td>
<td id="S4.T3.8.8.27.19.3" class="ltx_td ltx_align_center"><span id="S4.T3.8.8.27.19.3.1" class="ltx_text" style="font-size:90%;">4.3</span></td>
<td id="S4.T3.8.8.27.19.4" class="ltx_td ltx_align_center"><span id="S4.T3.8.8.27.19.4.1" class="ltx_text" style="font-size:90%;">81.6</span></td>
</tr>
<tr id="S4.T3.8.8.28.20" class="ltx_tr">
<th id="S4.T3.8.8.28.20.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T3.8.8.28.20.1.1" class="ltx_text" style="font-size:90%;">Twins-SVT-S </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.8.8.28.20.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib37" title="" class="ltx_ref">37</a><span id="S4.T3.8.8.28.20.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T3.8.8.28.20.2" class="ltx_td ltx_align_center"><span id="S4.T3.8.8.28.20.2.1" class="ltx_text" style="font-size:90%;">24.0</span></td>
<td id="S4.T3.8.8.28.20.3" class="ltx_td ltx_align_center"><span id="S4.T3.8.8.28.20.3.1" class="ltx_text" style="font-size:90%;">2.8</span></td>
<td id="S4.T3.8.8.28.20.4" class="ltx_td ltx_align_center"><span id="S4.T3.8.8.28.20.4.1" class="ltx_text" style="font-size:90%;">81.7</span></td>
</tr>
<tr id="S4.T3.8.8.29.21" class="ltx_tr">
<th id="S4.T3.8.8.29.21.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T3.8.8.29.21.1.1" class="ltx_text" style="font-size:90%;">PVTv2-B2-Li </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.8.8.29.21.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib97" title="" class="ltx_ref">97</a><span id="S4.T3.8.8.29.21.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T3.8.8.29.21.2" class="ltx_td ltx_align_center"><span id="S4.T3.8.8.29.21.2.1" class="ltx_text" style="font-size:90%;">22.6</span></td>
<td id="S4.T3.8.8.29.21.3" class="ltx_td ltx_align_center"><span id="S4.T3.8.8.29.21.3.1" class="ltx_text" style="font-size:90%;">3.9</span></td>
<td id="S4.T3.8.8.29.21.4" class="ltx_td ltx_align_center"><span id="S4.T3.8.8.29.21.4.1" class="ltx_text" style="font-size:90%;">82.1</span></td>
</tr>
<tr id="S4.T3.8.8.30.22" class="ltx_tr">
<th id="S4.T3.8.8.30.22.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T3.8.8.30.22.1.1" class="ltx_text" style="font-size:90%;">RegionViT-S </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.8.8.30.22.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib100" title="" class="ltx_ref">100</a><span id="S4.T3.8.8.30.22.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T3.8.8.30.22.2" class="ltx_td ltx_align_center"><span id="S4.T3.8.8.30.22.2.1" class="ltx_text" style="font-size:90%;">30.6</span></td>
<td id="S4.T3.8.8.30.22.3" class="ltx_td ltx_align_center"><span id="S4.T3.8.8.30.22.3.1" class="ltx_text" style="font-size:90%;">5.6</span></td>
<td id="S4.T3.8.8.30.22.4" class="ltx_td ltx_align_center"><span id="S4.T3.8.8.30.22.4.1" class="ltx_text" style="font-size:90%;">82.5</span></td>
</tr>
<tr id="S4.T3.8.8.31.23" class="ltx_tr">
<th id="S4.T3.8.8.31.23.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">
<span id="S4.T3.8.8.31.23.1.1" class="ltx_text" style="font-size:90%;">Lv-ViT-S </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.8.8.31.23.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib89" title="" class="ltx_ref">89</a><span id="S4.T3.8.8.31.23.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T3.8.8.31.23.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T3.8.8.31.23.2.1" class="ltx_text" style="font-size:90%;">26.0</span></td>
<td id="S4.T3.8.8.31.23.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T3.8.8.31.23.3.1" class="ltx_text" style="font-size:90%;">6.6</span></td>
<td id="S4.T3.8.8.31.23.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T3.8.8.31.23.4.1" class="ltx_text" style="font-size:90%;">83.3</span></td>
</tr>
</tbody>
</table>
</span></div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell 
                  ltx_flex_size_1">
<div id="S4.T3.18" class="ltx_inline-block ltx_flex_size_1 ltx_align_center ltx_transformed_outer" style="width:287.6pt;height:467.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-22.4pt,36.4pt) scale(0.865,0.865) ;">
<table id="S4.T3.18.10" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.18.10.11.1" class="ltx_tr" style="background-color:#E6E6E6;">
<th id="S4.T3.18.10.11.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S4.T3.18.10.11.1.1.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">Method</span></th>
<th id="S4.T3.18.10.11.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T3.18.10.11.1.2.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">#Param (M)</span></th>
<th id="S4.T3.18.10.11.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T3.18.10.11.1.3.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">GFLOPs</span></th>
<th id="S4.T3.18.10.11.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T3.18.10.11.1.4.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">Top-1 Acc (%)</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.9.1.1" class="ltx_tr">
<th id="S4.T3.9.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">
<span id="S4.T3.9.1.1.1.1" class="ltx_text" style="font-size:90%;">ResNet101 </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.9.1.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib67" title="" class="ltx_ref">67</a><span id="S4.T3.9.1.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.T3.9.1.1.1.4" class="ltx_text" style="font-size:90%;"> </span><math id="S4.T3.9.1.1.1.m1.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S4.T3.9.1.1.1.m1.1a"><mo mathsize="90%" id="S4.T3.9.1.1.1.m1.1.1" xref="S4.T3.9.1.1.1.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S4.T3.9.1.1.1.m1.1b"><ci id="S4.T3.9.1.1.1.m1.1.1.cmml" xref="S4.T3.9.1.1.1.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.9.1.1.1.m1.1c">\star</annotation><annotation encoding="application/x-llamapun" id="S4.T3.9.1.1.1.m1.1d">⋆</annotation></semantics></math>
</th>
<td id="S4.T3.9.1.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.9.1.1.2.1" class="ltx_text" style="font-size:90%;">44.7</span></td>
<td id="S4.T3.9.1.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.9.1.1.3.1" class="ltx_text" style="font-size:90%;">7.9</span></td>
<td id="S4.T3.9.1.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.9.1.1.4.1" class="ltx_text" style="font-size:90%;">77.4</span></td>
</tr>
<tr id="S4.T3.10.2.2" class="ltx_tr">
<th id="S4.T3.10.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T3.10.2.2.1.1" class="ltx_text" style="font-size:90%;">ResNeXt101-32x4d </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.10.2.2.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib244" title="" class="ltx_ref">244</a><span id="S4.T3.10.2.2.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><math id="S4.T3.10.2.2.1.m1.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S4.T3.10.2.2.1.m1.1a"><mo mathsize="90%" id="S4.T3.10.2.2.1.m1.1.1" xref="S4.T3.10.2.2.1.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S4.T3.10.2.2.1.m1.1b"><ci id="S4.T3.10.2.2.1.m1.1.1.cmml" xref="S4.T3.10.2.2.1.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.10.2.2.1.m1.1c">\star</annotation><annotation encoding="application/x-llamapun" id="S4.T3.10.2.2.1.m1.1d">⋆</annotation></semantics></math>
</th>
<td id="S4.T3.10.2.2.2" class="ltx_td ltx_align_center"><span id="S4.T3.10.2.2.2.1" class="ltx_text" style="font-size:90%;">44.2</span></td>
<td id="S4.T3.10.2.2.3" class="ltx_td ltx_align_center"><span id="S4.T3.10.2.2.3.1" class="ltx_text" style="font-size:90%;">8.0</span></td>
<td id="S4.T3.10.2.2.4" class="ltx_td ltx_align_center"><span id="S4.T3.10.2.2.4.1" class="ltx_text" style="font-size:90%;">78.8</span></td>
</tr>
<tr id="S4.T3.11.3.3" class="ltx_tr">
<th id="S4.T3.11.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T3.11.3.3.1.1" class="ltx_text" style="font-size:90%;">RegNetY-8G </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.11.3.3.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib86" title="" class="ltx_ref">86</a><span id="S4.T3.11.3.3.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><math id="S4.T3.11.3.3.1.m1.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S4.T3.11.3.3.1.m1.1a"><mo mathsize="90%" id="S4.T3.11.3.3.1.m1.1.1" xref="S4.T3.11.3.3.1.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S4.T3.11.3.3.1.m1.1b"><ci id="S4.T3.11.3.3.1.m1.1.1.cmml" xref="S4.T3.11.3.3.1.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.11.3.3.1.m1.1c">\star</annotation><annotation encoding="application/x-llamapun" id="S4.T3.11.3.3.1.m1.1d">⋆</annotation></semantics></math>
</th>
<td id="S4.T3.11.3.3.2" class="ltx_td ltx_align_center"><span id="S4.T3.11.3.3.2.1" class="ltx_text" style="font-size:90%;">39.0</span></td>
<td id="S4.T3.11.3.3.3" class="ltx_td ltx_align_center"><span id="S4.T3.11.3.3.3.1" class="ltx_text" style="font-size:90%;">8.0</span></td>
<td id="S4.T3.11.3.3.4" class="ltx_td ltx_align_center"><span id="S4.T3.11.3.3.4.1" class="ltx_text" style="font-size:90%;">81.7</span></td>
</tr>
<tr id="S4.T3.12.4.4" class="ltx_tr">
<th id="S4.T3.12.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T3.12.4.4.1.1" class="ltx_text" style="font-size:90%;">EfficientNet-B5 </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.12.4.4.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib87" title="" class="ltx_ref">87</a><span id="S4.T3.12.4.4.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.T3.12.4.4.1.4" class="ltx_text" style="font-size:90%;"> </span><math id="S4.T3.12.4.4.1.m1.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S4.T3.12.4.4.1.m1.1a"><mo mathsize="90%" id="S4.T3.12.4.4.1.m1.1.1" xref="S4.T3.12.4.4.1.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S4.T3.12.4.4.1.m1.1b"><ci id="S4.T3.12.4.4.1.m1.1.1.cmml" xref="S4.T3.12.4.4.1.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.12.4.4.1.m1.1c">\star</annotation><annotation encoding="application/x-llamapun" id="S4.T3.12.4.4.1.m1.1d">⋆</annotation></semantics></math>
</th>
<td id="S4.T3.12.4.4.2" class="ltx_td ltx_align_center"><span id="S4.T3.12.4.4.2.1" class="ltx_text" style="font-size:90%;">30.0</span></td>
<td id="S4.T3.12.4.4.3" class="ltx_td ltx_align_center"><span id="S4.T3.12.4.4.3.1" class="ltx_text" style="font-size:90%;">9.9</span></td>
<td id="S4.T3.12.4.4.4" class="ltx_td ltx_align_center"><span id="S4.T3.12.4.4.4.1" class="ltx_text" style="font-size:90%;">83.6</span></td>
</tr>
<tr id="S4.T3.18.10.12.1" class="ltx_tr">
<th id="S4.T3.18.10.12.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T3.18.10.12.1.1.1" class="ltx_text" style="font-size:90%;">CvT-21  </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.18.10.12.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib96" title="" class="ltx_ref">96</a><span id="S4.T3.18.10.12.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T3.18.10.12.1.2" class="ltx_td ltx_align_center"><span id="S4.T3.18.10.12.1.2.1" class="ltx_text" style="font-size:90%;">32.0</span></td>
<td id="S4.T3.18.10.12.1.3" class="ltx_td ltx_align_center"><span id="S4.T3.18.10.12.1.3.1" class="ltx_text" style="font-size:90%;">7.1</span></td>
<td id="S4.T3.18.10.12.1.4" class="ltx_td ltx_align_center"><span id="S4.T3.18.10.12.1.4.1" class="ltx_text" style="font-size:90%;">82.5</span></td>
</tr>
<tr id="S4.T3.18.10.13.2" class="ltx_tr">
<th id="S4.T3.18.10.13.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T3.18.10.13.2.1.1" class="ltx_text" style="font-size:90%;">CaiT-S-24 </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.18.10.13.2.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib243" title="" class="ltx_ref">243</a><span id="S4.T3.18.10.13.2.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T3.18.10.13.2.2" class="ltx_td ltx_align_center"><span id="S4.T3.18.10.13.2.2.1" class="ltx_text" style="font-size:90%;">32.2</span></td>
<td id="S4.T3.18.10.13.2.3" class="ltx_td ltx_align_center"><span id="S4.T3.18.10.13.2.3.1" class="ltx_text" style="font-size:90%;">9.4</span></td>
<td id="S4.T3.18.10.13.2.4" class="ltx_td ltx_align_center"><span id="S4.T3.18.10.13.2.4.1" class="ltx_text" style="font-size:90%;">82.7</span></td>
</tr>
<tr id="S4.T3.13.5.5" class="ltx_tr">
<th id="S4.T3.13.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T3.13.5.5.1.1" class="ltx_text" style="font-size:90%;">T2T-ViT</span><math id="S4.T3.13.5.5.1.m1.1" class="ltx_Math" alttext="{}_{t}" display="inline"><semantics id="S4.T3.13.5.5.1.m1.1a"><msub id="S4.T3.13.5.5.1.m1.1.1" xref="S4.T3.13.5.5.1.m1.1.1.cmml"><mi id="S4.T3.13.5.5.1.m1.1.1a" xref="S4.T3.13.5.5.1.m1.1.1.cmml"></mi><mi mathsize="90%" id="S4.T3.13.5.5.1.m1.1.1.1" xref="S4.T3.13.5.5.1.m1.1.1.1.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S4.T3.13.5.5.1.m1.1b"><apply id="S4.T3.13.5.5.1.m1.1.1.cmml" xref="S4.T3.13.5.5.1.m1.1.1"><ci id="S4.T3.13.5.5.1.m1.1.1.1.cmml" xref="S4.T3.13.5.5.1.m1.1.1.1">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.13.5.5.1.m1.1c">{}_{t}</annotation><annotation encoding="application/x-llamapun" id="S4.T3.13.5.5.1.m1.1d">start_FLOATSUBSCRIPT italic_t end_FLOATSUBSCRIPT</annotation></semantics></math><span id="S4.T3.13.5.5.1.2" class="ltx_text" style="font-size:90%;">-19 </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.13.5.5.1.3.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib35" title="" class="ltx_ref">35</a><span id="S4.T3.13.5.5.1.4.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T3.13.5.5.2" class="ltx_td ltx_align_center"><span id="S4.T3.13.5.5.2.1" class="ltx_text" style="font-size:90%;">39.0</span></td>
<td id="S4.T3.13.5.5.3" class="ltx_td ltx_align_center"><span id="S4.T3.13.5.5.3.1" class="ltx_text" style="font-size:90%;">9.8</span></td>
<td id="S4.T3.13.5.5.4" class="ltx_td ltx_align_center"><span id="S4.T3.13.5.5.4.1" class="ltx_text" style="font-size:90%;">81.4</span></td>
</tr>
<tr id="S4.T3.18.10.14.3" class="ltx_tr">
<th id="S4.T3.18.10.14.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T3.18.10.14.3.1.1" class="ltx_text" style="font-size:90%;">PVTv1-M </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.18.10.14.3.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib93" title="" class="ltx_ref">93</a><span id="S4.T3.18.10.14.3.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T3.18.10.14.3.2" class="ltx_td ltx_align_center"><span id="S4.T3.18.10.14.3.2.1" class="ltx_text" style="font-size:90%;">44.2</span></td>
<td id="S4.T3.18.10.14.3.3" class="ltx_td ltx_align_center"><span id="S4.T3.18.10.14.3.3.1" class="ltx_text" style="font-size:90%;">6.7</span></td>
<td id="S4.T3.18.10.14.3.4" class="ltx_td ltx_align_center"><span id="S4.T3.18.10.14.3.4.1" class="ltx_text" style="font-size:90%;">81.2</span></td>
</tr>
<tr id="S4.T3.18.10.15.4" class="ltx_tr">
<th id="S4.T3.18.10.15.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T3.18.10.15.4.1.1" class="ltx_text" style="font-size:90%;">PVTv2-B3 </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.18.10.15.4.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib97" title="" class="ltx_ref">97</a><span id="S4.T3.18.10.15.4.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T3.18.10.15.4.2" class="ltx_td ltx_align_center"><span id="S4.T3.18.10.15.4.2.1" class="ltx_text" style="font-size:90%;">45.2</span></td>
<td id="S4.T3.18.10.15.4.3" class="ltx_td ltx_align_center"><span id="S4.T3.18.10.15.4.3.1" class="ltx_text" style="font-size:90%;">6.9</span></td>
<td id="S4.T3.18.10.15.4.4" class="ltx_td ltx_align_center"><span id="S4.T3.18.10.15.4.4.1" class="ltx_text" style="font-size:90%;">83.2</span></td>
</tr>
<tr id="S4.T3.18.10.16.5" class="ltx_tr">
<th id="S4.T3.18.10.16.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T3.18.10.16.5.1.1" class="ltx_text" style="font-size:90%;">NesT-S </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.18.10.16.5.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib111" title="" class="ltx_ref">111</a><span id="S4.T3.18.10.16.5.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T3.18.10.16.5.2" class="ltx_td ltx_align_center"><span id="S4.T3.18.10.16.5.2.1" class="ltx_text" style="font-size:90%;">38.0</span></td>
<td id="S4.T3.18.10.16.5.3" class="ltx_td ltx_align_center"><span id="S4.T3.18.10.16.5.3.1" class="ltx_text" style="font-size:90%;">10.4</span></td>
<td id="S4.T3.18.10.16.5.4" class="ltx_td ltx_align_center"><span id="S4.T3.18.10.16.5.4.1" class="ltx_text" style="font-size:90%;">83.3</span></td>
</tr>
<tr id="S4.T3.14.6.6" class="ltx_tr">
<th id="S4.T3.14.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">
<span id="S4.T3.14.6.6.1.1" class="ltx_text" style="font-size:90%;">ResNet152 </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.14.6.6.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib67" title="" class="ltx_ref">67</a><span id="S4.T3.14.6.6.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.T3.14.6.6.1.4" class="ltx_text" style="font-size:90%;"> </span><math id="S4.T3.14.6.6.1.m1.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S4.T3.14.6.6.1.m1.1a"><mo mathsize="90%" id="S4.T3.14.6.6.1.m1.1.1" xref="S4.T3.14.6.6.1.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S4.T3.14.6.6.1.m1.1b"><ci id="S4.T3.14.6.6.1.m1.1.1.cmml" xref="S4.T3.14.6.6.1.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.14.6.6.1.m1.1c">\star</annotation><annotation encoding="application/x-llamapun" id="S4.T3.14.6.6.1.m1.1d">⋆</annotation></semantics></math>
</th>
<td id="S4.T3.14.6.6.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.14.6.6.2.1" class="ltx_text" style="font-size:90%;">60.2</span></td>
<td id="S4.T3.14.6.6.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.14.6.6.3.1" class="ltx_text" style="font-size:90%;">11.6</span></td>
<td id="S4.T3.14.6.6.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.14.6.6.4.1" class="ltx_text" style="font-size:90%;">78.3</span></td>
</tr>
<tr id="S4.T3.18.10.17.6" class="ltx_tr">
<th id="S4.T3.18.10.17.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T3.18.10.17.6.1.1" class="ltx_text" style="font-size:90%;">CaiT-S-36 </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.18.10.17.6.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib243" title="" class="ltx_ref">243</a><span id="S4.T3.18.10.17.6.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T3.18.10.17.6.2" class="ltx_td ltx_align_center"><span id="S4.T3.18.10.17.6.2.1" class="ltx_text" style="font-size:90%;">48.0</span></td>
<td id="S4.T3.18.10.17.6.3" class="ltx_td ltx_align_center"><span id="S4.T3.18.10.17.6.3.1" class="ltx_text" style="font-size:90%;">13.9</span></td>
<td id="S4.T3.18.10.17.6.4" class="ltx_td ltx_align_center"><span id="S4.T3.18.10.17.6.4.1" class="ltx_text" style="font-size:90%;">83.3</span></td>
</tr>
<tr id="S4.T3.15.7.7" class="ltx_tr">
<th id="S4.T3.15.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T3.15.7.7.1.1" class="ltx_text" style="font-size:90%;">T2T-ViT</span><math id="S4.T3.15.7.7.1.m1.1" class="ltx_Math" alttext="{}_{t}" display="inline"><semantics id="S4.T3.15.7.7.1.m1.1a"><msub id="S4.T3.15.7.7.1.m1.1.1" xref="S4.T3.15.7.7.1.m1.1.1.cmml"><mi id="S4.T3.15.7.7.1.m1.1.1a" xref="S4.T3.15.7.7.1.m1.1.1.cmml"></mi><mi mathsize="90%" id="S4.T3.15.7.7.1.m1.1.1.1" xref="S4.T3.15.7.7.1.m1.1.1.1.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S4.T3.15.7.7.1.m1.1b"><apply id="S4.T3.15.7.7.1.m1.1.1.cmml" xref="S4.T3.15.7.7.1.m1.1.1"><ci id="S4.T3.15.7.7.1.m1.1.1.1.cmml" xref="S4.T3.15.7.7.1.m1.1.1.1">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.15.7.7.1.m1.1c">{}_{t}</annotation><annotation encoding="application/x-llamapun" id="S4.T3.15.7.7.1.m1.1d">start_FLOATSUBSCRIPT italic_t end_FLOATSUBSCRIPT</annotation></semantics></math><span id="S4.T3.15.7.7.1.2" class="ltx_text" style="font-size:90%;">-24 </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.15.7.7.1.3.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib35" title="" class="ltx_ref">35</a><span id="S4.T3.15.7.7.1.4.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T3.15.7.7.2" class="ltx_td ltx_align_center"><span id="S4.T3.15.7.7.2.1" class="ltx_text" style="font-size:90%;">64.0</span></td>
<td id="S4.T3.15.7.7.3" class="ltx_td ltx_align_center"><span id="S4.T3.15.7.7.3.1" class="ltx_text" style="font-size:90%;">15.0</span></td>
<td id="S4.T3.15.7.7.4" class="ltx_td ltx_align_center"><span id="S4.T3.15.7.7.4.1" class="ltx_text" style="font-size:90%;">82.2</span></td>
</tr>
<tr id="S4.T3.18.10.18.7" class="ltx_tr">
<th id="S4.T3.18.10.18.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T3.18.10.18.7.1.1" class="ltx_text" style="font-size:90%;">PVTv1-L </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.18.10.18.7.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib93" title="" class="ltx_ref">93</a><span id="S4.T3.18.10.18.7.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T3.18.10.18.7.2" class="ltx_td ltx_align_center"><span id="S4.T3.18.10.18.7.2.1" class="ltx_text" style="font-size:90%;">61.4</span></td>
<td id="S4.T3.18.10.18.7.3" class="ltx_td ltx_align_center"><span id="S4.T3.18.10.18.7.3.1" class="ltx_text" style="font-size:90%;">9.8</span></td>
<td id="S4.T3.18.10.18.7.4" class="ltx_td ltx_align_center"><span id="S4.T3.18.10.18.7.4.1" class="ltx_text" style="font-size:90%;">81.7</span></td>
</tr>
<tr id="S4.T3.18.10.19.8" class="ltx_tr">
<th id="S4.T3.18.10.19.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T3.18.10.19.8.1.1" class="ltx_text" style="font-size:90%;">TNT-B </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.18.10.19.8.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib88" title="" class="ltx_ref">88</a><span id="S4.T3.18.10.19.8.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T3.18.10.19.8.2" class="ltx_td ltx_align_center"><span id="S4.T3.18.10.19.8.2.1" class="ltx_text" style="font-size:90%;">66.0</span></td>
<td id="S4.T3.18.10.19.8.3" class="ltx_td ltx_align_center"><span id="S4.T3.18.10.19.8.3.1" class="ltx_text" style="font-size:90%;">14.1</span></td>
<td id="S4.T3.18.10.19.8.4" class="ltx_td ltx_align_center"><span id="S4.T3.18.10.19.8.4.1" class="ltx_text" style="font-size:90%;">82.8</span></td>
</tr>
<tr id="S4.T3.18.10.20.9" class="ltx_tr">
<th id="S4.T3.18.10.20.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T3.18.10.20.9.1.1" class="ltx_text" style="font-size:90%;">Swin-S </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.18.10.20.9.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib36" title="" class="ltx_ref">36</a><span id="S4.T3.18.10.20.9.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T3.18.10.20.9.2" class="ltx_td ltx_align_center"><span id="S4.T3.18.10.20.9.2.1" class="ltx_text" style="font-size:90%;">50.0</span></td>
<td id="S4.T3.18.10.20.9.3" class="ltx_td ltx_align_center"><span id="S4.T3.18.10.20.9.3.1" class="ltx_text" style="font-size:90%;">8.7</span></td>
<td id="S4.T3.18.10.20.9.4" class="ltx_td ltx_align_center"><span id="S4.T3.18.10.20.9.4.1" class="ltx_text" style="font-size:90%;">83.0</span></td>
</tr>
<tr id="S4.T3.18.10.21.10" class="ltx_tr">
<th id="S4.T3.18.10.21.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T3.18.10.21.10.1.1" class="ltx_text" style="font-size:90%;">Twins-SVT-B </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.18.10.21.10.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib37" title="" class="ltx_ref">37</a><span id="S4.T3.18.10.21.10.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T3.18.10.21.10.2" class="ltx_td ltx_align_center"><span id="S4.T3.18.10.21.10.2.1" class="ltx_text" style="font-size:90%;">56.0</span></td>
<td id="S4.T3.18.10.21.10.3" class="ltx_td ltx_align_center"><span id="S4.T3.18.10.21.10.3.1" class="ltx_text" style="font-size:90%;">8.3</span></td>
<td id="S4.T3.18.10.21.10.4" class="ltx_td ltx_align_center"><span id="S4.T3.18.10.21.10.4.1" class="ltx_text" style="font-size:90%;">83.2</span></td>
</tr>
<tr id="S4.T3.18.10.22.11" class="ltx_tr">
<th id="S4.T3.18.10.22.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T3.18.10.22.11.1.1" class="ltx_text" style="font-size:90%;">RegionViT-B </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.18.10.22.11.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib100" title="" class="ltx_ref">100</a><span id="S4.T3.18.10.22.11.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T3.18.10.22.11.2" class="ltx_td ltx_align_center"><span id="S4.T3.18.10.22.11.2.1" class="ltx_text" style="font-size:90%;">72.7</span></td>
<td id="S4.T3.18.10.22.11.3" class="ltx_td ltx_align_center"><span id="S4.T3.18.10.22.11.3.1" class="ltx_text" style="font-size:90%;">13.0</span></td>
<td id="S4.T3.18.10.22.11.4" class="ltx_td ltx_align_center"><span id="S4.T3.18.10.22.11.4.1" class="ltx_text" style="font-size:90%;">83.3</span></td>
</tr>
<tr id="S4.T3.18.10.23.12" class="ltx_tr">
<th id="S4.T3.18.10.23.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T3.18.10.23.12.1.1" class="ltx_text" style="font-size:90%;">PVTv2-B4 </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.18.10.23.12.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib97" title="" class="ltx_ref">97</a><span id="S4.T3.18.10.23.12.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T3.18.10.23.12.2" class="ltx_td ltx_align_center"><span id="S4.T3.18.10.23.12.2.1" class="ltx_text" style="font-size:90%;">62.6</span></td>
<td id="S4.T3.18.10.23.12.3" class="ltx_td ltx_align_center"><span id="S4.T3.18.10.23.12.3.1" class="ltx_text" style="font-size:90%;">10.1</span></td>
<td id="S4.T3.18.10.23.12.4" class="ltx_td ltx_align_center"><span id="S4.T3.18.10.23.12.4.1" class="ltx_text" style="font-size:90%;">83.6</span></td>
</tr>
<tr id="S4.T3.16.8.8" class="ltx_tr">
<th id="S4.T3.16.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">
<span id="S4.T3.16.8.8.1.1" class="ltx_text" style="font-size:90%;">ResNeXt101-64x4d </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.16.8.8.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib244" title="" class="ltx_ref">244</a><span id="S4.T3.16.8.8.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.T3.16.8.8.1.4" class="ltx_text" style="font-size:90%;"> </span><math id="S4.T3.16.8.8.1.m1.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S4.T3.16.8.8.1.m1.1a"><mo mathsize="90%" id="S4.T3.16.8.8.1.m1.1.1" xref="S4.T3.16.8.8.1.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S4.T3.16.8.8.1.m1.1b"><ci id="S4.T3.16.8.8.1.m1.1.1.cmml" xref="S4.T3.16.8.8.1.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.16.8.8.1.m1.1c">\star</annotation><annotation encoding="application/x-llamapun" id="S4.T3.16.8.8.1.m1.1d">⋆</annotation></semantics></math>
</th>
<td id="S4.T3.16.8.8.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.16.8.8.2.1" class="ltx_text" style="font-size:90%;">83.5</span></td>
<td id="S4.T3.16.8.8.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.16.8.8.3.1" class="ltx_text" style="font-size:90%;">15.6</span></td>
<td id="S4.T3.16.8.8.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.16.8.8.4.1" class="ltx_text" style="font-size:90%;">79.6</span></td>
</tr>
<tr id="S4.T3.17.9.9" class="ltx_tr">
<th id="S4.T3.17.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T3.17.9.9.1.1" class="ltx_text" style="font-size:90%;">RegNetY-16G </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.17.9.9.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib86" title="" class="ltx_ref">86</a><span id="S4.T3.17.9.9.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.T3.17.9.9.1.4" class="ltx_text" style="font-size:90%;"> </span><math id="S4.T3.17.9.9.1.m1.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S4.T3.17.9.9.1.m1.1a"><mo mathsize="90%" id="S4.T3.17.9.9.1.m1.1.1" xref="S4.T3.17.9.9.1.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S4.T3.17.9.9.1.m1.1b"><ci id="S4.T3.17.9.9.1.m1.1.1.cmml" xref="S4.T3.17.9.9.1.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.17.9.9.1.m1.1c">\star</annotation><annotation encoding="application/x-llamapun" id="S4.T3.17.9.9.1.m1.1d">⋆</annotation></semantics></math>
</th>
<td id="S4.T3.17.9.9.2" class="ltx_td ltx_align_center"><span id="S4.T3.17.9.9.2.1" class="ltx_text" style="font-size:90%;">84.0</span></td>
<td id="S4.T3.17.9.9.3" class="ltx_td ltx_align_center"><span id="S4.T3.17.9.9.3.1" class="ltx_text" style="font-size:90%;">16.0</span></td>
<td id="S4.T3.17.9.9.4" class="ltx_td ltx_align_center"><span id="S4.T3.17.9.9.4.1" class="ltx_text" style="font-size:90%;">82.9</span></td>
</tr>
<tr id="S4.T3.18.10.10" class="ltx_tr">
<th id="S4.T3.18.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T3.18.10.10.1.1" class="ltx_text" style="font-size:90%;">EfficientNet-B6 </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.18.10.10.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib87" title="" class="ltx_ref">87</a><span id="S4.T3.18.10.10.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.T3.18.10.10.1.4" class="ltx_text" style="font-size:90%;"> </span><math id="S4.T3.18.10.10.1.m1.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S4.T3.18.10.10.1.m1.1a"><mo mathsize="90%" id="S4.T3.18.10.10.1.m1.1.1" xref="S4.T3.18.10.10.1.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S4.T3.18.10.10.1.m1.1b"><ci id="S4.T3.18.10.10.1.m1.1.1.cmml" xref="S4.T3.18.10.10.1.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.18.10.10.1.m1.1c">\star</annotation><annotation encoding="application/x-llamapun" id="S4.T3.18.10.10.1.m1.1d">⋆</annotation></semantics></math>
</th>
<td id="S4.T3.18.10.10.2" class="ltx_td ltx_align_center"><span id="S4.T3.18.10.10.2.1" class="ltx_text" style="font-size:90%;">43.0</span></td>
<td id="S4.T3.18.10.10.3" class="ltx_td ltx_align_center"><span id="S4.T3.18.10.10.3.1" class="ltx_text" style="font-size:90%;">19.0</span></td>
<td id="S4.T3.18.10.10.4" class="ltx_td ltx_align_center"><span id="S4.T3.18.10.10.4.1" class="ltx_text" style="font-size:90%;">84.0</span></td>
</tr>
<tr id="S4.T3.18.10.24.13" class="ltx_tr">
<th id="S4.T3.18.10.24.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T3.18.10.24.13.1.1" class="ltx_text" style="font-size:90%;">NesT-B </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.18.10.24.13.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib111" title="" class="ltx_ref">111</a><span id="S4.T3.18.10.24.13.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T3.18.10.24.13.2" class="ltx_td ltx_align_center"><span id="S4.T3.18.10.24.13.2.1" class="ltx_text" style="font-size:90%;">68.0</span></td>
<td id="S4.T3.18.10.24.13.3" class="ltx_td ltx_align_center"><span id="S4.T3.18.10.24.13.3.1" class="ltx_text" style="font-size:90%;">17.9</span></td>
<td id="S4.T3.18.10.24.13.4" class="ltx_td ltx_align_center"><span id="S4.T3.18.10.24.13.4.1" class="ltx_text" style="font-size:90%;">83.8</span></td>
</tr>
<tr id="S4.T3.18.10.25.14" class="ltx_tr">
<th id="S4.T3.18.10.25.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T3.18.10.25.14.1.1" class="ltx_text" style="font-size:90%;">ViT-B/16 </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.18.10.25.14.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib11" title="" class="ltx_ref">11</a><span id="S4.T3.18.10.25.14.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T3.18.10.25.14.2" class="ltx_td ltx_align_center"><span id="S4.T3.18.10.25.14.2.1" class="ltx_text" style="font-size:90%;">86.6</span></td>
<td id="S4.T3.18.10.25.14.3" class="ltx_td ltx_align_center"><span id="S4.T3.18.10.25.14.3.1" class="ltx_text" style="font-size:90%;">17.6</span></td>
<td id="S4.T3.18.10.25.14.4" class="ltx_td ltx_align_center"><span id="S4.T3.18.10.25.14.4.1" class="ltx_text" style="font-size:90%;">79.8</span></td>
</tr>
<tr id="S4.T3.18.10.26.15" class="ltx_tr">
<th id="S4.T3.18.10.26.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T3.18.10.26.15.1.1" class="ltx_text" style="font-size:90%;">DeiT-B/16 </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.18.10.26.15.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib12" title="" class="ltx_ref">12</a><span id="S4.T3.18.10.26.15.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T3.18.10.26.15.2" class="ltx_td ltx_align_center"><span id="S4.T3.18.10.26.15.2.1" class="ltx_text" style="font-size:90%;">86.6</span></td>
<td id="S4.T3.18.10.26.15.3" class="ltx_td ltx_align_center"><span id="S4.T3.18.10.26.15.3.1" class="ltx_text" style="font-size:90%;">17.6</span></td>
<td id="S4.T3.18.10.26.15.4" class="ltx_td ltx_align_center"><span id="S4.T3.18.10.26.15.4.1" class="ltx_text" style="font-size:90%;">81.8</span></td>
</tr>
<tr id="S4.T3.18.10.27.16" class="ltx_tr">
<th id="S4.T3.18.10.27.16.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T3.18.10.27.16.1.1" class="ltx_text" style="font-size:90%;">Swin-B </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.18.10.27.16.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib36" title="" class="ltx_ref">36</a><span id="S4.T3.18.10.27.16.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T3.18.10.27.16.2" class="ltx_td ltx_align_center"><span id="S4.T3.18.10.27.16.2.1" class="ltx_text" style="font-size:90%;">88.0</span></td>
<td id="S4.T3.18.10.27.16.3" class="ltx_td ltx_align_center"><span id="S4.T3.18.10.27.16.3.1" class="ltx_text" style="font-size:90%;">15.4</span></td>
<td id="S4.T3.18.10.27.16.4" class="ltx_td ltx_align_center"><span id="S4.T3.18.10.27.16.4.1" class="ltx_text" style="font-size:90%;">83.3</span></td>
</tr>
<tr id="S4.T3.18.10.28.17" class="ltx_tr">
<th id="S4.T3.18.10.28.17.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T3.18.10.28.17.1.1" class="ltx_text" style="font-size:90%;">Twins-SVT-L </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.18.10.28.17.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib37" title="" class="ltx_ref">37</a><span id="S4.T3.18.10.28.17.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T3.18.10.28.17.2" class="ltx_td ltx_align_center"><span id="S4.T3.18.10.28.17.2.1" class="ltx_text" style="font-size:90%;">99.2</span></td>
<td id="S4.T3.18.10.28.17.3" class="ltx_td ltx_align_center"><span id="S4.T3.18.10.28.17.3.1" class="ltx_text" style="font-size:90%;">14.8</span></td>
<td id="S4.T3.18.10.28.17.4" class="ltx_td ltx_align_center"><span id="S4.T3.18.10.28.17.4.1" class="ltx_text" style="font-size:90%;">83.7</span></td>
</tr>
<tr id="S4.T3.18.10.29.18" class="ltx_tr">
<th id="S4.T3.18.10.29.18.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T3.18.10.29.18.1.1" class="ltx_text" style="font-size:90%;">PVTv2-B5 </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.18.10.29.18.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib97" title="" class="ltx_ref">97</a><span id="S4.T3.18.10.29.18.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T3.18.10.29.18.2" class="ltx_td ltx_align_center"><span id="S4.T3.18.10.29.18.2.1" class="ltx_text" style="font-size:90%;">82.0</span></td>
<td id="S4.T3.18.10.29.18.3" class="ltx_td ltx_align_center"><span id="S4.T3.18.10.29.18.3.1" class="ltx_text" style="font-size:90%;">11.8</span></td>
<td id="S4.T3.18.10.29.18.4" class="ltx_td ltx_align_center"><span id="S4.T3.18.10.29.18.4.1" class="ltx_text" style="font-size:90%;">83.8</span></td>
</tr>
<tr id="S4.T3.18.10.30.19" class="ltx_tr">
<th id="S4.T3.18.10.30.19.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">
<span id="S4.T3.18.10.30.19.1.1" class="ltx_text" style="font-size:90%;">Lv-ViT-M </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.18.10.30.19.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib89" title="" class="ltx_ref">89</a><span id="S4.T3.18.10.30.19.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T3.18.10.30.19.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T3.18.10.30.19.2.1" class="ltx_text" style="font-size:90%;">56.0</span></td>
<td id="S4.T3.18.10.30.19.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T3.18.10.30.19.3.1" class="ltx_text" style="font-size:90%;">16.0</span></td>
<td id="S4.T3.18.10.30.19.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T3.18.10.30.19.4.1" class="ltx_text" style="font-size:90%;">84.1</span></td>
</tr>
</tbody>
</table>
</span></div>
</div>
</div>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">TABLE III: </span><span id="S4.T3.20.1" class="ltx_text" style="color:#000000;">A Comparative analysis between different vision transformer and CNN models in terms of their parameter complexity and top-1 (%) accuracy on ImageNet validation set. For a direct comparison, we consider models that are trained on ImageNet from scratch on input of size 224x224. <math id="S4.T3.20.1.m1.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S4.T3.20.1.m1.1b"><mo mathcolor="#000000" id="S4.T3.20.1.m1.1.1" xref="S4.T3.20.1.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S4.T3.20.1.m1.1c"><ci id="S4.T3.20.1.m1.1.1.cmml" xref="S4.T3.20.1.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.20.1.m1.1d">\star</annotation><annotation encoding="application/x-llamapun" id="S4.T3.20.1.m1.1e">⋆</annotation></semantics></math> denotes pure CNN-based methods.</span></figcaption>
</figure>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.4" class="ltx_p">In the language domain, recent works focus on reducing the high complexity of Transformer models (basically arising from the self-attention mechanism <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> where a token’s representation is updated by considering all tokens from the previous layer). For example, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib245" title="" class="ltx_ref">245</a>, <a href="#bib.bib103" title="" class="ltx_ref">103</a>]</cite> explore selective or sparse attention to previous layer tokens while updating each next layer token. Linformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> reduces complexity of standard self-attention operation from <math id="S4.SS1.p3.1.m1.1" class="ltx_Math" alttext="\mathcal{O}(n^{2})" display="inline"><semantics id="S4.SS1.p3.1.m1.1a"><mrow id="S4.SS1.p3.1.m1.1.1" xref="S4.SS1.p3.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.p3.1.m1.1.1.3" xref="S4.SS1.p3.1.m1.1.1.3.cmml">𝒪</mi><mo id="S4.SS1.p3.1.m1.1.1.2" xref="S4.SS1.p3.1.m1.1.1.2.cmml" lspace='0px' rspace='0px'></mo><mrow id="S4.SS1.p3.1.m1.1.1.1.1" xref="S4.SS1.p3.1.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.SS1.p3.1.m1.1.1.1.1.2" xref="S4.SS1.p3.1.m1.1.1.1.1.1.cmml">(</mo><msup id="S4.SS1.p3.1.m1.1.1.1.1.1" xref="S4.SS1.p3.1.m1.1.1.1.1.1.cmml"><mi id="S4.SS1.p3.1.m1.1.1.1.1.1.2" xref="S4.SS1.p3.1.m1.1.1.1.1.1.2.cmml">n</mi><mn id="S4.SS1.p3.1.m1.1.1.1.1.1.3" xref="S4.SS1.p3.1.m1.1.1.1.1.1.3.cmml">2</mn></msup><mo stretchy="false" id="S4.SS1.p3.1.m1.1.1.1.1.3" xref="S4.SS1.p3.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.1.m1.1b"><apply id="S4.SS1.p3.1.m1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1"><times id="S4.SS1.p3.1.m1.1.1.2.cmml" xref="S4.SS1.p3.1.m1.1.1.2"></times><ci id="S4.SS1.p3.1.m1.1.1.3.cmml" xref="S4.SS1.p3.1.m1.1.1.3">𝒪</ci><apply id="S4.SS1.p3.1.m1.1.1.1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p3.1.m1.1.1.1.1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1.1.1">superscript</csymbol><ci id="S4.SS1.p3.1.m1.1.1.1.1.1.2.cmml" xref="S4.SS1.p3.1.m1.1.1.1.1.1.2">𝑛</ci><cn type="integer" id="S4.SS1.p3.1.m1.1.1.1.1.1.3.cmml" xref="S4.SS1.p3.1.m1.1.1.1.1.1.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.1.m1.1c">\mathcal{O}(n^{2})</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p3.1.m1.1d">caligraphic_O ( italic_n start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT )</annotation></semantics></math> to <math id="S4.SS1.p3.2.m2.1" class="ltx_Math" alttext="\mathcal{O}(n)" display="inline"><semantics id="S4.SS1.p3.2.m2.1a"><mrow id="S4.SS1.p3.2.m2.1.2" xref="S4.SS1.p3.2.m2.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.p3.2.m2.1.2.2" xref="S4.SS1.p3.2.m2.1.2.2.cmml">𝒪</mi><mo id="S4.SS1.p3.2.m2.1.2.1" xref="S4.SS1.p3.2.m2.1.2.1.cmml" lspace='0px' rspace='0px'></mo><mrow id="S4.SS1.p3.2.m2.1.2.3.2" xref="S4.SS1.p3.2.m2.1.2.cmml"><mo stretchy="false" id="S4.SS1.p3.2.m2.1.2.3.2.1" xref="S4.SS1.p3.2.m2.1.2.cmml">(</mo><mi id="S4.SS1.p3.2.m2.1.1" xref="S4.SS1.p3.2.m2.1.1.cmml">n</mi><mo stretchy="false" id="S4.SS1.p3.2.m2.1.2.3.2.2" xref="S4.SS1.p3.2.m2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.2.m2.1b"><apply id="S4.SS1.p3.2.m2.1.2.cmml" xref="S4.SS1.p3.2.m2.1.2"><times id="S4.SS1.p3.2.m2.1.2.1.cmml" xref="S4.SS1.p3.2.m2.1.2.1"></times><ci id="S4.SS1.p3.2.m2.1.2.2.cmml" xref="S4.SS1.p3.2.m2.1.2.2">𝒪</ci><ci id="S4.SS1.p3.2.m2.1.1.cmml" xref="S4.SS1.p3.2.m2.1.1">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.2.m2.1c">\mathcal{O}(n)</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p3.2.m2.1d">caligraphic_O ( italic_n )</annotation></semantics></math> (both in time and memory requirements). The main idea is to show that a low-rank matrix is sufficient to model the self-attention mechanism. The Reformer model
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib246" title="" class="ltx_ref">246</a>]</cite> employed locally-sensitive hashing (LSH) to minimize the complexity of self-attention from <math id="S4.SS1.p3.3.m3.1" class="ltx_Math" alttext="\mathcal{O}(n^{2})" display="inline"><semantics id="S4.SS1.p3.3.m3.1a"><mrow id="S4.SS1.p3.3.m3.1.1" xref="S4.SS1.p3.3.m3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.p3.3.m3.1.1.3" xref="S4.SS1.p3.3.m3.1.1.3.cmml">𝒪</mi><mo id="S4.SS1.p3.3.m3.1.1.2" xref="S4.SS1.p3.3.m3.1.1.2.cmml" lspace='0px' rspace='0px'></mo><mrow id="S4.SS1.p3.3.m3.1.1.1.1" xref="S4.SS1.p3.3.m3.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.SS1.p3.3.m3.1.1.1.1.2" xref="S4.SS1.p3.3.m3.1.1.1.1.1.cmml">(</mo><msup id="S4.SS1.p3.3.m3.1.1.1.1.1" xref="S4.SS1.p3.3.m3.1.1.1.1.1.cmml"><mi id="S4.SS1.p3.3.m3.1.1.1.1.1.2" xref="S4.SS1.p3.3.m3.1.1.1.1.1.2.cmml">n</mi><mn id="S4.SS1.p3.3.m3.1.1.1.1.1.3" xref="S4.SS1.p3.3.m3.1.1.1.1.1.3.cmml">2</mn></msup><mo stretchy="false" id="S4.SS1.p3.3.m3.1.1.1.1.3" xref="S4.SS1.p3.3.m3.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.3.m3.1b"><apply id="S4.SS1.p3.3.m3.1.1.cmml" xref="S4.SS1.p3.3.m3.1.1"><times id="S4.SS1.p3.3.m3.1.1.2.cmml" xref="S4.SS1.p3.3.m3.1.1.2"></times><ci id="S4.SS1.p3.3.m3.1.1.3.cmml" xref="S4.SS1.p3.3.m3.1.1.3">𝒪</ci><apply id="S4.SS1.p3.3.m3.1.1.1.1.1.cmml" xref="S4.SS1.p3.3.m3.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p3.3.m3.1.1.1.1.1.1.cmml" xref="S4.SS1.p3.3.m3.1.1.1.1">superscript</csymbol><ci id="S4.SS1.p3.3.m3.1.1.1.1.1.2.cmml" xref="S4.SS1.p3.3.m3.1.1.1.1.1.2">𝑛</ci><cn type="integer" id="S4.SS1.p3.3.m3.1.1.1.1.1.3.cmml" xref="S4.SS1.p3.3.m3.1.1.1.1.1.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.3.m3.1c">\mathcal{O}(n^{2})</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p3.3.m3.1d">caligraphic_O ( italic_n start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT )</annotation></semantics></math> to <math id="S4.SS1.p3.4.m4.2" class="ltx_Math" alttext="\mathcal{O}(nlog(n))" display="inline"><semantics id="S4.SS1.p3.4.m4.2a"><mrow id="S4.SS1.p3.4.m4.2.2" xref="S4.SS1.p3.4.m4.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.p3.4.m4.2.2.3" xref="S4.SS1.p3.4.m4.2.2.3.cmml">𝒪</mi><mo id="S4.SS1.p3.4.m4.2.2.2" xref="S4.SS1.p3.4.m4.2.2.2.cmml" lspace='0px' rspace='0px'></mo><mrow id="S4.SS1.p3.4.m4.2.2.1.1" xref="S4.SS1.p3.4.m4.2.2.1.1.1.cmml"><mo stretchy="false" id="S4.SS1.p3.4.m4.2.2.1.1.2" xref="S4.SS1.p3.4.m4.2.2.1.1.1.cmml">(</mo><mrow id="S4.SS1.p3.4.m4.2.2.1.1.1" xref="S4.SS1.p3.4.m4.2.2.1.1.1.cmml"><mi id="S4.SS1.p3.4.m4.2.2.1.1.1.2" xref="S4.SS1.p3.4.m4.2.2.1.1.1.2.cmml">n</mi><mo id="S4.SS1.p3.4.m4.2.2.1.1.1.1" xref="S4.SS1.p3.4.m4.2.2.1.1.1.1.cmml" lspace='0px' rspace='0px'></mo><mi id="S4.SS1.p3.4.m4.2.2.1.1.1.3" xref="S4.SS1.p3.4.m4.2.2.1.1.1.3.cmml">l</mi><mo id="S4.SS1.p3.4.m4.2.2.1.1.1.1a" xref="S4.SS1.p3.4.m4.2.2.1.1.1.1.cmml" lspace='0px' rspace='0px'></mo><mi id="S4.SS1.p3.4.m4.2.2.1.1.1.4" xref="S4.SS1.p3.4.m4.2.2.1.1.1.4.cmml">o</mi><mo id="S4.SS1.p3.4.m4.2.2.1.1.1.1b" xref="S4.SS1.p3.4.m4.2.2.1.1.1.1.cmml" lspace='0px' rspace='0px'></mo><mi id="S4.SS1.p3.4.m4.2.2.1.1.1.5" xref="S4.SS1.p3.4.m4.2.2.1.1.1.5.cmml">g</mi><mo id="S4.SS1.p3.4.m4.2.2.1.1.1.1c" xref="S4.SS1.p3.4.m4.2.2.1.1.1.1.cmml" lspace='0px' rspace='0px'></mo><mrow id="S4.SS1.p3.4.m4.2.2.1.1.1.6.2" xref="S4.SS1.p3.4.m4.2.2.1.1.1.cmml"><mo stretchy="false" id="S4.SS1.p3.4.m4.2.2.1.1.1.6.2.1" xref="S4.SS1.p3.4.m4.2.2.1.1.1.cmml">(</mo><mi id="S4.SS1.p3.4.m4.1.1" xref="S4.SS1.p3.4.m4.1.1.cmml">n</mi><mo stretchy="false" id="S4.SS1.p3.4.m4.2.2.1.1.1.6.2.2" xref="S4.SS1.p3.4.m4.2.2.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S4.SS1.p3.4.m4.2.2.1.1.3" xref="S4.SS1.p3.4.m4.2.2.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.4.m4.2b"><apply id="S4.SS1.p3.4.m4.2.2.cmml" xref="S4.SS1.p3.4.m4.2.2"><times id="S4.SS1.p3.4.m4.2.2.2.cmml" xref="S4.SS1.p3.4.m4.2.2.2"></times><ci id="S4.SS1.p3.4.m4.2.2.3.cmml" xref="S4.SS1.p3.4.m4.2.2.3">𝒪</ci><apply id="S4.SS1.p3.4.m4.2.2.1.1.1.cmml" xref="S4.SS1.p3.4.m4.2.2.1.1"><times id="S4.SS1.p3.4.m4.2.2.1.1.1.1.cmml" xref="S4.SS1.p3.4.m4.2.2.1.1.1.1"></times><ci id="S4.SS1.p3.4.m4.2.2.1.1.1.2.cmml" xref="S4.SS1.p3.4.m4.2.2.1.1.1.2">𝑛</ci><ci id="S4.SS1.p3.4.m4.2.2.1.1.1.3.cmml" xref="S4.SS1.p3.4.m4.2.2.1.1.1.3">𝑙</ci><ci id="S4.SS1.p3.4.m4.2.2.1.1.1.4.cmml" xref="S4.SS1.p3.4.m4.2.2.1.1.1.4">𝑜</ci><ci id="S4.SS1.p3.4.m4.2.2.1.1.1.5.cmml" xref="S4.SS1.p3.4.m4.2.2.1.1.1.5">𝑔</ci><ci id="S4.SS1.p3.4.m4.1.1.cmml" xref="S4.SS1.p3.4.m4.1.1">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.4.m4.2c">\mathcal{O}(nlog(n))</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p3.4.m4.2d">caligraphic_O ( italic_n italic_l italic_o italic_g ( italic_n ) )</annotation></semantics></math>. <span id="S4.SS1.p3.4.1" class="ltx_text" style="color:#000000;">In similar pursuit, the recent Lambda Networks propose to model local context as a linear function which helps reduce complexity of self-attention <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib247" title="" class="ltx_ref">247</a>]</cite>. These linear function lambdas are applied to the input query to model contextual relationships between pixels.</span></p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.3" class="ltx_p">Vyas <span id="S4.SS1.p4.3.1" class="ltx_text ltx_font_italic">et al.<cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS1.p4.3.1.1.1" class="ltx_text ltx_font_upright">[</span><a href="#bib.bib248" title="" class="ltx_ref">248</a><span id="S4.SS1.p4.3.1.2.2" class="ltx_text ltx_font_upright">]</span></cite></span> developed an efficient <em id="S4.SS1.p4.3.2" class="ltx_emph ltx_font_italic">cluster attention</em> to deal with large input sequences that approximates the original self-attention. The cluster attention groups queries into clusters and then computes attention between cluster centers (instead of attention between all the queries that leads to quadratic complexity). The main idea is that the queries close in the Euclidean space should have similar attention distributions. With a fixed number of clusters, this intuition helps reduce the quadratic complexity to linear complexity of <math id="S4.SS1.p4.1.m1.1" class="ltx_Math" alttext="\mathcal{O}(nc)" display="inline"><semantics id="S4.SS1.p4.1.m1.1a"><mrow id="S4.SS1.p4.1.m1.1.1" xref="S4.SS1.p4.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.p4.1.m1.1.1.3" xref="S4.SS1.p4.1.m1.1.1.3.cmml">𝒪</mi><mo id="S4.SS1.p4.1.m1.1.1.2" xref="S4.SS1.p4.1.m1.1.1.2.cmml" lspace='0px' rspace='0px'></mo><mrow id="S4.SS1.p4.1.m1.1.1.1.1" xref="S4.SS1.p4.1.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.SS1.p4.1.m1.1.1.1.1.2" xref="S4.SS1.p4.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="S4.SS1.p4.1.m1.1.1.1.1.1" xref="S4.SS1.p4.1.m1.1.1.1.1.1.cmml"><mi id="S4.SS1.p4.1.m1.1.1.1.1.1.2" xref="S4.SS1.p4.1.m1.1.1.1.1.1.2.cmml">n</mi><mo id="S4.SS1.p4.1.m1.1.1.1.1.1.1" xref="S4.SS1.p4.1.m1.1.1.1.1.1.1.cmml" lspace='0px' rspace='0px'></mo><mi id="S4.SS1.p4.1.m1.1.1.1.1.1.3" xref="S4.SS1.p4.1.m1.1.1.1.1.1.3.cmml">c</mi></mrow><mo stretchy="false" id="S4.SS1.p4.1.m1.1.1.1.1.3" xref="S4.SS1.p4.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.1.m1.1b"><apply id="S4.SS1.p4.1.m1.1.1.cmml" xref="S4.SS1.p4.1.m1.1.1"><times id="S4.SS1.p4.1.m1.1.1.2.cmml" xref="S4.SS1.p4.1.m1.1.1.2"></times><ci id="S4.SS1.p4.1.m1.1.1.3.cmml" xref="S4.SS1.p4.1.m1.1.1.3">𝒪</ci><apply id="S4.SS1.p4.1.m1.1.1.1.1.1.cmml" xref="S4.SS1.p4.1.m1.1.1.1.1"><times id="S4.SS1.p4.1.m1.1.1.1.1.1.1.cmml" xref="S4.SS1.p4.1.m1.1.1.1.1.1.1"></times><ci id="S4.SS1.p4.1.m1.1.1.1.1.1.2.cmml" xref="S4.SS1.p4.1.m1.1.1.1.1.1.2">𝑛</ci><ci id="S4.SS1.p4.1.m1.1.1.1.1.1.3.cmml" xref="S4.SS1.p4.1.m1.1.1.1.1.1.3">𝑐</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.1.m1.1c">\mathcal{O}(nc)</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p4.1.m1.1d">caligraphic_O ( italic_n italic_c )</annotation></semantics></math> with respect to the input sequence length <math id="S4.SS1.p4.2.m2.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S4.SS1.p4.2.m2.1a"><mi id="S4.SS1.p4.2.m2.1.1" xref="S4.SS1.p4.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.2.m2.1b"><ci id="S4.SS1.p4.2.m2.1.1.cmml" xref="S4.SS1.p4.2.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.2.m2.1c">n</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p4.2.m2.1d">italic_n</annotation></semantics></math> (where <math id="S4.SS1.p4.3.m3.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S4.SS1.p4.3.m3.1a"><mi id="S4.SS1.p4.3.m3.1.1" xref="S4.SS1.p4.3.m3.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.3.m3.1b"><ci id="S4.SS1.p4.3.m3.1.1.cmml" xref="S4.SS1.p4.3.m3.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.3.m3.1c">c</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p4.3.m3.1d">italic_c</annotation></semantics></math> is the number of clusters). We refer interested readers to a survey on efficient Transformers in NLP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>.</p>
</div>
<div id="S4.SS1.p5" class="ltx_para">
<p id="S4.SS1.p5.4" class="ltx_p">Similar to the NLP domain, computer vision models also suffer from the high computational cost of Transformer models. For example, image generators that are based on sequence-based Transformers (<em id="S4.SS1.p5.4.1" class="ltx_emph ltx_font_italic">e.g.</em>, iGPT) have a high compute cost limiting their applicability to high-resolution inputs. The time and memory cost of core self-attention operation in Transformers increases quadratically with the number of patches, i.e. <math id="S4.SS1.p5.1.m1.1" class="ltx_Math" alttext="\mathcal{O}(n^{2})" display="inline"><semantics id="S4.SS1.p5.1.m1.1a"><mrow id="S4.SS1.p5.1.m1.1.1" xref="S4.SS1.p5.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.p5.1.m1.1.1.3" xref="S4.SS1.p5.1.m1.1.1.3.cmml">𝒪</mi><mo id="S4.SS1.p5.1.m1.1.1.2" xref="S4.SS1.p5.1.m1.1.1.2.cmml" lspace='0px' rspace='0px'></mo><mrow id="S4.SS1.p5.1.m1.1.1.1.1" xref="S4.SS1.p5.1.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.SS1.p5.1.m1.1.1.1.1.2" xref="S4.SS1.p5.1.m1.1.1.1.1.1.cmml">(</mo><msup id="S4.SS1.p5.1.m1.1.1.1.1.1" xref="S4.SS1.p5.1.m1.1.1.1.1.1.cmml"><mi id="S4.SS1.p5.1.m1.1.1.1.1.1.2" xref="S4.SS1.p5.1.m1.1.1.1.1.1.2.cmml">n</mi><mn id="S4.SS1.p5.1.m1.1.1.1.1.1.3" xref="S4.SS1.p5.1.m1.1.1.1.1.1.3.cmml">2</mn></msup><mo stretchy="false" id="S4.SS1.p5.1.m1.1.1.1.1.3" xref="S4.SS1.p5.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p5.1.m1.1b"><apply id="S4.SS1.p5.1.m1.1.1.cmml" xref="S4.SS1.p5.1.m1.1.1"><times id="S4.SS1.p5.1.m1.1.1.2.cmml" xref="S4.SS1.p5.1.m1.1.1.2"></times><ci id="S4.SS1.p5.1.m1.1.1.3.cmml" xref="S4.SS1.p5.1.m1.1.1.3">𝒪</ci><apply id="S4.SS1.p5.1.m1.1.1.1.1.1.cmml" xref="S4.SS1.p5.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p5.1.m1.1.1.1.1.1.1.cmml" xref="S4.SS1.p5.1.m1.1.1.1.1">superscript</csymbol><ci id="S4.SS1.p5.1.m1.1.1.1.1.1.2.cmml" xref="S4.SS1.p5.1.m1.1.1.1.1.1.2">𝑛</ci><cn type="integer" id="S4.SS1.p5.1.m1.1.1.1.1.1.3.cmml" xref="S4.SS1.p5.1.m1.1.1.1.1.1.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p5.1.m1.1c">\mathcal{O}(n^{2})</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p5.1.m1.1d">caligraphic_O ( italic_n start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT )</annotation></semantics></math>, for <math id="S4.SS1.p5.2.m2.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S4.SS1.p5.2.m2.1a"><mi id="S4.SS1.p5.2.m2.1.1" xref="S4.SS1.p5.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p5.2.m2.1b"><ci id="S4.SS1.p5.2.m2.1.1.cmml" xref="S4.SS1.p5.2.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p5.2.m2.1c">n</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p5.2.m2.1d">italic_n</annotation></semantics></math> image patches (in some applications, e.g., low-level vision, <math id="S4.SS1.p5.3.m3.1" class="ltx_Math" alttext="n=H\times W" display="inline"><semantics id="S4.SS1.p5.3.m3.1a"><mrow id="S4.SS1.p5.3.m3.1.1" xref="S4.SS1.p5.3.m3.1.1.cmml"><mi id="S4.SS1.p5.3.m3.1.1.2" xref="S4.SS1.p5.3.m3.1.1.2.cmml">n</mi><mo id="S4.SS1.p5.3.m3.1.1.1" xref="S4.SS1.p5.3.m3.1.1.1.cmml">=</mo><mrow id="S4.SS1.p5.3.m3.1.1.3" xref="S4.SS1.p5.3.m3.1.1.3.cmml"><mi id="S4.SS1.p5.3.m3.1.1.3.2" xref="S4.SS1.p5.3.m3.1.1.3.2.cmml">H</mi><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.p5.3.m3.1.1.3.1" xref="S4.SS1.p5.3.m3.1.1.3.1.cmml">×</mo><mi id="S4.SS1.p5.3.m3.1.1.3.3" xref="S4.SS1.p5.3.m3.1.1.3.3.cmml">W</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p5.3.m3.1b"><apply id="S4.SS1.p5.3.m3.1.1.cmml" xref="S4.SS1.p5.3.m3.1.1"><eq id="S4.SS1.p5.3.m3.1.1.1.cmml" xref="S4.SS1.p5.3.m3.1.1.1"></eq><ci id="S4.SS1.p5.3.m3.1.1.2.cmml" xref="S4.SS1.p5.3.m3.1.1.2">𝑛</ci><apply id="S4.SS1.p5.3.m3.1.1.3.cmml" xref="S4.SS1.p5.3.m3.1.1.3"><times id="S4.SS1.p5.3.m3.1.1.3.1.cmml" xref="S4.SS1.p5.3.m3.1.1.3.1"></times><ci id="S4.SS1.p5.3.m3.1.1.3.2.cmml" xref="S4.SS1.p5.3.m3.1.1.3.2">𝐻</ci><ci id="S4.SS1.p5.3.m3.1.1.3.3.cmml" xref="S4.SS1.p5.3.m3.1.1.3.3">𝑊</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p5.3.m3.1c">n=H\times W</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p5.3.m3.1d">italic_n = italic_H × italic_W</annotation></semantics></math> where <math id="S4.SS1.p5.4.m4.2" class="ltx_Math" alttext="H,W" display="inline"><semantics id="S4.SS1.p5.4.m4.2a"><mrow id="S4.SS1.p5.4.m4.2.3.2" xref="S4.SS1.p5.4.m4.2.3.1.cmml"><mi id="S4.SS1.p5.4.m4.1.1" xref="S4.SS1.p5.4.m4.1.1.cmml">H</mi><mo id="S4.SS1.p5.4.m4.2.3.2.1" xref="S4.SS1.p5.4.m4.2.3.1.cmml">,</mo><mi id="S4.SS1.p5.4.m4.2.2" xref="S4.SS1.p5.4.m4.2.2.cmml">W</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p5.4.m4.2b"><list id="S4.SS1.p5.4.m4.2.3.1.cmml" xref="S4.SS1.p5.4.m4.2.3.2"><ci id="S4.SS1.p5.4.m4.1.1.cmml" xref="S4.SS1.p5.4.m4.1.1">𝐻</ci><ci id="S4.SS1.p5.4.m4.2.2.cmml" xref="S4.SS1.p5.4.m4.2.2">𝑊</ci></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p5.4.m4.2c">H,W</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p5.4.m4.2d">italic_H , italic_W</annotation></semantics></math> denote the height and width of the image). This is a major drawback of existing Transformers that hinders their application to most tasks involving high-resolution (HR) images, such as object detection and segmentation (in high-level vision), and super-resolution, deblurring, denoising, etc. (in low-level vision). Numerous methods have been proposed that make special design choices to perform self-attention more ‘efficiently’, for instance employing pooling/downsampling in self-attention <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib97" title="" class="ltx_ref">97</a>, <a href="#bib.bib219" title="" class="ltx_ref">219</a>, <a href="#bib.bib249" title="" class="ltx_ref">249</a>]</cite>, local window-based attention <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib250" title="" class="ltx_ref">250</a>]</cite>, axial-attention <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib179" title="" class="ltx_ref">179</a>, <a href="#bib.bib251" title="" class="ltx_ref">251</a>]</cite>, low-rank projection attention <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib252" title="" class="ltx_ref">252</a>, <a href="#bib.bib253" title="" class="ltx_ref">253</a>]</cite>, kernelizable attention <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib254" title="" class="ltx_ref">254</a>, <a href="#bib.bib255" title="" class="ltx_ref">255</a>]</cite>, and similarity-clustering based methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib246" title="" class="ltx_ref">246</a>, <a href="#bib.bib256" title="" class="ltx_ref">256</a>]</cite>. However, almost all of these approaches either come with a trade-off between complexity and accuracy, require special hardware specifications or are still not applicable to very large images. Therefore, there is a pressing need to develop an efficient self-attention mechanism that can be applied to HR images on resource-limited systems without compromising accuracy. It will be interesting to explore how existing models can be extended to high-dimensional cases <em id="S4.SS1.p5.4.2" class="ltx_emph ltx_font_italic">e.g.</em>, using a <em id="S4.SS1.p5.4.3" class="ltx_emph ltx_font_italic">multi-scale transformer</em> design with a somewhat local context modeling. By inducing inductive biases based on our understanding of the visual learning tasks (e.g., spatial relationships in the local neighbourhood), the high computational cost can be reduced. Similarly, using sparse attention maps modeled with low-rank factorization in the matrices can also help towards reducing the computational cost <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib211" title="" class="ltx_ref">211</a>]</cite>.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span><span id="S4.SS2.1.1" class="ltx_text ltx_font_italic">Large Data Requirements</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Since Transformer architectures do not inherently encode inductive biases (prior knowledge) to deal with visual data, they typically require large amount of training to figure out the underlying modality-specific rules. For example, a CNN has inbuilt translation invariance, weight sharing, and partial scale invariance due to pooling operations or multi-scale processing blocks. However, a Transformer network needs to figure out these image-specific concepts on its own from the training examples. Similarly, relationships between video frames need to be discovered automatically by the self-attention mechanism by looking at a large database of video sequences. This results in longer training times, a significant increase in computational requirements, and large datasets for processing. For example, the ViT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> model requires hundreds of millions of image examples to obtain reasonable performance on the ImageNet benchmark dataset. The question of learning a Transformer in a data-efficient manner is an open research problem and recent works report encouraging steps towards its resolution. For example, DeiT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> uses a distillation approach to achieve data efficiency while T2T (Tokens-to-Token) ViT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> models local structure by combining spatially close tokens together, thus leading to competitive performance when trained only on ImageNet from scratch (without pre-training). <span id="S4.SS2.p1.1.1" class="ltx_text" style="color:#000000;">By incorporating CNNs like feature hierarchies in ViTs to effectively capture local image cues, ViTs (e.g., CCT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib106" title="" class="ltx_ref">106</a>]</cite>, NesT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib111" title="" class="ltx_ref">111</a>]</cite>) can be trained from scratch even on small-scale datasets (e.g., CIFAR-10).
Another approach to data efficient training of ViTs is proposed in <span id="S4.SS2.p1.1.1.1" class="ltx_text ltx_font_italic">et al.<cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS2.p1.1.1.1.1.1" class="ltx_text ltx_font_upright">[</span><a href="#bib.bib257" title="" class="ltx_ref">257</a><span id="S4.SS2.p1.1.1.1.2.2" class="ltx_text ltx_font_upright">]</span></cite></span>. The authors show that by smoothing the local loss surface using sharpness-aware minimizer (SAM) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib258" title="" class="ltx_ref">258</a>]</cite>, ViTs can be trained with simple data augmentation scheme (random crop, and horizontal flip) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib259" title="" class="ltx_ref">259</a>]</cite>, instead of employing compute intensive strong data augmentation strategies, and can outperform their counterpart ResNet models.</span></p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span><span id="S4.SS3.1.1" class="ltx_text ltx_font_italic">Vision Tailored Transformer Designs</span>
</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">We note that most of the existing works focused on vision tasks tend to directly apply NLP Transformer models on computer vision problems. These include architectures designed for image recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, video understanding <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> and especially multi-modal processing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib181" title="" class="ltx_ref">181</a>]</cite>. Although the initial results from these simple applications are quite encouraging and motivate us to look further into the strengths of self-attention and self-supervised learning, current architectures may still remain better tailored for language problems (with a sequence structure) and need further intuitions to make them more efficient for visual inputs. For example, vector attention from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib82" title="" class="ltx_ref">82</a>]</cite> is a nice work in this direction which attempts to specifically tailor self-attention operation for visual inputs via learning channel-wise attentions. Similarly, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib260" title="" class="ltx_ref">260</a>]</cite> uses a Jigsaw puzzle based self-supervision loss as a parallel branch in the Transformers to improve person re-identification. A recent work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> rearranges the spatially close tokens to better model relationships in spatially proximal locations. <span id="S4.SS3.p1.1.1" class="ltx_text" style="color:#000000;">Token distillation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> from pre-trained CNN models has also been used as a remedy to inject domain biases in the representations. One may argue that the architectures like Transformer models should remain generic to be directly applicable across domains, we notice that the high computational and time cost for pre-training such models demands novel design strategies to make their training more affordable on vision problems.</span></p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsection">4.4 </span><span id="S4.SS4.1.1" class="ltx_text ltx_font_italic">Neural Architecture Search for ViTs </span>
</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p"><span id="S4.SS4.p1.1.1" class="ltx_text" style="color:#000000;">While Nerual Architecuter Search (NAS) has been well explored for CNNs to find an optimized architecture, it is relatively less explored in Transformers (even for language transformers </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS4.p1.1.2.1" class="ltx_text" style="color:#000000;">[</span><a href="#bib.bib261" title="" class="ltx_ref">261</a>, <a href="#bib.bib262" title="" class="ltx_ref">262</a><span id="S4.SS4.p1.1.3.2" class="ltx_text" style="color:#000000;">]</span></cite><span id="S4.SS4.p1.1.4" class="ltx_text" style="color:#000000;">). Chen </span><span id="S4.SS4.p1.1.5" class="ltx_text ltx_font_italic" style="color:#000000;">et al.<cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS4.p1.1.5.1.1" class="ltx_text ltx_font_upright">[</span><a href="#bib.bib263" title="" class="ltx_ref">263</a><span id="S4.SS4.p1.1.5.2.2" class="ltx_text ltx_font_upright">]</span></cite></span><span id="S4.SS4.p1.1.6" class="ltx_text" style="color:#000000;"> propose a one-shot NAS for vision transformers, called AutoFormer. BossNAS </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS4.p1.1.7.1" class="ltx_text" style="color:#000000;">[</span><a href="#bib.bib264" title="" class="ltx_ref">264</a><span id="S4.SS4.p1.1.8.2" class="ltx_text" style="color:#000000;">]</span></cite><span id="S4.SS4.p1.1.9" class="ltx_text" style="color:#000000;"> searches for a hybrid architecture (CNN and Transformer).</span><span id="S4.SS4.p1.1.10" class="ltx_text"> </span><span id="S4.SS4.p1.1.11" class="ltx_text" style="color:#000000;">Another recent effort studies the trade-off between global and local information in Transformers in the context of vision applications </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS4.p1.1.12.1" class="ltx_text" style="color:#000000;">[</span><a href="#bib.bib265" title="" class="ltx_ref">265</a><span id="S4.SS4.p1.1.13.2" class="ltx_text" style="color:#000000;">]</span></cite><span id="S4.SS4.p1.1.14" class="ltx_text" style="color:#000000;">. It will be insightful to further explore the domain-specific design choices (e.g., the contrasting requirements between language and vision domains) using NAS to design more efficient and light-weight models similar to CNNs </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS4.p1.1.15.1" class="ltx_text" style="color:#000000;">[</span><a href="#bib.bib87" title="" class="ltx_ref">87</a><span id="S4.SS4.p1.1.16.2" class="ltx_text" style="color:#000000;">]</span></cite><span id="S4.SS4.p1.1.17" class="ltx_text" style="color:#000000;">.</span><span id="S4.SS4.p1.1.18" class="ltx_text"></span></p>
</div>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span><span id="S4.SS5.1.1" class="ltx_text ltx_font_italic">Interpretability of Transformers</span>
</h3>

<div id="S4.SS5.p1" class="ltx_para">
<p id="S4.SS5.p1.1" class="ltx_p"><span id="S4.SS5.p1.1.1" class="ltx_text" style="color:#000000;">Through an extensive set of carefully designed experiments, Naseer <span id="S4.SS5.p1.1.1.1" class="ltx_text ltx_font_italic">et al.<cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS5.p1.1.1.1.1.1" class="ltx_text ltx_font_upright">[</span><a href="#bib.bib266" title="" class="ltx_ref">266</a><span id="S4.SS5.p1.1.1.1.2.2" class="ltx_text ltx_font_upright">]</span></cite></span> investigate multiple intriguing properties of ViTs in terms of their generalization and robustness. They show that, compared with CNNs, ViTs demonstrate strong robustness against texture changes and severe occlusions, <em id="S4.SS5.p1.1.1.2" class="ltx_emph ltx_font_italic">e.g.</em>ViTs retain upto 60% top-1 accuracy on ImageNet once 80% of the image content is randomly occluded.
</span>
Given the strong performance of Transformer architectures, it is interesting and critical to interpret their decisions, <em id="S4.SS5.p1.1.2" class="ltx_emph ltx_font_italic">e.g.</em>, by visualizing relevant regions in an image for a given classification decision.
The main challenge is that the attention originating in each layer, gets inter-mixed in the subsequent layers in a complex manner, making it difficult to visualize the relative contribution of input tokens towards final predictions. This is an open problem, however, some recent works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib267" title="" class="ltx_ref">267</a>, <a href="#bib.bib268" title="" class="ltx_ref">268</a>, <a href="#bib.bib269" title="" class="ltx_ref">269</a>]</cite> target enhanced interpretability of Transformers and report encouraging results. Attention roll-out and attention flow methods were proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib268" title="" class="ltx_ref">268</a>]</cite> to estimate the accurate attentions. However, this method functions in an ad-hoc manner and makes simplistic assumptions <em id="S4.SS5.p1.1.3" class="ltx_emph ltx_font_italic">e.g.</em>, input tokens are linearly combined using attention weights across the layers. Chefer <span id="S4.SS5.p1.1.4" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib269" title="" class="ltx_ref">269</a>]</cite> note that the attention scores obtained directly via the self-attention process (encoding relationships between tokens) or reassignments in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib268" title="" class="ltx_ref">268</a>]</cite> do not provide an optimal solution. As an alternative, they propose to assign and propagate <em id="S4.SS5.p1.1.5" class="ltx_emph ltx_font_italic">relevancy scores</em> in the Transformer network such that the sum of relevancy is constant throughout the network. Their design can handle both the positive and negative attributions experienced in the self-attention layer. The proposed framework has an added advantage of being able to provide class-specific visualizations. Despite these seminal works, visualizing and interpreting Transformers is an unsolved problem and methods are needed to obtain spatially precise activation-specific visualizations. Further progress in this direction can help in better understanding the Transformer models, diagnosing any erroneous behaviors and biases in the decision process. It can also help us design novel architectures that can help us avoid any biases.</p>
</div>
</section>
<section id="S4.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.6 </span><span id="S4.SS6.1.1" class="ltx_text ltx_font_italic">Hardware Efficient Designs</span>
</h3>

<div id="S4.SS6.p1" class="ltx_para">
<p id="S4.SS6.p1.1" class="ltx_p">Large-scale Transformer networks can have intensive power and computation requirements, hindering their deployment on edge devices and resource-constrained environments such as internet-of-things (IoT) platforms. Some recent efforts have been reported to compress and accelerate NLP models on embedded systems such as FPGAs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib270" title="" class="ltx_ref">270</a>]</cite>. Li <span id="S4.SS6.p1.1.1" class="ltx_text ltx_font_italic">et al.<cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS6.p1.1.1.1.1" class="ltx_text ltx_font_upright">[</span><a href="#bib.bib270" title="" class="ltx_ref">270</a><span id="S4.SS6.p1.1.1.2.2" class="ltx_text ltx_font_upright">]</span></cite></span> used an enhanced block-circulant matrix-based representation to compress NLP models and proposed a new Field Programmable Gate Array (FPGA) architecture design to efficiently manage resources for high throughput and low latency. They could achieve 27x, 3x and 81x improvements in performance (throughput measured in FPS), reduced power consumption, and energy efficiency relative a CPU for RoBERTa model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. Towards this goal, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib262" title="" class="ltx_ref">262</a>]</cite> proposed to design Hardware-Aware Transformers (HAT) using neural architecture search strategies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib271" title="" class="ltx_ref">271</a>, <a href="#bib.bib272" title="" class="ltx_ref">272</a>, <a href="#bib.bib273" title="" class="ltx_ref">273</a>]</cite>. Specifically, a SuperTransformer model is first trained for performance approximation which can estimate a model’s performance without fully training it. This model comprises the largest possible model in the search space while sharing weights between common parts. Eventually, an evolutionary search is performed considering the hardware latency constraints to find a suitable SubTransformer model for a target hardware platform (<em id="S4.SS6.p1.1.2" class="ltx_emph ltx_font_italic">e.g.</em>, IoT device, GPU, CPU).
However, such hardware efficient designs are currently lacking for the vision Transformers to enable their seamless deployment in resource-constrained devices. Further, the search cost of the evolutionary algorithms remains significant with the associated impact of CO2 emissions on the environment.</p>
</div>
</section>
<section id="S4.SS7" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsection">4.7 </span><span id="S4.SS7.1.1" class="ltx_text ltx_font_italic">Towards Integrating All Modalities</span>
</h3>

<div id="S4.SS7.p1" class="ltx_para">
<p id="S4.SS7.p1.1" class="ltx_p"><span id="S4.SS7.p1.1.1" class="ltx_text" style="color:#000000;">Since Transformers provide a unified design to process different modalities, recent efforts also focus on proposing more generic general purpose reasoning systems based on Transformers.
Inspired by the biological systems that can process information from a diverse range of modalities, Perceiver model </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS7.p1.1.2.1" class="ltx_text" style="color:#000000;">[</span><a href="#bib.bib274" title="" class="ltx_ref">274</a><span id="S4.SS7.p1.1.3.2" class="ltx_text" style="color:#000000;">]</span></cite><span id="S4.SS7.p1.1.4" class="ltx_text" style="color:#000000;"> aims to learn a unified model that can process any given input modality without making domain-specific architectural assumptions. In order to scale to high-dimensional inputs, Perceiver uses an asymmetric cross attention method to distill input information into low-dimensional latent bottleneck features. Once the features are distilled in a compact and fixed-dimensional form, regular Transformer blocks are applied in the latent space. The original Perceiver model shows performance competitive to ResNets and ViTs on image classification and can process 3D data, audio, images, video or their combinations. However, this model can only generate fixed outputs e.g., class probabilities. A recent improvement called Perceiver IO </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS7.p1.1.5.1" class="ltx_text" style="color:#000000;">[</span><a href="#bib.bib275" title="" class="ltx_ref">275</a><span id="S4.SS7.p1.1.6.2" class="ltx_text" style="color:#000000;">]</span></cite><span id="S4.SS7.p1.1.7" class="ltx_text" style="color:#000000;"> aims to learn models with both flexible inputs as well as arbitrary sized outputs. This allows application to problems which demand structured outputs such as natural language tasks and visual comprehension. While these models avoid modality dependent architectural choices, the learning itself still involves modality dependent choices e.g., specific augmentations or positional encodings. An interesting and open future direction is to achieve total modality-agnosticism in the learning pipeline. </span><span id="S4.SS7.p1.1.8" class="ltx_text"></span></p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">Attention has played a key role in delivering efficient and accurate computer vision systems, while simultaneously providing insights into the function of deep neural networks. This survey reviews the self-attention approaches and specifically focuses on the Transformer and bi-directional encoding architectures that are built on the principle of self-attention. We first cover fundamental concepts pertaining to self-attention architectures and later provide an in-depth analysis of competing approaches for a broad range of computer vision applications. Specifically, we include state of the art self-attention models for image recognition, object detection, semantic and instance segmentation, video analysis and classification, visual question answering, visual commonsense reasoning, image captioning, vision-language navigation, clustering, few-shot learning, and 3D data analysis. We systematically highlight the key strengths and limitations of the existing methods and particularly elaborate on the important future research directions. With its specific focus on computer vision tasks, this survey provides a unique view of the recent progress in self-attention and Transformer-based methods. We hope this effort will drive further interest in the vision community to leverage the potential of Transformer models and improve on their current limitations <em id="S5.p1.1.1" class="ltx_emph ltx_font_italic">e.g.</em>, reducing their carbon footprint.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Acknowledgments</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p"><span id="Sx1.p1.1.1" class="ltx_text" style="font-size:80%;">The authors would like to thank Tim Prangemeier (TU Darmstadt), Luowei Zhou (Microsoft Research), Jason Corso (University of Michigan), Pichao Wang (Alibaba Group), Yuqing Wang (Meituan), Alex Meinke (Uni-Tuebingen), Irwan Bello (Google Brain) and Manoj Kumar (Google Brain) for their helpful feedback on the survey. We would also like to thank Mohamed Afham for his help with a figure. </span></p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” in <span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">NeurIPS</span>, 2017.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
M. Ott, S. Edunov, D. Grangier, and M. Auli, “Scaling neural machine
translation,” in <span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">WMT</span>, 2018.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training of
deep bidirectional transformers for language understanding,” <span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">arXiv
preprint arXiv:1810.04805</span>, 2018.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever, “Improving language
understanding by generative pre-training,” tech. rep., OpenAI, 2018.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever, “Language
models are unsupervised multitask learners,” tech. rep., OpenAI, 2019.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal,
A. Neelakantan, P. Shyam, G. Sastry, A. Askell, <span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">et al.</span>, “Language
models are few-shot learners,” <span id="bib.bib6.2.2" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2005.14165</span>, 2020.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,
L. Zettlemoyer, and V. Stoyanov, “RoBERTa: A robustly optimized bert
pretraining approach,” <span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1907.11692</span>, 2019.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou,
W. Li, and P. J. Liu, “Exploring the limits of transfer learning with a
unified text-to-text transformer,” <span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1910.10683</span>,
2019.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
D. Lepikhin, H. Lee, Y. Xu, D. Chen, O. Firat, Y. Huang, M. Krikun, N. Shazeer,
and Z. Chen, “Gshard: Scaling giant models with conditional computation and
automatic sharding,” <span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2006.16668</span>, 2020.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
W. Fedus, B. Zoph, and N. Shazeer, “Switch transformers: Scaling to trillion
parameter models with simple and efficient sparsity,” <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">arXiv preprint
arXiv:2101.03961</span>.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,
T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, <span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">et al.</span>,
“An image is worth 16x16 words: Transformers for image recognition at
scale,” <span id="bib.bib11.2.2" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2010.11929</span>, 2020.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and H. Jégou,
“Training data-efficient image transformers &amp; distillation through
attention,” <span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2012.12877</span>, 2020.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko,
“End-to-end object detection with transformers,” <span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">arXiv preprint
arXiv:2005.12872</span>, 2020.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
X. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai, “Deformable DETR:
Deformable transformers for end-to-end object detection,” <span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">arXiv
preprint arXiv:2010.04159</span>, 2020.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
L. Ye, M. Rochan, Z. Liu, and Y. Wang, “Cross-modal self-attention network for
referring image segmentation,” in <span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">CVPR</span>, 2019.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
F. Yang, H. Yang, J. Fu, H. Lu, and B. Guo, “Learning texture transformer
network for image super-resolution,” in <span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">CVPR</span>, 2020.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
C. Sun, A. Myers, C. Vondrick, K. Murphy, and C. Schmid, “VideoBERT: A joint
model for video and language representation learning,” in <span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">ICCV</span>, 2019.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
R. Girdhar, J. Carreira, C. Doersch, and A. Zisserman, “Video action
transformer network,” in <span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">CVPR</span>, 2019.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
H. Chen, Y. Wang, T. Guo, C. Xu, Y. Deng, Z. Liu, S. Ma, C. Xu, C. Xu, and
W. Gao, “Pre-trained image processing transformer,” <span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">arXiv preprint
arXiv:2012.00364</span>, 2020.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
A. Ramesh, M. Pavlov, G. Goh, and S. Gray, “DALL·E: Creating images from
text,” tech. rep., OpenAI, 2021.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
H. Tan and M. Bansal, “LXMERT: Learning cross-modality encoder
representations from transformers,” in <span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">EMNLP-IJCNLP</span>, 2019.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
W. Su, X. Zhu, Y. Cao, B. Li, L. Lu, F. Wei, and J. Dai, “VL-BERT:
Pre-training of generic visual-linguistic representations,” <span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">arXiv
preprint arXiv:1908.08530</span>, 2019.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
X. Wang, C. Yeshwanth, and M. Nießner, “SceneFormer: Indoor scene
generation with transformers,” <span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2012.09793</span>, 2020.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
M. Kumar, D. Weissenborn, and N. Kalchbrenner, “Colorization transformer,” in
<span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">ICLR</span>, 2021.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
C. Doersch, A. Gupta, and A. Zisserman, “CrossTransformers: spatially-aware
few-shot transfer,” <span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">NeurIPS</span>, 2020.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
H.-J. Ye, H. Hu, D.-C. Zhan, and F. Sha, “Few-shot learning via embedding
adaptation with set-to-set functions,” in <span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">CVPR</span>, 2020.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
S. Chaudhari, G. Polatkan, R. Ramanath, and V. Mithal, “An attentive survey of
attention models,” <span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1904.02874</span>, 2019.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
A. de Santana Correia and E. L. Colombini, “Attention, please! asurvey of
neural attention models in deep learning,” <span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">arXiv preprint
arXiv:2103.16775</span>, 2021.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
O. Vinyals, A. Toshev, S. Bengio, and D. Erhan, “Show and tell: A neural image
caption generator,” in <span id="bib.bib29.1.1" class="ltx_text ltx_font_italic">CVPR</span>, 2015.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Y. Bengio, I. Goodfellow, and A. Courville, <span id="bib.bib30.1.1" class="ltx_text ltx_font_italic">Deep learning</span>.

</span>
<span class="ltx_bibblock">MIT press, 2017.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” <span id="bib.bib31.1.1" class="ltx_text ltx_font_italic">Nature</span>, 2015.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
S. Hochreiter and J. Schmidhuber, “Long short-term memory,” <span id="bib.bib32.1.1" class="ltx_text ltx_font_italic">Neural
computation</span>, 1997.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
D. Hu, “An introductory survey on attention mechanisms in nlp problems,” in
<span id="bib.bib33.1.1" class="ltx_text ltx_font_italic">IntelliSys</span>, 2019.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Y. Tay, M. Dehghani, D. Bahri, and D. Metzler, “Efficient transformers: A
survey,” <span id="bib.bib34.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2009.06732</span>, 2020.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
L. Yuan, Y. Chen, T. Wang, W. Yu, Y. Shi, F. E. Tay, J. Feng, and S. Yan,
“Tokens-to-token vit: Training vision transformers from scratch on
imagenet,” <span id="bib.bib35.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2101.11986</span>, 2021.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo, “Swin
transformer: Hierarchical vision transformer using shifted windows,” <span id="bib.bib36.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2103.14030</span>, 2021.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
X. Chu, Z. Tian, Y. Wang, B. Zhang, H. Ren, X. Wei, H. Xia, and C. Shen,
“Twins: Revisiting the design of spatial attention in vision transformers,”
<span id="bib.bib37.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2104.13840</span>, 2021.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
S. Wang, B. Li, M. Khabsa, H. Fang, and H. Ma, “Linformer: Self-attention with
linear complexity,” <span id="bib.bib38.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2006.04768</span>, 2020.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
H. Zhang, I. Goodfellow, D. Metaxas, and A. Odena, “Self-attention generative
adversarial networks,” in <span id="bib.bib39.1.1" class="ltx_text ltx_font_italic">International conference on machine
learning</span>, pp. 7354–7363, PMLR, 2019.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
J. Pérez, J. Marinković, and P. Barceló, “On the turing
completeness of modern neural network architectures,” in <span id="bib.bib40.1.1" class="ltx_text ltx_font_italic">International
Conference on Learning Representations</span>, 2018.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
J.-B. Cordonnier, A. Loukas, and M. Jaggi, “On the relationship between
self-attention and convolutional layers,” in <span id="bib.bib41.1.1" class="ltx_text ltx_font_italic">International Conference
on Learning Representations</span>, 2019.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
J. Dai, H. Qi, Y. Xiong, Y. Li, G. Zhang, H. Hu, and Y. Wei, “Deformable
convolutional networks,” in <span id="bib.bib42.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE international
conference on computer vision</span>, pp. 764–773, 2017.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Y.-C. Chen, L. Li, L. Yu, A. El Kholy, F. Ahmed, Z. Gan, Y. Cheng, and J. Liu,
“UNITER: Universal image-text representation learning,” in <span id="bib.bib43.1.1" class="ltx_text ltx_font_italic">ECCV</span>,
2020.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
X. Li, X. Yin, C. Li, P. Zhang, X. Hu, L. Zhang, L. Wang, H. Hu, L. Dong,
F. Wei, <span id="bib.bib44.1.1" class="ltx_text ltx_font_italic">et al.</span>, “Oscar: Object-semantics aligned pre-training for
vision-language tasks,” in <span id="bib.bib44.2.2" class="ltx_text ltx_font_italic">ECCV</span>, 2020.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
K. Lin, L. Wang, and Z. Liu, “End-to-end human pose and mesh reconstruction
with transformers,” <span id="bib.bib45.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2012.09760</span>, 2020.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
S. Gidaris, P. Singh, and N. Komodakis, “Unsupervised representation learning
by predicting image rotations,” in <span id="bib.bib46.1.1" class="ltx_text ltx_font_italic">ICLR</span>, 2018.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
“Revisiting the unreasonable effectiveness of data.”
<a target="_blank" href="https://ai.googleblog.com/2017/07/revisiting-unreasonable-effectiveness.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://ai.googleblog.com/2017/07/revisiting-unreasonable-effectiveness.html</a>.

</span>
<span class="ltx_bibblock">Accessed: 2020-12-31.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
L. Jing and Y. Tian, “Self-supervised visual feature learning with deep neural
networks: A survey,” <span id="bib.bib48.1.1" class="ltx_text ltx_font_italic">TPAMI</span>, 2020.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
X. Liu, F. Zhang, Z. Hou, Z. Wang, L. Mian, J. Zhang, and J. Tang,
“Self-supervised learning: Generative or contrastive,” <span id="bib.bib49.1.1" class="ltx_text ltx_font_italic">arXiv preprint
arXiv:2006.08218</span>, 2020.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
“Aaai 2020 keynotes turing award winners event.”
<a target="_blank" href="https://www.youtube.com/watch?v=UX8OubxsY8w" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.youtube.com/watch?v=UX8OubxsY8w</a>.

</span>
<span class="ltx_bibblock">Accessed: 2020-12-31.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
R. Zhang, P. Isola, and A. A. Efros, “Colorful image colorization,” in <span id="bib.bib51.1.1" class="ltx_text ltx_font_italic">ECCV</span>, 2016.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
C. Ledig, L. Theis, F. Huszár, J. Caballero, A. Cunningham, A. Acosta,
A. Aitken, A. Tejani, J. Totz, Z. Wang, <span id="bib.bib52.1.1" class="ltx_text ltx_font_italic">et al.</span>, “Photo-realistic
single image super-resolution using a generative adversarial network,” in
<span id="bib.bib52.2.2" class="ltx_text ltx_font_italic">CVPR</span>, 2017.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
D. Pathak, P. Krahenbuhl, J. Donahue, T. Darrell, and A. Efros, “Context
encoders: Feature learning by inpainting,” in <span id="bib.bib53.1.1" class="ltx_text ltx_font_italic">CVPR</span>, 2016.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
A. Courville, and Y. Bengio, “Generative adversarial nets,” in <span id="bib.bib54.1.1" class="ltx_text ltx_font_italic">NeurIPS</span>, 2014.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
D. Lin, K. Fu, Y. Wang, G. Xu, and X. Sun, “MARTA GANs: Unsupervised
representation learning for remote sensing image classification,” <span id="bib.bib55.1.1" class="ltx_text ltx_font_italic">GRSL</span>, 2017.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
U. Ahsan, R. Madhok, and I. Essa, “Video jigsaw: Unsupervised learning of
spatiotemporal context for video action recognition,” in <span id="bib.bib56.1.1" class="ltx_text ltx_font_italic">WACV</span>, 2019.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
M. Noroozi and P. Favaro, “Unsupervised learning of visual representations by
solving jigsaw puzzles,” in <span id="bib.bib57.1.1" class="ltx_text ltx_font_italic">ECCV</span>, 2016.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
D. Kim, D. Cho, D. Yoo, and I. S. Kweon, “Learning image representations by
completing damaged jigsaw puzzles,” <span id="bib.bib58.1.1" class="ltx_text ltx_font_italic">WACV</span>, 2018.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
L. Jing, X. Yang, J. Liu, and Y. Tian, “Self-supervised spatiotemporal feature
learning via video rotation prediction,” <span id="bib.bib59.1.1" class="ltx_text ltx_font_italic">arXiv preprint
arXiv:1811.11387</span>, 2018.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
H.-Y. Lee, J.-B. Huang, M. Singh, and M.-H. Yang, “Unsupervised representation
learning by sorting sequences,” in <span id="bib.bib60.1.1" class="ltx_text ltx_font_italic">ICCV</span>, 2017.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
I. Misra, C. L. Zitnick, and M. Hebert, “Shuffle and learn: unsupervised
learning using temporal order verification,” in <span id="bib.bib61.1.1" class="ltx_text ltx_font_italic">ECCV</span>, 2016.

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
D. Wei, J. J. Lim, A. Zisserman, and W. T. Freeman, “Learning and using the
arrow of time,” in <span id="bib.bib62.1.1" class="ltx_text ltx_font_italic">CVPR</span>, 2018.

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
L. H. Li, M. Yatskar, D. Yin, C.-J. Hsieh, and K.-W. Chang, “VisualBERT: A
simple and performant baseline for vision and language,” in <span id="bib.bib63.1.1" class="ltx_text ltx_font_italic">Arxiv
preprint arXiv:1908.03557</span>, 2019.

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
B. Korbar, D. Tran, and L. T., “Cooperative learning of audio and video models
from self-supervised synchronization,” in <span id="bib.bib64.1.1" class="ltx_text ltx_font_italic">NeurIPS</span>, 2018.

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
R. Arandjelovic and A. Zisserman, “Look, listen and learn,” in <span id="bib.bib65.1.1" class="ltx_text ltx_font_italic">ICCV</span>,
2017.

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
N. Sayed, B. Brattoli, and B. Ommer, “Cross and learn: Cross-modal
self-supervision,” in <span id="bib.bib66.1.1" class="ltx_text ltx_font_italic">GCPR</span>, 2018.

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” in <span id="bib.bib67.1.1" class="ltx_text ltx_font_italic">CVPR</span>, 2016.

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
J. L. Ba, J. R. Kiros, and G. E. Hinton, “Layer normalization,” <span id="bib.bib68.1.1" class="ltx_text ltx_font_italic">arXiv
preprint arXiv:1607.06450</span>, 2016.

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
A. Buades, B. Coll, and J.-M. Morel, “A non-local algorithm for image
denoising,” in <span id="bib.bib69.1.1" class="ltx_text ltx_font_italic">CVPR</span>, 2005.

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">
X. Wang, R. Girshick, A. Gupta, and K. He, “Non-local neural networks,” in
<span id="bib.bib70.1.1" class="ltx_text ltx_font_italic">CVPR</span>, 2018.

</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">
W. Kay, J. Carreira, K. Simonyan, B. Zhang, C. Hillier, S. Vijayanarasimhan,
F. Viola, T. Green, T. Back, P. Natsev, <span id="bib.bib71.1.1" class="ltx_text ltx_font_italic">et al.</span>, “The kinetics human
action video dataset,” <span id="bib.bib71.2.2" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1705.06950</span>, 2017.

</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock">
Z. Huang, X. Wang, L. Huang, C. Huang, Y. Wei, and W. Liu, “CCNet:
Criss-cross attention for semantic segmentation,” in <span id="bib.bib72.1.1" class="ltx_text ltx_font_italic">ICCV</span>, 2019.

</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock">
M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson,
U. Franke, S. Roth, and B. Schiele, “The cityscapes dataset for semantic
urban scene understanding,” in <span id="bib.bib73.1.1" class="ltx_text ltx_font_italic">CVPR</span>, 2016.

</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock">
B. Zhou, H. Zhao, X. Puig, S. Fidler, A. Barriuso, and A. Torralba, “Scene
parsing through ade20k dataset,” in <span id="bib.bib74.1.1" class="ltx_text ltx_font_italic">CVPR</span>, 2017.

</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock">
T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,
P. Dollár, and C. L. Zitnick, “Microsoft COCO: Common objects in
context,” in <span id="bib.bib75.1.1" class="ltx_text ltx_font_italic">ECCV</span>, 2014.

</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock">
X. Liang, K. Gong, X. Shen, and L. Lin, “Look into person: Joint body parsing
&amp; pose estimation network and a new benchmark,” <span id="bib.bib76.1.1" class="ltx_text ltx_font_italic">TPAMI</span>, 2018.

</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock">
G. J. Brostow, J. Fauqueur, and R. Cipolla, “Semantic object classes in video:
A high-definition ground truth database,” <span id="bib.bib77.1.1" class="ltx_text ltx_font_italic">Pattern Recognition Letters</span>,
2009.

</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[78]</span>
<span class="ltx_bibblock">
H. Hu, Z. Zhang, Z. Xie, and S. Lin, “Local relation networks for image
recognition,” in <span id="bib.bib78.1.1" class="ltx_text ltx_font_italic">ICCV</span>, 2019.

</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[79]</span>
<span class="ltx_bibblock">
I. Bello, B. Zoph, A. Vaswani, J. Shlens, and Q. V. Le, “Attention augmented
convolutional networks,” in <span id="bib.bib79.1.1" class="ltx_text ltx_font_italic">ICCV</span>, 2019.

</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[80]</span>
<span class="ltx_bibblock">
P. Shaw, J. Uszkoreit, and A. Vaswani, “Self-attention with relative position
representations,” in <span id="bib.bib80.1.1" class="ltx_text ltx_font_italic">NAACL</span>, 2018.

</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[81]</span>
<span class="ltx_bibblock">
N. Parmar, P. Ramachandran, A. Vaswani, I. Bello, A. Levskaya, and J. Shlens,
“Stand-alone self-attention in vision models,” in <span id="bib.bib81.1.1" class="ltx_text ltx_font_italic">NeurIPS</span>, 2019.

</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[82]</span>
<span class="ltx_bibblock">
H. Zhao, J. Jia, and V. Koltun, “Exploring self-attention for image
recognition,” in <span id="bib.bib82.1.1" class="ltx_text ltx_font_italic">CVPR</span>, 2020.

</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[83]</span>
<span class="ltx_bibblock">
C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and
R. Fergus, “Intriguing properties of neural networks,” <span id="bib.bib83.1.1" class="ltx_text ltx_font_italic">arXiv preprint
arXiv:1312.6199</span>, 2013.

</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[84]</span>
<span class="ltx_bibblock">
M. M. Naseer, S. H. Khan, M. H. Khan, F. S. Khan, and F. Porikli,
“Cross-domain transferability of adversarial perturbations,” in <span id="bib.bib84.1.1" class="ltx_text ltx_font_italic">NeurIPS</span>, 2019.

</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[85]</span>
<span class="ltx_bibblock">
M. Naseer, K. Ranasinghe, S. Khan, F. S. Khan, and F. Porikli, “On improving
adversarial transferability of vision transformers,” <span id="bib.bib85.1.1" class="ltx_text ltx_font_italic">arXiv preprint
arXiv:2106.04169</span>, 2021.

</span>
</li>
<li id="bib.bib86" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[86]</span>
<span class="ltx_bibblock">
I. Radosavovic, R. P. Kosaraju, R. Girshick, K. He, and P. Dollár,
“Designing network design spaces,” in <span id="bib.bib86.1.1" class="ltx_text ltx_font_italic">CVPR</span>, 2020.

</span>
</li>
<li id="bib.bib87" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[87]</span>
<span class="ltx_bibblock">
M. Tan and Q. V. Le, “EfficientNet: Rethinking model scaling for
convolutional neural networks,” in <span id="bib.bib87.1.1" class="ltx_text ltx_font_italic">ICML</span>, 2019.

</span>
</li>
<li id="bib.bib88" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[88]</span>
<span class="ltx_bibblock">
K. Han, A. Xiao, E. Wu, J. Guo, C. Xu, and Y. Wang, “Transformer in
transformer,” <span id="bib.bib88.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2103.00112</span>, 2021.

</span>
</li>
<li id="bib.bib89" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[89]</span>
<span class="ltx_bibblock">
Z. Jiang, Q. Hou, L. Yuan, D. Zhou, Y. Shi, X. Jin, A. Wang, and J. Feng, “All
tokens matter: Token labeling for training better vision transformers,” <span id="bib.bib89.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2104.10858</span>, 2021.

</span>
</li>
<li id="bib.bib90" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[90]</span>
<span class="ltx_bibblock">
S. Yun, D. Han, S. J. Oh, S. Chun, J. Choe, and Y. Yoo, “Cutmix:
Regularization strategy to train strong classifiers with localizable
features,” in <span id="bib.bib90.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF International Conference on
Computer Vision</span>, pp. 6023–6032, 2019.

</span>
</li>
<li id="bib.bib91" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[91]</span>
<span class="ltx_bibblock">
A. El-Nouby, H. Touvron, M. Caron, P. Bojanowski, M. Douze, A. Joulin,
I. Laptev, N. Neverova, G. Synnaeve, J. Verbeek, and H. Jegou, “Xcit:
Cross-covariance image transformers,” 2021.

</span>
</li>
<li id="bib.bib92" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[92]</span>
<span class="ltx_bibblock">
D. Zhou, B. Kang, X. Jin, L. Yang, X. Lian, Z. Jiang, Q. Hou, and J. Feng,
“Deepvit: Towards deeper vision transformer,” 2021.

</span>
</li>
<li id="bib.bib93" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[93]</span>
<span class="ltx_bibblock">
W. Wang, E. Xie, X. Li, D.-P. Fan, K. Song, D. Liang, T. Lu, P. Luo, and
L. Shao, “Pyramid vision transformer: A versatile backbone for dense
prediction without convolutions,” <span id="bib.bib93.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2102.12122</span>,
2021.

</span>
</li>
<li id="bib.bib94" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[94]</span>
<span class="ltx_bibblock">
J. Yang, C. Li, P. Zhang, X. Dai, B. Xiao, L. Yuan, and J. Gao, “Focal
self-attention for local-global interactions in vision transformers,” 2021.

</span>
</li>
<li id="bib.bib95" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[95]</span>
<span class="ltx_bibblock">
Z. Huang, Y. Ben, G. Luo, P. Cheng, G. Yu, and B. Fu, “Shuffle transformer:
Rethinking spatial shuffle for vision transformer,” 2021.

</span>
</li>
<li id="bib.bib96" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[96]</span>
<span class="ltx_bibblock">
H. Wu, B. Xiao, N. Codella, M. Liu, X. Dai, L. Yuan, and L. Zhang, “Cvt:
Introducing convolutions to vision transformers,” <span id="bib.bib96.1.1" class="ltx_text ltx_font_italic">arXiv preprint
arXiv:2103.15808</span>, 2021.

</span>
</li>
<li id="bib.bib97" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[97]</span>
<span class="ltx_bibblock">
W. Wang, E. Xie, X. Li, D.-P. Fan, K. Song, D. Liang, T. Lu, P. Luo, and
L. Shao, “Pvtv2: Improved baselines with pyramid vision transformer,” 2021.

</span>
</li>
<li id="bib.bib98" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[98]</span>
<span class="ltx_bibblock">
W. Xu, Y. Xu, T. Chang, and Z. Tu, “Co-scale conv-attentional image
transformers,” 2021.

</span>
</li>
<li id="bib.bib99" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[99]</span>
<span class="ltx_bibblock">
W. Wang, L. Yao, L. Chen, D. Cai, X. He, and W. Liu, “Crossformer: A versatile
vision transformer based on cross-scale attention,” <span id="bib.bib99.1.1" class="ltx_text ltx_font_italic">arXiv preprint
arXiv:2108.00154</span>, 2021.

</span>
</li>
<li id="bib.bib100" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[100]</span>
<span class="ltx_bibblock">
C.-F. Chen, R. Panda, and Q. Fan, “Regionvit: Regional-to-local attention for
vision transformers,” 2021.

</span>
</li>
<li id="bib.bib101" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[101]</span>
<span class="ltx_bibblock">
E. Xie, W. Wang, Z. Yu, A. Anandkumar, J. M. Alvarez, and P. Luo, “Segformer:
Simple and efficient design for semantic segmentation with transformers,”
2021.

</span>
</li>
<li id="bib.bib102" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[102]</span>
<span class="ltx_bibblock">
P. Zhang, X. Dai, J. Yang, B. Xiao, L. Yuan, L. Zhang, and J. Gao,
“Multi-scale vision longformer: A new vision transformer for high-resolution
image encoding,” <span id="bib.bib102.1.1" class="ltx_text ltx_font_italic">ICCV 2021</span>, 2021.

</span>
</li>
<li id="bib.bib103" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[103]</span>
<span class="ltx_bibblock">
I. Beltagy, M. E. Peters, and A. Cohan, “Longformer: The long-document
transformer,” <span id="bib.bib103.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2004.05150</span>, 2020.

</span>
</li>
<li id="bib.bib104" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[104]</span>
<span class="ltx_bibblock">
C.-F. Chen, Q. Fan, and R. Panda, “Crossvit: Cross-attention multi-scale
vision transformer for image classification,” <span id="bib.bib104.1.1" class="ltx_text ltx_font_italic">arXiv preprint
arXiv:2103.14899</span>, 2021.

</span>
</li>
<li id="bib.bib105" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[105]</span>
<span class="ltx_bibblock">
K. Yuan, S. Guo, Z. Liu, A. Zhou, F. Yu, and W. Wu, “Incorporating convolution
designs into visual transformers,” <span id="bib.bib105.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2103.11816</span>,
2021.

</span>
</li>
<li id="bib.bib106" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[106]</span>
<span class="ltx_bibblock">
A. Hassani, S. Walton, N. Shah, A. Abuduweili, J. Li, and H. Shi, “Escaping
the big data paradigm with compact transformers,” 2021.

</span>
</li>
<li id="bib.bib107" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[107]</span>
<span class="ltx_bibblock">
Y. Li, K. Zhang, J. Cao, R. Timofte, and L. V. Gool, “Localvit: Bringing
locality to vision transformers,” 2021.

</span>
</li>
<li id="bib.bib108" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[108]</span>
<span class="ltx_bibblock">
B. Graham, A. El-Nouby, H. Touvron, P. Stock, A. Joulin, H. Jégou, and
M. Douze, “Levit: a vision transformer in convnet’s clothing for faster
inference,” 2021.

</span>
</li>
<li id="bib.bib109" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[109]</span>
<span class="ltx_bibblock">
Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and
L. D. Jackel, “Backpropagation applied to handwritten zip code
recognition,” <span id="bib.bib109.1.1" class="ltx_text ltx_font_italic">Neural computation</span>, vol. 1, no. 4, pp. 541–551, 1989.

</span>
</li>
<li id="bib.bib110" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[110]</span>
<span class="ltx_bibblock">
Q. Zhang and Y. Yang, “Rest: An efficient transformer for visual
recognition,” <span id="bib.bib110.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2105.13677</span>, 2021.

</span>
</li>
<li id="bib.bib111" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[111]</span>
<span class="ltx_bibblock">
Z. Zhang, H. Zhang, L. Zhao, T. Chen, and T. Pfister, “Aggregating nested
transformers,” in <span id="bib.bib111.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2105.12723</span>, 2021.

</span>
</li>
<li id="bib.bib112" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[112]</span>
<span class="ltx_bibblock">
Z. Dai, H. Liu, Q. V. Le, and M. Tan, “Coatnet: Marrying convolution and
attention for all data sizes,” 2021.

</span>
</li>
<li id="bib.bib113" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[113]</span>
<span class="ltx_bibblock">
X. Chu, Z. Tian, B. Zhang, X. Wang, X. Wei, H. Xia, and C. Shen, “Conditional
positional encodings for vision transformers,” 2021.

</span>
</li>
<li id="bib.bib114" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[114]</span>
<span class="ltx_bibblock">
Y. Liu, G. Sun, Y. Qiu, L. Zhang, A. Chhatkuli, and L. Van Gool, “Transformer
in convolutional neural networks,” <span id="bib.bib114.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2106.03180</span>,
2021.

</span>
</li>
<li id="bib.bib115" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[115]</span>
<span class="ltx_bibblock">
X. Chen, S. Xie, and K. He, “An empirical study of training self-supervised
visual transformers,” <span id="bib.bib115.1.1" class="ltx_text ltx_font_italic">arXiv e-prints</span>, pp. arXiv–2104, 2021.

</span>
</li>
<li id="bib.bib116" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[116]</span>
<span class="ltx_bibblock">
X. Chen, H. Fan, R. Girshick, and K. He, “Improved baselines with momentum
contrastive learning,” <span id="bib.bib116.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2003.04297</span>, 2020.

</span>
</li>
<li id="bib.bib117" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[117]</span>
<span class="ltx_bibblock">
K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick, “Momentum contrast for
unsupervised visual representation learning,” in <span id="bib.bib117.1.1" class="ltx_text ltx_font_italic">Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>,
pp. 9729–9738, 2020.

</span>
</li>
<li id="bib.bib118" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[118]</span>
<span class="ltx_bibblock">
Z. Xie, Y. Lin, Z. Yao, Z. Zhang, Q. Dai, Y. Cao, and H. Hu, “Self-supervised
learning with swin transformers,” <span id="bib.bib118.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2105.04553</span>,
2021.

</span>
</li>
<li id="bib.bib119" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[119]</span>
<span class="ltx_bibblock">
J.-B. Grill, F. Strub, F. Altché, C. Tallec, P. H. Richemond,
E. Buchatskaya, C. Doersch, B. A. Pires, Z. D. Guo, M. G. Azar, <span id="bib.bib119.1.1" class="ltx_text ltx_font_italic">et al.</span>,
“Bootstrap your own latent: A new approach to self-supervised learning,”
<span id="bib.bib119.2.2" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2006.07733</span>, 2020.

</span>
</li>
<li id="bib.bib120" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[120]</span>
<span class="ltx_bibblock">
M. Caron, H. Touvron, I. Misra, H. Jégou, J. Mairal, P. Bojanowski, and
A. Joulin, “Emerging properties in self-supervised vision transformers,”
<span id="bib.bib120.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2104.14294</span>, 2021.

</span>
</li>
<li id="bib.bib121" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[121]</span>
<span class="ltx_bibblock">
C. Li, J. Yang, P. Zhang, M. Gao, B. Xiao, X. Dai, L. Yuan, and J. Gao,
“Efficient self-supervised vision transformers for representation
learning,” <span id="bib.bib121.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2106.09785</span>, 2021.

</span>
</li>
<li id="bib.bib122" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[122]</span>
<span class="ltx_bibblock">
Y. Wang, X. Zhang, T. Yang, and J. Sun, “Anchor detr: Query design for
transformer-based detector,” 2021.

</span>
</li>
<li id="bib.bib123" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[123]</span>
<span class="ltx_bibblock">
T. Chen, S. Saxena, L. Li, D. J. Fleet, and G. Hinton, “Pix2seq: A language
modeling framework for object detection,” 2021.

</span>
</li>
<li id="bib.bib124" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[124]</span>
<span class="ltx_bibblock">
Y. Fang, B. Liao, X. Wang, J. Fang, J. Qi, R. Wu, J. Niu, and W. Liu, “You
only look at one sequence: Rethinking transformer in vision through object
detection,” 2021.

</span>
</li>
<li id="bib.bib125" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[125]</span>
<span class="ltx_bibblock">
S. Ren, K. He, R. Girshick, and J. Sun, “Faster R-CNN: Towards real-time
object detection with region proposal networks,” <span id="bib.bib125.1.1" class="ltx_text ltx_font_italic">TPAMI</span>, 2016.

</span>
</li>
<li id="bib.bib126" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[126]</span>
<span class="ltx_bibblock">
R. Girshick, “Fast R-CNN,” in <span id="bib.bib126.1.1" class="ltx_text ltx_font_italic">ICCV</span>, 2015.

</span>
</li>
<li id="bib.bib127" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[127]</span>
<span class="ltx_bibblock">
K. He, G. Gkioxari, P. Dollár, and R. Girshick, “Mask R-CNN,” in <span id="bib.bib127.1.1" class="ltx_text ltx_font_italic">ICCV</span>, 2017.

</span>
</li>
<li id="bib.bib128" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[128]</span>
<span class="ltx_bibblock">
J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You only look once:
Unified, real-time object detection,” in <span id="bib.bib128.1.1" class="ltx_text ltx_font_italic">CVPR</span>, 2016.

</span>
</li>
<li id="bib.bib129" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[129]</span>
<span class="ltx_bibblock">
W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, and A. C. Berg,
“SSD: Single shot multibox detector,” in <span id="bib.bib129.1.1" class="ltx_text ltx_font_italic">ECCV</span>, 2016.

</span>
</li>
<li id="bib.bib130" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[130]</span>
<span class="ltx_bibblock">
T. Prangemeier, C. Reich, and H. Koeppl, “Attention-based transformers for
instance segmentation of cells in microstructures,” in <span id="bib.bib130.1.1" class="ltx_text ltx_font_italic">2020 IEEE
International Conference on Bioinformatics and Biomedicine (BIBM)</span>,
pp. 700–707, IEEE, 2020.

</span>
</li>
<li id="bib.bib131" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[131]</span>
<span class="ltx_bibblock">
T.-Y. Lin, P. Dollár, R. Girshick, K. He, B. Hariharan, and S. Belongie,
“Feature pyramid networks for object detection,” in <span id="bib.bib131.1.1" class="ltx_text ltx_font_italic">CVPR</span>, 2017.

</span>
</li>
<li id="bib.bib132" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[132]</span>
<span class="ltx_bibblock">
A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,
T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit,
and N. Houlsby, “An image is worth 16x16 words: Transformers for image
recognition at scale,” 2020.

</span>
</li>
<li id="bib.bib133" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[133]</span>
<span class="ltx_bibblock">
H. Wang, Y. Zhu, B. Green, H. Adam, A. Yuille, and L.-C. Chen,
“Axial-DeepLab: Stand-alone axial-attention for panoptic segmentation,”
<span id="bib.bib133.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2003.07853</span>, 2020.

</span>
</li>
<li id="bib.bib134" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[134]</span>
<span class="ltx_bibblock">
S. Zheng, J. Lu, H. Zhao, X. Zhu, Z. Luo, Y. Wang, Y. Fu, J. Feng, T. Xiang,
P. H. S. Torr, and L. Zhang, “Rethinking semantic segmentation from a
sequence-to-sequence perspective with transformers,” 2021.

</span>
</li>
<li id="bib.bib135" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[135]</span>
<span class="ltx_bibblock">
R. Strudel, R. Garcia, I. Laptev, and C. Schmid, “Segmenter: Transformer for
semantic segmentation,” 2021.

</span>
</li>
<li id="bib.bib136" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[136]</span>
<span class="ltx_bibblock">
A. Kirillov, K. He, R. Girshick, C. Rother, and P. Dollár, “Panoptic
segmentation,” in <span id="bib.bib136.1.1" class="ltx_text ltx_font_italic">CVPR</span>, 2019.

</span>
</li>
<li id="bib.bib137" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[137]</span>
<span class="ltx_bibblock">
G. Neuhold, T. Ollmann, S. Rota Bulo, and P. Kontschieder, “The mapillary
vistas dataset for semantic understanding of street scenes,” in <span id="bib.bib137.1.1" class="ltx_text ltx_font_italic">ICCV</span>,
2017.

</span>
</li>
<li id="bib.bib138" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[138]</span>
<span class="ltx_bibblock">
J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “ImageNet: A
large-scale hierarchical image database,” in <span id="bib.bib138.1.1" class="ltx_text ltx_font_italic">CVPR</span>, 2009.

</span>
</li>
<li id="bib.bib139" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[139]</span>
<span class="ltx_bibblock">
L. Yu, P. Poirson, S. Yang, A. C. Berg, and T. L. Berg, “Modeling context in
referring expressions,” in <span id="bib.bib139.1.1" class="ltx_text ltx_font_italic">ECCV</span>, 2016.

</span>
</li>
<li id="bib.bib140" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[140]</span>
<span class="ltx_bibblock">
J. Mao, J. Huang, A. Toshev, O. Camburu, A. L. Yuille, and K. Murphy,
“Generation and comprehension of unambiguous object descriptions,” in <span id="bib.bib140.1.1" class="ltx_text ltx_font_italic">CVPR</span>, 2016.

</span>
</li>
<li id="bib.bib141" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[141]</span>
<span class="ltx_bibblock">
S. Kazemzadeh, V. Ordonez, M. Matten, and T. Berg, “Referitgame: Referring to
objects in photographs of natural scenes,” in <span id="bib.bib141.1.1" class="ltx_text ltx_font_italic">EMNLP</span>, 2014.

</span>
</li>
<li id="bib.bib142" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[142]</span>
<span class="ltx_bibblock">
N. Parmar, A. Vaswani, J. Uszkoreit, Ł. Kaiser, N. Shazeer, A. Ku, and
D. Tran, “Image transformer,” in <span id="bib.bib142.1.1" class="ltx_text ltx_font_italic">ICML</span>, 2018.

</span>
</li>
<li id="bib.bib143" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[143]</span>
<span class="ltx_bibblock">
M. Chen, A. Radford, R. Child, J. Wu, H. Jun, D. Luan, and I. Sutskever,
“Generative pretraining from pixels,” in <span id="bib.bib143.1.1" class="ltx_text ltx_font_italic">ICML</span>, 2020.

</span>
</li>
<li id="bib.bib144" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[144]</span>
<span class="ltx_bibblock">
P. Esser, R. Rombach, and B. Ommer, “Taming transformers for high-resolution
image synthesis,” <span id="bib.bib144.1.1" class="ltx_text ltx_font_italic">arXiv:2012.09841</span>, 2020.

</span>
</li>
<li id="bib.bib145" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[145]</span>
<span class="ltx_bibblock">
Y. Jiang, S. Chang, and Z. Wang, “Transgan: Two transformers can make one
strong gan,” 2021.

</span>
</li>
<li id="bib.bib146" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[146]</span>
<span class="ltx_bibblock">
A. K. Bhunia, S. Khan, H. Cholakkal, R. M. Anwer, F. S. Khan, and M. Shah,
“Handwriting transformers,” <span id="bib.bib146.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2104.03964</span>, 2021.

</span>
</li>
<li id="bib.bib147" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[147]</span>
<span class="ltx_bibblock">
A. Van den Oord, N. Kalchbrenner, L. Espeholt, O. Vinyals, A. Graves, <span id="bib.bib147.1.1" class="ltx_text ltx_font_italic">et al.</span>, “Conditional image generation with pixelcnn decoders,” in <span id="bib.bib147.2.2" class="ltx_text ltx_font_italic">NeurIPS</span>, 2016.

</span>
</li>
<li id="bib.bib148" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[148]</span>
<span class="ltx_bibblock">
A. Krizhevsky, “Learning multiple layers of features from tiny images,” tech.
rep., 2009.

</span>
</li>
<li id="bib.bib149" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[149]</span>
<span class="ltx_bibblock">
A. Coates, A. Ng, and H. Lee, “An analysis of single-layer networks in
unsupervised feature learning,” in <span id="bib.bib149.1.1" class="ltx_text ltx_font_italic">AISTATS</span>, 2011.

</span>
</li>
<li id="bib.bib150" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[150]</span>
<span class="ltx_bibblock">
T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, “A simple framework for
contrastive learning of visual representations,” <span id="bib.bib150.1.1" class="ltx_text ltx_font_italic">arXiv preprint
arXiv:2002.05709</span>, 2020.

</span>
</li>
<li id="bib.bib151" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[151]</span>
<span class="ltx_bibblock">
P. Bachman, R. Hjelm, and W. Buchwalter, “Learning representations by
maximizing mutual information across views,” in <span id="bib.bib151.1.1" class="ltx_text ltx_font_italic">NeurIPS</span>, 2019.

</span>
</li>
<li id="bib.bib152" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[152]</span>
<span class="ltx_bibblock">
O. J. Hénaff, A. Srinivas, J. De Fauw, A. Razavi, C. Doersch, S. Eslami,
and A. v. d. Oord, “Data-efficient image recognition with contrastive
predictive coding,” <span id="bib.bib152.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1905.09272</span>, 2019.

</span>
</li>
<li id="bib.bib153" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[153]</span>
<span class="ltx_bibblock">
Y. Tian, D. Krishnan, and P. Isola, “Contrastive multiview coding,” <span id="bib.bib153.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1906.05849</span>, 2019.

</span>
</li>
<li id="bib.bib154" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[154]</span>
<span class="ltx_bibblock">
S. Khan, H. Rahmani, S. A. A. Shah, and M. Bennamoun, “A guide to
convolutional neural networks for computer vision,” <span id="bib.bib154.1.1" class="ltx_text ltx_font_italic">Synthesis Lectures
on Computer Vision</span>, 2018.

</span>
</li>
<li id="bib.bib155" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[155]</span>
<span class="ltx_bibblock">
A. Radford, L. Metz, and S. Chintala, “Unsupervised representation learning
with deep convolutional generative adversarial networks,” <span id="bib.bib155.1.1" class="ltx_text ltx_font_italic">arXiv
preprint arXiv:1511.06434</span>, 2015.

</span>
</li>
<li id="bib.bib156" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[156]</span>
<span class="ltx_bibblock">
C. Gao, Y. Chen, S. Liu, Z. Tan, and S. Yan, “Adversarialnas: Adversarial
neural architecture search for gans,” in <span id="bib.bib156.1.1" class="ltx_text ltx_font_italic">CVPR</span>, pp. 5680–5689, 2020.

</span>
</li>
<li id="bib.bib157" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[157]</span>
<span class="ltx_bibblock">
T. Karras, S. Laine, M. Aittala, J. Hellsten, J. Lehtinen, and T. Aila,
“Analyzing and improving the image quality of stylegan,” in <span id="bib.bib157.1.1" class="ltx_text ltx_font_italic">CVPR</span>,
pp. 8110–8119, 2020.

</span>
</li>
<li id="bib.bib158" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[158]</span>
<span class="ltx_bibblock">
S. Reed, Z. Akata, X. Yan, L. Logeswaran, B. Schiele, and H. Lee, “Generative
adversarial text to image synthesis,” in <span id="bib.bib158.1.1" class="ltx_text ltx_font_italic">ICML</span>, 2016.

</span>
</li>
<li id="bib.bib159" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[159]</span>
<span class="ltx_bibblock">
H. Zhang, T. Xu, H. Li, S. Zhang, X. Wang, X. Huang, and D. N. Metaxas,
“StackGAN: Text to photo-realistic image synthesis with stacked generative
adversarial networks,” in <span id="bib.bib159.1.1" class="ltx_text ltx_font_italic">ICCV</span>, 2017.

</span>
</li>
<li id="bib.bib160" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[160]</span>
<span class="ltx_bibblock">
H. Zhang, T. Xu, H. Li, S. Zhang, X. Wang, X. Huang, and D. N. Metaxas,
“StackGAN++: Realistic image synthesis with stacked generative adversarial
networks,” <span id="bib.bib160.1.1" class="ltx_text ltx_font_italic">TPAMI</span>, 2018.

</span>
</li>
<li id="bib.bib161" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[161]</span>
<span class="ltx_bibblock">
T. Xu, P. Zhang, Q. Huang, H. Zhang, Z. Gan, X. Huang, and X. He, “AttnGAN:
Fine-grained text to image generation with attentional generative adversarial
networks,” in <span id="bib.bib161.1.1" class="ltx_text ltx_font_italic">CVPR</span>, 2018.

</span>
</li>
<li id="bib.bib162" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[162]</span>
<span class="ltx_bibblock">
D. P. Kingma and M. Welling, “Auto-encoding variational bayes,” <span id="bib.bib162.1.1" class="ltx_text ltx_font_italic">arXiv
preprint arXiv:1312.6114</span>, 2013.

</span>
</li>
<li id="bib.bib163" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[163]</span>
<span class="ltx_bibblock">
A. Razavi, A. van den Oord, and O. Vinyals, “Generating diverse high-fidelity
images with vq-vae-2,” in <span id="bib.bib163.1.1" class="ltx_text ltx_font_italic">NeurISP</span>, 2019.

</span>
</li>
<li id="bib.bib164" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[164]</span>
<span class="ltx_bibblock">
J. Liang, J. Cao, G. Sun, K. Zhang, L. Van Gool, and R. Timofte, “Swinir:
Image restoration using swin transformer,” in <span id="bib.bib164.1.1" class="ltx_text ltx_font_italic">ICCVW</span>, 2021.

</span>
</li>
<li id="bib.bib165" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[165]</span>
<span class="ltx_bibblock">
Z. Wang, X. Cun, J. Bao, and J. Liu, “Uformer: A general u-shaped transformer
for image restoration,” <span id="bib.bib165.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2106.03106</span>, 2021.

</span>
</li>
<li id="bib.bib166" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[166]</span>
<span class="ltx_bibblock">
Z. Lu, H. Liu, J. Li, and L. Zhang, “Efficient transformer for single image
super-resolution,” <span id="bib.bib166.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2108.11084</span>, 2021.

</span>
</li>
<li id="bib.bib167" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[167]</span>
<span class="ltx_bibblock">
Y. Zhang, K. Li, K. Li, L. Wang, B. Zhong, and Y. Fu, “Image super-resolution
using very deep residual channel attention networks,” in <span id="bib.bib167.1.1" class="ltx_text ltx_font_italic">ECCV</span>, 2018.

</span>
</li>
<li id="bib.bib168" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[168]</span>
<span class="ltx_bibblock">
T. Dai, J. Cai, Y. Zhang, S. Xia, and L. Zhang, “Second-order attention
network for single image super-resolution,” in <span id="bib.bib168.1.1" class="ltx_text ltx_font_italic">CVPR</span>, 2019.

</span>
</li>
<li id="bib.bib169" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[169]</span>
<span class="ltx_bibblock">
B. Niu, W. Wen, W. Ren, X. Zhang, L. Yang, S. Wang, K. Zhang, X. Cao, and
H. Shen, “Single image super-resolution via a holistic attention network,”
in <span id="bib.bib169.1.1" class="ltx_text ltx_font_italic">ECCV</span>, 2020.

</span>
</li>
<li id="bib.bib170" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[170]</span>
<span class="ltx_bibblock">
B. Lim, S. Son, H. Kim, S. Nah, and K. Mu Lee, “Enhanced deep residual
networks for single image super-resolution,” in <span id="bib.bib170.1.1" class="ltx_text ltx_font_italic">CVPRW</span>, 2017.

</span>
</li>
<li id="bib.bib171" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[171]</span>
<span class="ltx_bibblock">
Y. Tai, J. Yang, and X. Liu, “Image super-resolution via deep recursive
residual network,” in <span id="bib.bib171.1.1" class="ltx_text ltx_font_italic">CVPR</span>, 2017.

</span>
</li>
<li id="bib.bib172" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[172]</span>
<span class="ltx_bibblock">
W. Han, S. Chang, D. Liu, M. Yu, M. Witbrock, and T. Huang, “Image
super-resolution via dual-state recurrent networks,” in <span id="bib.bib172.1.1" class="ltx_text ltx_font_italic">CVPR</span>, 2018.

</span>
</li>
<li id="bib.bib173" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[173]</span>
<span class="ltx_bibblock">
Y. Zhang, Y. Tian, Y. Kong, B. Zhong, and Y. Fu, “Residual dense network for
image restoration,” <span id="bib.bib173.1.1" class="ltx_text ltx_font_italic">TPAMI</span>, 2020.

</span>
</li>
<li id="bib.bib174" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[174]</span>
<span class="ltx_bibblock">
X. Wang, K. Yu, S. Wu, J. Gu, Y. Liu, C. Dong, Y. Qiao, and C. Change Loy,
“ESRGAN: enhanced super-resolution generative adversarial networks,” in
<span id="bib.bib174.1.1" class="ltx_text ltx_font_italic">ECCVW</span>, 2018.

</span>
</li>
<li id="bib.bib175" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[175]</span>
<span class="ltx_bibblock">
S.-J. Park, H. Son, S. Cho, K.-S. Hong, and S. Lee, “SRFEAT: Single image
super-resolution with feature discrimination,” in <span id="bib.bib175.1.1" class="ltx_text ltx_font_italic">ECCV</span>, 2018.

</span>
</li>
<li id="bib.bib176" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[176]</span>
<span class="ltx_bibblock">
M. S. Sajjadi, B. Scholkopf, and M. Hirsch, “EnhanceNet: Single image
super-resolution through automated texture synthesis,” in <span id="bib.bib176.1.1" class="ltx_text ltx_font_italic">ICCV</span>, 2017.

</span>
</li>
<li id="bib.bib177" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[177]</span>
<span class="ltx_bibblock">
C. Ledig, L. Theis, F. Huszár, J. Caballero, A. Cunningham, A. Acosta,
A. Aitken, A. Tejani, J. Totz, Z. Wang, <span id="bib.bib177.1.1" class="ltx_text ltx_font_italic">et al.</span>, “Photo-realistic
single image super-resolution using a generative adversarial network,” in
<span id="bib.bib177.2.2" class="ltx_text ltx_font_italic">CVPR</span>, 2017.

</span>
</li>
<li id="bib.bib178" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[178]</span>
<span class="ltx_bibblock">
J. Johnson, A. Alahi, and L. Fei-Fei, “Perceptual losses for real-time style
transfer and super-resolution,” in <span id="bib.bib178.1.1" class="ltx_text ltx_font_italic">ECCV</span>, 2016.

</span>
</li>
<li id="bib.bib179" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[179]</span>
<span class="ltx_bibblock">
J. Ho, N. Kalchbrenner, D. Weissenborn, and T. Salimans, “Axial attention in
multidimensional transformers,” <span id="bib.bib179.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1912.12180</span>, 2019.

</span>
</li>
<li id="bib.bib180" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[180]</span>
<span class="ltx_bibblock">
G. Li, N. Duan, Y. Fang, M. Gong, D. Jiang, and M. Zhou, “Unicoder-VL: A
universal encoder for vision and language by cross-modal pre-training.,” in
<span id="bib.bib180.1.1" class="ltx_text ltx_font_italic">AAAI</span>, 2020.

</span>
</li>
<li id="bib.bib181" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[181]</span>
<span class="ltx_bibblock">
J. Lu, D. Batra, D. Parikh, and S. Lee, “Vilbert: Pretraining task-agnostic
visiolinguistic representations for vision-and-language tasks,” in <span id="bib.bib181.1.1" class="ltx_text ltx_font_italic">NeurIPS</span>, 2019.

</span>
</li>
<li id="bib.bib182" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[182]</span>
<span class="ltx_bibblock">
S. Lee, Y. Yu, G. Kim, T. Breuel, J. Kautz, and Y. Song, “Parameter efficient
multimodal transformers for video representation learning,” <span id="bib.bib182.1.1" class="ltx_text ltx_font_italic">arXiv
preprint arXiv:2012.04124</span>, 2020.

</span>
</li>
<li id="bib.bib183" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[183]</span>
<span class="ltx_bibblock">
S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. Lawrence Zitnick, and
D. Parikh, “VQA: Visual question answering,” in <span id="bib.bib183.1.1" class="ltx_text ltx_font_italic">ICCV</span>, 2015.

</span>
</li>
<li id="bib.bib184" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[184]</span>
<span class="ltx_bibblock">
R. Zellers, Y. Bisk, A. Farhadi, and Y. Choi, “From recognition to cognition:
Visual commonsense reasoning,” in <span id="bib.bib184.1.1" class="ltx_text ltx_font_italic">CVPR</span>, 2019.

</span>
</li>
<li id="bib.bib185" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[185]</span>
<span class="ltx_bibblock">
K.-H. Lee, X. Chen, G. Hua, H. Hu, and X. He, “Stacked cross attention for
image-text matching,” in <span id="bib.bib185.1.1" class="ltx_text ltx_font_italic">ECCV</span>, 2018.

</span>
</li>
<li id="bib.bib186" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[186]</span>
<span class="ltx_bibblock">
A. Suhr, S. Zhou, A. Zhang, I. Zhang, H. Bai, and Y. Artzi, “A corpus for
reasoning about natural language grounded in photographs,” <span id="bib.bib186.1.1" class="ltx_text ltx_font_italic">arXiv
preprint arXiv:1811.00491</span>, 2018.

</span>
</li>
<li id="bib.bib187" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[187]</span>
<span class="ltx_bibblock">
J. Carreira, E. Noland, C. Hillier, and A. Zisserman, “A short note on the
kinetics-700 human action dataset,” <span id="bib.bib187.1.1" class="ltx_text ltx_font_italic">arXiv:1907.06987</span>, 2019.

</span>
</li>
<li id="bib.bib188" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[188]</span>
<span class="ltx_bibblock">
K. Soomro, A. R. Zamir, and M. Shah, “UCF101: A dataset of 101 human actions
classes from videos in the wild,” <span id="bib.bib188.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1212.0402</span>,
2012.

</span>
</li>
<li id="bib.bib189" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[189]</span>
<span class="ltx_bibblock">
J. F. Gemmeke, D. P. Ellis, D. Freedman, A. Jansen, W. Lawrence, R. C. Moore,
M. Plakal, and M. Ritter, “Audio set: An ontology and human-labeled dataset
for audio events,” in <span id="bib.bib189.1.1" class="ltx_text ltx_font_italic">ICASSP</span>, 2017.

</span>
</li>
<li id="bib.bib190" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[190]</span>
<span class="ltx_bibblock">
G. A. Sigurdsson, G. Varol, X. Wang, A. Farhadi, I. Laptev, and A. Gupta,
“Hollywood in homes: Crowdsourcing data collection for activity
understanding,” in <span id="bib.bib190.1.1" class="ltx_text ltx_font_italic">ECCV</span>, 2016.

</span>
</li>
<li id="bib.bib191" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[191]</span>
<span class="ltx_bibblock">
H. Tan and M. Bansal, “Vokenization: Improving language understanding with
contextualized, visual-grounded supervision,” in <span id="bib.bib191.1.1" class="ltx_text ltx_font_italic">EMNLP</span>, 2020.

</span>
</li>
<li id="bib.bib192" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[192]</span>
<span class="ltx_bibblock">
W. Hao, C. Li, X. Li, L. Carin, and J. Gao, “Towards learning a generic agent
for vision-and-language navigation via pre-training,” in <span id="bib.bib192.1.1" class="ltx_text ltx_font_italic">CVPR</span>, 2020.

</span>
</li>
<li id="bib.bib193" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[193]</span>
<span class="ltx_bibblock">
A. Majumdar, A. Shrivastava, S. Lee, P. Anderson, D. Parikh, and D. Batra,
“Improving vision-and-language navigation with image-text pairs from the
web,” <span id="bib.bib193.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2004.14973</span>, 2020.

</span>
</li>
<li id="bib.bib194" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[194]</span>
<span class="ltx_bibblock">
K. Chen, J. K. Chen, J. Chuang, M. Vázquez, and S. Savarese, “Topological
planning with transformers for vision-and-language navigation,” <span id="bib.bib194.1.1" class="ltx_text ltx_font_italic">arXiv
preprint arXiv:2012.05292</span>, 2020.

</span>
</li>
<li id="bib.bib195" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[195]</span>
<span class="ltx_bibblock">
A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry,
A. Askell, P. Mishkin, J. Clark, <span id="bib.bib195.1.1" class="ltx_text ltx_font_italic">et al.</span>, “Learning transferable visual
models from natural language supervision,” <span id="bib.bib195.2.2" class="ltx_text ltx_font_italic">Image</span>, vol. 2, p. T2, 2021.

</span>
</li>
<li id="bib.bib196" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[196]</span>
<span class="ltx_bibblock">
P. Sharma, N. Ding, S. Goodman, and R. Soricut, “Conceptual captions: A
cleaned, hypernymed, image alt-text dataset for automatic image captioning,”
in <span id="bib.bib196.1.1" class="ltx_text ltx_font_italic">ACL</span>, 2018.

</span>
</li>
<li id="bib.bib197" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[197]</span>
<span class="ltx_bibblock">
L. Zhou, H. Palangi, L. Zhang, H. Hu, J. Corso, and J. Gao, “Unified
vision-language pre-training for image captioning and vqa,” in <span id="bib.bib197.1.1" class="ltx_text ltx_font_italic">AAAI</span>,
vol. 34, pp. 13041–13049, 2020.

</span>
</li>
<li id="bib.bib198" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[198]</span>
<span class="ltx_bibblock">
C. Sun, F. Baradel, K. Murphy, and C. Schmid, “Learning video representations
using contrastive bidirectional transformer,” <span id="bib.bib198.1.1" class="ltx_text ltx_font_italic">arXiv preprint
arXiv:1906.05743</span>, 2019.

</span>
</li>
<li id="bib.bib199" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[199]</span>
<span class="ltx_bibblock">
C. Alberti, J. Ling, M. Collins, and D. Reitter, “Fusion of detected objects
in text for visual question answering,” in <span id="bib.bib199.1.1" class="ltx_text ltx_font_italic">EMNLP</span>, 2019.

</span>
</li>
<li id="bib.bib200" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[200]</span>
<span class="ltx_bibblock">
R. Krishna, Y. Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz, S. Chen,
Y. Kalantidis, L.-J. Li, D. A. Shamma, <span id="bib.bib200.1.1" class="ltx_text ltx_font_italic">et al.</span>, “Visual genome:
Connecting language and vision using crowdsourced dense image annotations,”
<span id="bib.bib200.2.2" class="ltx_text ltx_font_italic">IJCV</span>, 2017.

</span>
</li>
<li id="bib.bib201" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[201]</span>
<span class="ltx_bibblock">
V. Ordonez, G. Kulkarni, and T. L. Berg, “Im2text: Describing images using 1
million captioned photographs,” in <span id="bib.bib201.1.1" class="ltx_text ltx_font_italic">NeurIPS</span>, 2011.

</span>
</li>
<li id="bib.bib202" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[202]</span>
<span class="ltx_bibblock">
P. Zhang, X. Li, X. Hu, J. Yang, L. Zhang, L. Wang, Y. Choi, and J. Gao,
“Vinvl: Revisiting visual representations in vision-language models,” in
<span id="bib.bib202.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition</span>, pp. 5579–5588, 2021.

</span>
</li>
<li id="bib.bib203" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[203]</span>
<span class="ltx_bibblock">
A. Kamath, M. Singh, Y. LeCun, I. Misra, G. Synnaeve, and N. Carion,
“Mdetr–modulated detection for end-to-end multi-modal understanding,” <span id="bib.bib203.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2104.12763</span>, 2021.

</span>
</li>
<li id="bib.bib204" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[204]</span>
<span class="ltx_bibblock">
J. Deng, Z. Yang, T. Chen, W. Zhou, and H. Li, “Transvg: End-to-end visual
grounding with transformers,” 2021.

</span>
</li>
<li id="bib.bib205" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[205]</span>
<span class="ltx_bibblock">
M. Li and L. Sigal, “Referring transformer: A one-step approach to multi-task
visual grounding,” <span id="bib.bib205.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2106.03089</span>, 2021.

</span>
</li>
<li id="bib.bib206" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[206]</span>
<span class="ltx_bibblock">
Y. Du, Z. Fu, Q. Liu, and Y. Wang, “Visual grounding with transformers,” <span id="bib.bib206.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2105.04281</span>, 2021.

</span>
</li>
<li id="bib.bib207" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[207]</span>
<span class="ltx_bibblock">
S. Ging, M. Zolfaghari, H. Pirsiavash, and T. Brox, “COOT: Cooperative
hierarchical transformer for video-text representation learning,” <span id="bib.bib207.1.1" class="ltx_text ltx_font_italic">arXiv
preprint arXiv:2011.00597</span>, 2020.

</span>
</li>
<li id="bib.bib208" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[208]</span>
<span class="ltx_bibblock">
H. Seong, J. Hyun, and E. Kim, “Video multitask transformer network,” in <span id="bib.bib208.1.1" class="ltx_text ltx_font_italic">ICCV Workshops</span>, pp. 0–0, 2019.

</span>
</li>
<li id="bib.bib209" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[209]</span>
<span class="ltx_bibblock">
Y. Wang, Z. Xu, X. Wang, C. Shen, B. Cheng, H. Shen, and H. Xia, “End-to-end
video instance segmentation with transformers,” <span id="bib.bib209.1.1" class="ltx_text ltx_font_italic">arXiv preprint
arXiv:2011.14503</span>, 2020.

</span>
</li>
<li id="bib.bib210" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[210]</span>
<span class="ltx_bibblock">
L. Zhou, Y. Zhou, J. Corso, R. Socher, and C. Xiong, “End-to-end dense video
captioning with masked transformer,” in <span id="bib.bib210.1.1" class="ltx_text ltx_font_italic">CVPR</span>, 2018.

</span>
</li>
<li id="bib.bib211" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[211]</span>
<span class="ltx_bibblock">
D. Neimark, O. Bar, M. Zohar, and D. Asselmann, “Video transformer network,”
<span id="bib.bib211.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2102.00719</span>, 2021.

</span>
</li>
<li id="bib.bib212" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[212]</span>
<span class="ltx_bibblock">
A. Arnab, M. Dehghani, G. Heigold, C. Sun, M. Lučić, and C. Schmid,
“Vivit: A video vision transformer,” <span id="bib.bib212.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2103.15691</span>,
2021.

</span>
</li>
<li id="bib.bib213" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[213]</span>
<span class="ltx_bibblock">
G. Bertasius, H. Wang, and L. Torresani, “Is space-time attention all you need
for video understanding?,” in <span id="bib.bib213.1.1" class="ltx_text ltx_font_italic">Proceedings of the International
Conference on Machine Learning (ICML)</span>, July 2021.

</span>
</li>
<li id="bib.bib214" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[214]</span>
<span class="ltx_bibblock">
R. Krishna, K. Hata, F. Ren, L. Fei-Fei, and J. Carlos Niebles,
“Dense-captioning events in videos,” in <span id="bib.bib214.1.1" class="ltx_text ltx_font_italic">ICCV</span>, pp. 706–715, 2017.

</span>
</li>
<li id="bib.bib215" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[215]</span>
<span class="ltx_bibblock">
L. Zhou, C. Xu, and J. Corso, “Towards automatic learning of procedures from
web instructional videos,” in <span id="bib.bib215.1.1" class="ltx_text ltx_font_italic">AAAI</span>, vol. 32, 2018.

</span>
</li>
<li id="bib.bib216" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[216]</span>
<span class="ltx_bibblock">
C. Plizzari, M. Cannici, and M. Matteucci, “Spatial temporal transformer
network for skeleton-based action recognition,” <span id="bib.bib216.1.1" class="ltx_text ltx_font_italic">arXiv preprint
arXiv:2008.07404</span>, 2020.

</span>
</li>
<li id="bib.bib217" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[217]</span>
<span class="ltx_bibblock">
A. Shahroudy, J. Liu, T.-T. Ng, and G. Wang, “NTU RGB+D: A large scale
dataset for 3d human activity analysis,” in <span id="bib.bib217.1.1" class="ltx_text ltx_font_italic">CVPR</span>, 2016.

</span>
</li>
<li id="bib.bib218" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[218]</span>
<span class="ltx_bibblock">
J. Liu, A. Shahroudy, M. Perez, G. Wang, L.-Y. Duan, and A. C. Kot, “NTU
RGB+D 120: A large-scale benchmark for 3d human activity understanding,”
<span id="bib.bib218.1.1" class="ltx_text ltx_font_italic">TPAMI</span>, 2019.

</span>
</li>
<li id="bib.bib219" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[219]</span>
<span class="ltx_bibblock">
H. Fan, B. Xiong, K. Mangalam, Y. Li, Z. Yan, J. Malik, and C. Feichtenhofer,
“Multiscale vision transformers,” 2021.

</span>
</li>
<li id="bib.bib220" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[220]</span>
<span class="ltx_bibblock">
J. Wang, G. Bertasius, D. Tran, and L. Torresani, “Long-short temporal
contrastive learning of video transformers,” <span id="bib.bib220.1.1" class="ltx_text ltx_font_italic">arXiv preprint
arXiv:2106.09212</span>, 2021.

</span>
</li>
<li id="bib.bib221" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[221]</span>
<span class="ltx_bibblock">
L. Yang, Y. Fan, and N. Xu, “Video instance segmentation,” in <span id="bib.bib221.1.1" class="ltx_text ltx_font_italic">ICCV</span>,
pp. 5188–5197, 2019.

</span>
</li>
<li id="bib.bib222" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[222]</span>
<span class="ltx_bibblock">
G. Bertasius and L. Torresani, “Classifying, segmenting, and tracking object
instances in video with mask propagation,” in <span id="bib.bib222.1.1" class="ltx_text ltx_font_italic">CVPR</span>, pp. 9739–9748,
2020.

</span>
</li>
<li id="bib.bib223" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[223]</span>
<span class="ltx_bibblock">
E. Triantafillou, T. Zhu, V. Dumoulin, P. Lamblin, U. Evci, K. Xu, R. Goroshin,
C. Gelada, K. Swersky, P.-A. Manzagol, <span id="bib.bib223.1.1" class="ltx_text ltx_font_italic">et al.</span>, “Meta-dataset: A
dataset of datasets for learning to learn from few examples,” in <span id="bib.bib223.2.2" class="ltx_text ltx_font_italic">ICLR</span>,
2020.

</span>
</li>
<li id="bib.bib224" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[224]</span>
<span class="ltx_bibblock">
T. N. Kipf and M. Welling, “Semi-supervised classification with graph
convolutional networks,” <span id="bib.bib224.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1609.02907</span>, 2016.

</span>
</li>
<li id="bib.bib225" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[225]</span>
<span class="ltx_bibblock">
M. Zaheer, S. Kottur, S. Ravanbakhsh, B. Poczos, R. R. Salakhutdinov, and A. J.
Smola, “Deep sets,” in <span id="bib.bib225.1.1" class="ltx_text ltx_font_italic">NeurIPS</span>, 2017.

</span>
</li>
<li id="bib.bib226" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[226]</span>
<span class="ltx_bibblock">
L. Liu, W. Hamilton, G. Long, J. Jiang, and H. Larochelle, “A universal
representation transformer layer for few-shot image classification,” 2020.

</span>
</li>
<li id="bib.bib227" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[227]</span>
<span class="ltx_bibblock">
H. Edwards and A. Storkey, “Towards a neural statistician,” <span id="bib.bib227.1.1" class="ltx_text ltx_font_italic">arXiv
preprint arXiv:1606.02185</span>, 2016.

</span>
</li>
<li id="bib.bib228" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[228]</span>
<span class="ltx_bibblock">
J. Lee, Y. Lee, J. Kim, A. Kosiorek, S. Choi, and Y. W. Teh, “Set transformer:
A framework for attention-based permutation-invariant neural networks,” in
<span id="bib.bib228.1.1" class="ltx_text ltx_font_italic">ICML</span>, 2019.

</span>
</li>
<li id="bib.bib229" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[229]</span>
<span class="ltx_bibblock">
J. Lee, Y. Lee, and Y. W. Teh, “Deep amortized clustering,” <span id="bib.bib229.1.1" class="ltx_text ltx_font_italic">arXiv
preprint arXiv:1909.13433</span>, 2019.

</span>
</li>
<li id="bib.bib230" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[230]</span>
<span class="ltx_bibblock">
H. Zhao, L. Jiang, J. Jia, P. Torr, and V. Koltun, “Point transformer,” <span id="bib.bib230.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2012.09164</span>, 2020.

</span>
</li>
<li id="bib.bib231" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[231]</span>
<span class="ltx_bibblock">
M.-H. Guo, J.-X. Cai, Z.-N. Liu, T.-J. Mu, R. R. Martin, and S.-M. Hu, “Pct:
Point cloud transformer,” <span id="bib.bib231.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2012.09688</span>, 2020.

</span>
</li>
<li id="bib.bib232" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[232]</span>
<span class="ltx_bibblock">
Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and J. Xiao, “3D
ShapeNets: A deep representation for volumetric shapes,” in <span id="bib.bib232.1.1" class="ltx_text ltx_font_italic">CVPR</span>,
2015.

</span>
</li>
<li id="bib.bib233" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[233]</span>
<span class="ltx_bibblock">
A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. Li,
S. Savarese, M. Savva, S. Song, H. Su, J. Xiao, L. Yi, and F. Yu,
“ShapeNet: An information-rich 3d model repository,” <span id="bib.bib233.1.1" class="ltx_text ltx_font_italic">arXiv preprint
arXiv:1512.03012</span>, 2015.

</span>
</li>
<li id="bib.bib234" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[234]</span>
<span class="ltx_bibblock">
C. Ionescu, D. Papava, V. Olaru, and C. Sminchisescu, “Human3.6M: Large
scale datasets and predictive methods for 3D human sensing in natural
environments,” <span id="bib.bib234.1.1" class="ltx_text ltx_font_italic">TPAMI</span>, 2013.

</span>
</li>
<li id="bib.bib235" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[235]</span>
<span class="ltx_bibblock">
T. von Marcard, R. Henschel, M. J. Black, B. Rosenhahn, and G. Pons-Moll,
“Recovering accurate 3d human pose in the wild using imus and a moving
camera,” in <span id="bib.bib235.1.1" class="ltx_text ltx_font_italic">ECCV</span>, 2018.

</span>
</li>
<li id="bib.bib236" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[236]</span>
<span class="ltx_bibblock">
C. Zimmermann, D. Ceylan, J. Yang, B. Russell, M. Argus, and T. Brox,
“FreiHAND: A dataset for markerless capture of hand pose and shape from
single rgb images,” in <span id="bib.bib236.1.1" class="ltx_text ltx_font_italic">ICCV</span>, 2019.

</span>
</li>
<li id="bib.bib237" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[237]</span>
<span class="ltx_bibblock">
“OpenAI’s GPT-3 language model: A technical overview.”
<a target="_blank" href="https://lambdalabs.com/blog/demystifying-gpt-3/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://lambdalabs.com/blog/demystifying-gpt-3/</a>.

</span>
<span class="ltx_bibblock">Accessed: 2020-12-31.

</span>
</li>
<li id="bib.bib238" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[238]</span>
<span class="ltx_bibblock">
X. Zhai, A. Kolesnikov, N. Houlsby, and L. Beyer, “Scaling vision
transformers,” 2021.

</span>
</li>
<li id="bib.bib239" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[239]</span>
<span class="ltx_bibblock">
P. Young, A. Lai, M. Hodosh, and J. Hockenmaier, “From image descriptions to
visual denotations: New similarity metrics for semantic inference over event
descriptions,” <span id="bib.bib239.1.1" class="ltx_text ltx_font_italic">TACL</span>, 2014.

</span>
</li>
<li id="bib.bib240" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[240]</span>
<span class="ltx_bibblock">
Y. Goyal, T. Khot, D. Summers-Stay, D. Batra, and D. Parikh, “Making the v in
vqa matter: Elevating the role of image understanding in visual question
answering,” in <span id="bib.bib240.1.1" class="ltx_text ltx_font_italic">CVPR</span>, 2017.

</span>
</li>
<li id="bib.bib241" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[241]</span>
<span class="ltx_bibblock">
B. A. Plummer, L. Wang, C. M. Cervantes, J. C. Caicedo, J. Hockenmaier, and
S. Lazebnik, “Flickr30k entities: Collecting region-to-phrase
correspondences for richer image-to-sentence models,” in <span id="bib.bib241.1.1" class="ltx_text ltx_font_italic">ICCV</span>, 2015.

</span>
</li>
<li id="bib.bib242" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[242]</span>
<span class="ltx_bibblock">
C. R. Qi, L. Yi, H. Su, and L. J. Guibas, “PointNet++: Deep hierarchical
feature learning on point sets in a metric space,” <span id="bib.bib242.1.1" class="ltx_text ltx_font_italic">NeurIPS</span>, 2017.

</span>
</li>
<li id="bib.bib243" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[243]</span>
<span class="ltx_bibblock">
H. Touvron, M. Cord, A. Sablayrolles, G. Synnaeve, and H. Jégou, “Going
deeper with image transformers,” <span id="bib.bib243.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2103.17239</span>,
2021.

</span>
</li>
<li id="bib.bib244" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[244]</span>
<span class="ltx_bibblock">
S. Xie, R. Girshick, P. Dollár, Z. Tu, and K. He, “Aggregated residual
transformations for deep neural networks,” in <span id="bib.bib244.1.1" class="ltx_text ltx_font_italic">CVPR</span>, 2017.

</span>
</li>
<li id="bib.bib245" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[245]</span>
<span class="ltx_bibblock">
R. Child, S. Gray, A. Radford, and I. Sutskever, “Generating long sequences
with sparse transformers,” <span id="bib.bib245.1.1" class="ltx_text ltx_font_italic">arXiv:1904.10509</span>, 2019.

</span>
</li>
<li id="bib.bib246" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[246]</span>
<span class="ltx_bibblock">
N. Kitaev, Ł. Kaiser, and A. Levskaya, “Reformer: The efficient
transformer,” in <span id="bib.bib246.1.1" class="ltx_text ltx_font_italic">ICLR</span>, 2020.

</span>
</li>
<li id="bib.bib247" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[247]</span>
<span class="ltx_bibblock">
I. Bello, “Lambdanetworks: Modeling long-range interactions without
attention,” in <span id="bib.bib247.1.1" class="ltx_text ltx_font_italic">International Conference on Learning Representations</span>,
2021.

</span>
</li>
<li id="bib.bib248" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[248]</span>
<span class="ltx_bibblock">
A. Vyas, A. Katharopoulos, and F. Fleuret, “Fast transformers with clustered
attention,” <span id="bib.bib248.1.1" class="ltx_text ltx_font_italic">NeurIPS</span>, 2020.

</span>
</li>
<li id="bib.bib249" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[249]</span>
<span class="ltx_bibblock">
Y.-H. Wu, Y. Liu, X. Zhan, and M.-M. Cheng, “P2t: Pyramid pooling transformer
for scene understanding,” <span id="bib.bib249.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2106.12011</span>, 2021.

</span>
</li>
<li id="bib.bib250" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[250]</span>
<span class="ltx_bibblock">
A. Vaswani, P. Ramachandran, A. Srinivas, N. Parmar, B. Hechtman, and
J. Shlens, “Scaling local self-attention for parameter efficient visual
backbones,” in <span id="bib.bib250.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition</span>, pp. 12894–12904, 2021.

</span>
</li>
<li id="bib.bib251" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[251]</span>
<span class="ltx_bibblock">
X. Dong, J. Bao, D. Chen, W. Zhang, N. Yu, L. Yuan, D. Chen, and B. Guo,
“Cswin transformer: A general vision transformer backbone with cross-shaped
windows,” <span id="bib.bib251.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2107.00652</span>, 2021.

</span>
</li>
<li id="bib.bib252" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[252]</span>
<span class="ltx_bibblock">
Y. Xiong, Z. Zeng, R. Chakraborty, M. Tan, G. Fung, Y. Li, and V. Singh,
“Nystr<math id="bib.bib252.1.m1.1" class="ltx_Math" alttext="\backslash" display="inline"><semantics id="bib.bib252.1.m1.1a"><mo id="bib.bib252.1.m1.1.1" xref="bib.bib252.1.m1.1.1.cmml">\</mo><annotation-xml encoding="MathML-Content" id="bib.bib252.1.m1.1b"><ci id="bib.bib252.1.m1.1.1.cmml" xref="bib.bib252.1.m1.1.1">\</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib252.1.m1.1c">\backslash</annotation><annotation encoding="application/x-llamapun" id="bib.bib252.1.m1.1d">\</annotation></semantics></math>” omformer: A nystr<math id="bib.bib252.2.m2.1" class="ltx_Math" alttext="\backslash" display="inline"><semantics id="bib.bib252.2.m2.1a"><mo id="bib.bib252.2.m2.1.1" xref="bib.bib252.2.m2.1.1.cmml">\</mo><annotation-xml encoding="MathML-Content" id="bib.bib252.2.m2.1b"><ci id="bib.bib252.2.m2.1.1.cmml" xref="bib.bib252.2.m2.1.1">\</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib252.2.m2.1c">\backslash</annotation><annotation encoding="application/x-llamapun" id="bib.bib252.2.m2.1d">\</annotation></semantics></math>” om-based algorithm for
approximating self-attention,” in <span id="bib.bib252.3.1" class="ltx_text ltx_font_italic">AAAI</span>, 2021.

</span>
</li>
<li id="bib.bib253" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[253]</span>
<span class="ltx_bibblock">
Y. Tay, D. Bahri, D. Metzler, D. Juan, Z. Zhao, and C. Zheng, “Synthesizer:
Rethinking self-attention in transformer models,” in <span id="bib.bib253.1.1" class="ltx_text ltx_font_italic">ICML</span>, 2021.

</span>
</li>
<li id="bib.bib254" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[254]</span>
<span class="ltx_bibblock">
H. Peng, N. Pappas, D. Yogatama, R. Schwartz, N. A. Smith, and L. Kong,
“Random feature attention,” in <span id="bib.bib254.1.1" class="ltx_text ltx_font_italic">ICLR</span>, 2021.

</span>
</li>
<li id="bib.bib255" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[255]</span>
<span class="ltx_bibblock">
K. Choromanski, V. Likhosherstov, D. Dohan, X. Song, A. Gane, T. Sarlos,
P. Hawkins, J. Davis, A. Mohiuddin, L. Kaiser, <span id="bib.bib255.1.1" class="ltx_text ltx_font_italic">et al.</span>, “Rethinking
attention with performers,” in <span id="bib.bib255.2.2" class="ltx_text ltx_font_italic">ICLR</span>, 2021.

</span>
</li>
<li id="bib.bib256" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[256]</span>
<span class="ltx_bibblock">
Y. Tay, D. Bahri, L. Yang, D. Metzler, and D.-C. Juan, “Sparse sinkhorn
attention,” in <span id="bib.bib256.1.1" class="ltx_text ltx_font_italic">ICML</span>, 2020.

</span>
</li>
<li id="bib.bib257" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[257]</span>
<span class="ltx_bibblock">
X. Chen, C.-J. Hsieh, and B. Gong, “When vision transformers outperform
resnets without pretraining or strong data augmentations,” <span id="bib.bib257.1.1" class="ltx_text ltx_font_italic">arXiv
preprint arXiv:2106.01548</span>, 2021.

</span>
</li>
<li id="bib.bib258" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[258]</span>
<span class="ltx_bibblock">
P. Foret, A. Kleiner, H. Mobahi, and B. Neyshabur, “Sharpness-aware
minimization for efficiently improving generalization,” <span id="bib.bib258.1.1" class="ltx_text ltx_font_italic">arXiv preprint
arXiv:2010.01412</span>, 2020.

</span>
</li>
<li id="bib.bib259" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[259]</span>
<span class="ltx_bibblock">
C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, “Rethinking the
inception architecture for computer vision,” in <span id="bib.bib259.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE
conference on computer vision and pattern recognition</span>, pp. 2818–2826, 2016.

</span>
</li>
<li id="bib.bib260" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[260]</span>
<span class="ltx_bibblock">
S. He, H. Luo, P. Wang, F. Wang, H. Li, and W. Jiang, “Transreid:
Transformer-based object re-identification,” <span id="bib.bib260.1.1" class="ltx_text ltx_font_italic">arXiv:2102.04378</span>, 2021.

</span>
</li>
<li id="bib.bib261" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[261]</span>
<span class="ltx_bibblock">
D. R. So, C. Liang, and Q. V. Le, “The evolved transformer,” 2019.

</span>
</li>
<li id="bib.bib262" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[262]</span>
<span class="ltx_bibblock">
H. Wang, Z. Wu, Z. Liu, H. Cai, L. Zhu, C. Gan, and S. Han, “Hat:
Hardware-aware transformers for efficient natural language processing,”
2020.

</span>
</li>
<li id="bib.bib263" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[263]</span>
<span class="ltx_bibblock">
M. Chen, H. Peng, J. Fu, and H. Ling, “Autoformer: Searching transformers for
visual recognition,” <span id="bib.bib263.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2107.00651</span>, 2021.

</span>
</li>
<li id="bib.bib264" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[264]</span>
<span class="ltx_bibblock">
C. Li, T. Tang, G. Wang, J. Peng, B. Wang, X. Liang, and X. Chang, “Bossnas:
Exploring hybrid cnn-transformers with block-wisely self-supervised neural
architecture search,” <span id="bib.bib264.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2103.12424</span>, 2021.

</span>
</li>
<li id="bib.bib265" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[265]</span>
<span class="ltx_bibblock">
B. Chen, P. Li, C. Li, B. Li, L. Bai, C. Lin, M. Sun, W. Ouyang, <span id="bib.bib265.1.1" class="ltx_text ltx_font_italic">et al.</span>,
“Glit: Neural architecture search for global and local image transformer,”
<span id="bib.bib265.2.2" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2107.02960</span>, 2021.

</span>
</li>
<li id="bib.bib266" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[266]</span>
<span class="ltx_bibblock">
M. Naseer, K. Ranasinghe, S. Khan, M. Hayat, F. S. Khan, and M.-H. Yang,
“Intriguing properties of vision transformers,” <span id="bib.bib266.1.1" class="ltx_text ltx_font_italic">arXiv preprint
arXiv:2105.10497</span>, 2021.

</span>
</li>
<li id="bib.bib267" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[267]</span>
<span class="ltx_bibblock">
E. Voita, D. Talbot, F. Moiseev, R. Sennrich, and I. Titov, “Analyzing
multi-head self-attention: Specialized heads do the heavy lifting, the rest
can be pruned,” <span id="bib.bib267.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1905.09418</span>, 2019.

</span>
</li>
<li id="bib.bib268" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[268]</span>
<span class="ltx_bibblock">
S. Abnar and W. Zuidema, “Quantifying attention flow in transformers,” <span id="bib.bib268.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2005.00928</span>, 2020.

</span>
</li>
<li id="bib.bib269" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[269]</span>
<span class="ltx_bibblock">
H. Chefer, S. Gur, and L. Wolf, “Transformer interpretability beyond attention
visualization,” <span id="bib.bib269.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2012.09838</span>, 2020.

</span>
</li>
<li id="bib.bib270" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[270]</span>
<span class="ltx_bibblock">
B. Li, S. Pandey, H. Fang, Y. Lyv, J. Li, J. Chen, M. Xie, L. Wan, H. Liu, and
C. Ding, “FTRANS: energy-efficient acceleration of transformers using
fpga,” in <span id="bib.bib270.1.1" class="ltx_text ltx_font_italic">ISLPED</span>, 2020.

</span>
</li>
<li id="bib.bib271" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[271]</span>
<span class="ltx_bibblock">
G. Bender, P.-J. Kindermans, B. Zoph, V. Vasudevan, and Q. Le, “Understanding
and simplifying one-shot architecture search,” in <span id="bib.bib271.1.1" class="ltx_text ltx_font_italic">ICML</span>, 2018.

</span>
</li>
<li id="bib.bib272" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[272]</span>
<span class="ltx_bibblock">
Z. Guo, X. Zhang, H. Mu, W. Heng, Z. Liu, Y. Wei, and J. Sun, “Single path
one-shot neural architecture search with uniform sampling,” <span id="bib.bib272.1.1" class="ltx_text ltx_font_italic">arXiv
preprint arXiv:1904.00420</span>, 2019.

</span>
</li>
<li id="bib.bib273" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[273]</span>
<span class="ltx_bibblock">
H. Pham, M. Y. Guan, B. Zoph, Q. V. Le, and J. Dean, “Efficient neural
architecture search via parameter sharing,” in <span id="bib.bib273.1.1" class="ltx_text ltx_font_italic">ICML</span>, 2018.

</span>
</li>
<li id="bib.bib274" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[274]</span>
<span class="ltx_bibblock">
A. Jaegle, F. Gimeno, A. Brock, A. Zisserman, O. Vinyals, and J. Carreira,
“Perceiver: General perception with iterative attention,” <span id="bib.bib274.1.1" class="ltx_text ltx_font_italic">arXiv
preprint arXiv:2103.03206</span>, 2021.

</span>
</li>
<li id="bib.bib275" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[275]</span>
<span class="ltx_bibblock">
A. Jaegle, S. Borgeaud, J.-B. Alayrac, C. Doersch, C. Ionescu, D. Ding,
S. Koppula, D. Zoran, A. Brock, E. Shelhamer, <span id="bib.bib275.1.1" class="ltx_text ltx_font_italic">et al.</span>, “Perceiver io: A
general architecture for structured inputs &amp; outputs,” <span id="bib.bib275.2.2" class="ltx_text ltx_font_italic">arXiv preprint
arXiv:2107.14795</span>, 2021.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2101.01168" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2101.01169" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2101.01169">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2101.01169" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2101.01170" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Dec 14 20:48:19 2022 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span style="font-size:70%;position:relative; bottom:2.2pt;">A</span>T<span style="position:relative; bottom:-0.4ex;">E</span></span><span class="ltx_font_smallcaps">xml</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="[LOGO]"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
